
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Project - Machine Learning\n",
    "- **Leonardo Caio de Ladalardo Martins**\n",
    "- **ID: 3075420**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is comparing the performance of a Multi-Layer Perceptron with that of Random Forest. While doing this, we need to address some problems that are inherent to our analysis such as the imbalance over the labels that could generate a bias over the analysis, as also to improve the performance of both algorithms working with its parameters with the objective to find the one that would provide the higher accuracy & score over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start this notebook performing the usual imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time as t\n",
    "import matplotlib.patches as mpatches\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we're going to import the sklearn packages that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Reading And Interpreting The Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our analysis over the dataset, we're firstly going to import it using pandas library, converting it into a treatable object and also performing some basic statistics to understand it in a more deeper way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('mldata_0307542001.csv', dtype='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we're going to define it from the beggining to show all columns of the dataset while exposing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>num.feature 1</th>\n",
       "      <th>num.feature 2</th>\n",
       "      <th>num.feature 3</th>\n",
       "      <th>num.feature 4</th>\n",
       "      <th>num.feature 5</th>\n",
       "      <th>num.feature 6</th>\n",
       "      <th>num.feature 7</th>\n",
       "      <th>num.feature 8</th>\n",
       "      <th>num.feature 9</th>\n",
       "      <th>num.feature 10</th>\n",
       "      <th>num.feature 11</th>\n",
       "      <th>num.feature 12</th>\n",
       "      <th>num.feature 13</th>\n",
       "      <th>num.feature 14</th>\n",
       "      <th>num.feature 15</th>\n",
       "      <th>num.feature 16</th>\n",
       "      <th>num.feature 17</th>\n",
       "      <th>num.feature 18</th>\n",
       "      <th>num.feature 19</th>\n",
       "      <th>num.feature 20</th>\n",
       "      <th>num.feature 21</th>\n",
       "      <th>num.feature 22</th>\n",
       "      <th>num.feature 23</th>\n",
       "      <th>num.feature 24</th>\n",
       "      <th>num.feature 25</th>\n",
       "      <th>num.feature 26</th>\n",
       "      <th>num.feature 27</th>\n",
       "      <th>num.feature 28</th>\n",
       "      <th>num.feature 29</th>\n",
       "      <th>num.feature 30</th>\n",
       "      <th>num.feature 31</th>\n",
       "      <th>num.feature 32</th>\n",
       "      <th>num.feature 33</th>\n",
       "      <th>num.feature 34</th>\n",
       "      <th>num.feature 35</th>\n",
       "      <th>cat.feature 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.655293710354379</td>\n",
       "      <td>2.014054833232026</td>\n",
       "      <td>1.5312187343606998</td>\n",
       "      <td>-0.9373708832551105</td>\n",
       "      <td>-8.469637272851203</td>\n",
       "      <td>1.113903439986374</td>\n",
       "      <td>-0.42905952145040327</td>\n",
       "      <td>0.6427846330833655</td>\n",
       "      <td>-0.9785354410320942</td>\n",
       "      <td>-0.5434369150743821</td>\n",
       "      <td>0.5004595994572275</td>\n",
       "      <td>4.616519808018674</td>\n",
       "      <td>0.6808881962924479</td>\n",
       "      <td>0.6730755131774082</td>\n",
       "      <td>-0.08742126693713967</td>\n",
       "      <td>-0.2651697228510659</td>\n",
       "      <td>3.1860948215502742</td>\n",
       "      <td>0.2884074352632435</td>\n",
       "      <td>0.5738177798006181</td>\n",
       "      <td>4.190982582810352</td>\n",
       "      <td>2.9961116566301853</td>\n",
       "      <td>1.0343228168083176</td>\n",
       "      <td>-0.5895395047491798</td>\n",
       "      <td>1.0498511724513335</td>\n",
       "      <td>-4.98876034321755</td>\n",
       "      <td>0.24448168911127613</td>\n",
       "      <td>0.1383888203646912</td>\n",
       "      <td>12.733733188446942</td>\n",
       "      <td>-0.8323277554390629</td>\n",
       "      <td>3.144878740193841</td>\n",
       "      <td>0.12085391702036802</td>\n",
       "      <td>-0.02292218197000398</td>\n",
       "      <td>-1.9719619919503475</td>\n",
       "      <td>0.21237607040740009</td>\n",
       "      <td>7.124590112731301</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.7984256395878813</td>\n",
       "      <td>1.6919170435177888</td>\n",
       "      <td>1.0482356462149778</td>\n",
       "      <td>5.480638906415307</td>\n",
       "      <td>-3.819468279526532</td>\n",
       "      <td>-0.5861169375133876</td>\n",
       "      <td>-0.46316164430857876</td>\n",
       "      <td>1.391275736650314</td>\n",
       "      <td>0.6458916220445187</td>\n",
       "      <td>-2.6107190116090493</td>\n",
       "      <td>2.7712434705333826</td>\n",
       "      <td>6.195669313471689</td>\n",
       "      <td>0.023358605844600047</td>\n",
       "      <td>0.7458273315641616</td>\n",
       "      <td>5.188554130471982</td>\n",
       "      <td>-0.15475539955618664</td>\n",
       "      <td>-0.416003130089185</td>\n",
       "      <td>-1.4060201020375653</td>\n",
       "      <td>0.6506604070563828</td>\n",
       "      <td>-9.614876401476089</td>\n",
       "      <td>-0.32205287764062207</td>\n",
       "      <td>-1.8570570670815592</td>\n",
       "      <td>0.8000777999684054</td>\n",
       "      <td>0.08218760403803332</td>\n",
       "      <td>0.9954991307626876</td>\n",
       "      <td>2.8747424812395947</td>\n",
       "      <td>0.024378006410350678</td>\n",
       "      <td>-4.526591825701548</td>\n",
       "      <td>1.4579170472956164</td>\n",
       "      <td>-3.9400304528827994</td>\n",
       "      <td>0.9550817747742019</td>\n",
       "      <td>1.46598584281507</td>\n",
       "      <td>1.2647753222284333</td>\n",
       "      <td>3.3286913493038273</td>\n",
       "      <td>-7.046315021752932</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.0391516448049285</td>\n",
       "      <td>0.04324375221837573</td>\n",
       "      <td>2.215229914095391</td>\n",
       "      <td>4.953622286722998</td>\n",
       "      <td>-0.46074817959280046</td>\n",
       "      <td>-1.099156946118343</td>\n",
       "      <td>0.528542562842154</td>\n",
       "      <td>0.8383435021509663</td>\n",
       "      <td>-0.43736465142855263</td>\n",
       "      <td>0.8292222734847019</td>\n",
       "      <td>-0.07646348234231906</td>\n",
       "      <td>-6.2893705673957045</td>\n",
       "      <td>1.44246196213609</td>\n",
       "      <td>1.6162737578517006</td>\n",
       "      <td>2.094705618605038</td>\n",
       "      <td>-0.9239445521479827</td>\n",
       "      <td>1.3543952308200025</td>\n",
       "      <td>1.3675549575475838</td>\n",
       "      <td>2.0596880344459954</td>\n",
       "      <td>-3.5218346365348845</td>\n",
       "      <td>-2.521186677581191</td>\n",
       "      <td>2.539471007050667</td>\n",
       "      <td>-0.6872770667753036</td>\n",
       "      <td>-2.234257003388035</td>\n",
       "      <td>-0.45277656602690397</td>\n",
       "      <td>2.161038370562584</td>\n",
       "      <td>-0.8736782865731195</td>\n",
       "      <td>-6.071563370820246</td>\n",
       "      <td>-0.06540227028915366</td>\n",
       "      <td>0.644367853055584</td>\n",
       "      <td>0.9017071775335213</td>\n",
       "      <td>0.6687721887419774</td>\n",
       "      <td>1.1048654203483457</td>\n",
       "      <td>1.0166929319060005</td>\n",
       "      <td>4.950654571548156</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5921064476513824</td>\n",
       "      <td>-0.5829057500291885</td>\n",
       "      <td>1.6713136588869428</td>\n",
       "      <td>-1.7808474667492604</td>\n",
       "      <td>-3.9707821371419914</td>\n",
       "      <td>-0.3083914226602489</td>\n",
       "      <td>0.08230135499113403</td>\n",
       "      <td>-0.7718830352052108</td>\n",
       "      <td>0.38717443073067614</td>\n",
       "      <td>-0.5760098910106438</td>\n",
       "      <td>1.90776030729306</td>\n",
       "      <td>2.1408898893771773</td>\n",
       "      <td>-0.40531870135667847</td>\n",
       "      <td>-0.8114189854928818</td>\n",
       "      <td>0.13282802694888865</td>\n",
       "      <td>-0.7215621772611183</td>\n",
       "      <td>-1.2725606946795458</td>\n",
       "      <td>-0.713358914048789</td>\n",
       "      <td>0.3825142723790069</td>\n",
       "      <td>-1.7828476331602812</td>\n",
       "      <td>-3.408004805995034</td>\n",
       "      <td>0.4768822067812818</td>\n",
       "      <td>1.7680094586003825</td>\n",
       "      <td>-0.3625841676276228</td>\n",
       "      <td>-1.3400761818240001</td>\n",
       "      <td>1.984914518287928</td>\n",
       "      <td>0.03431028675969114</td>\n",
       "      <td>2.025872914529067</td>\n",
       "      <td>-0.0013047645060802748</td>\n",
       "      <td>1.163716371655199</td>\n",
       "      <td>-0.34438202044532085</td>\n",
       "      <td>-0.5874840931411275</td>\n",
       "      <td>2.9165003844150528</td>\n",
       "      <td>-0.386704638315042</td>\n",
       "      <td>4.7163399882388815</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.183333848599415</td>\n",
       "      <td>-0.8720288566362178</td>\n",
       "      <td>1.771248550898655</td>\n",
       "      <td>-1.3240326709389698</td>\n",
       "      <td>-17.33915954056416</td>\n",
       "      <td>0.36202581349779717</td>\n",
       "      <td>-3.351701763089841</td>\n",
       "      <td>-0.0548368003767373</td>\n",
       "      <td>0.2681452236265388</td>\n",
       "      <td>1.1355369238529915</td>\n",
       "      <td>0.08094436964186573</td>\n",
       "      <td>7.619684323987867</td>\n",
       "      <td>1.1445597193621535</td>\n",
       "      <td>-0.022261203920839176</td>\n",
       "      <td>-1.5140761508081317</td>\n",
       "      <td>-1.3092885395699658</td>\n",
       "      <td>3.9539260643442504</td>\n",
       "      <td>1.450660955152561</td>\n",
       "      <td>-0.9991184296745591</td>\n",
       "      <td>-2.009372814613767</td>\n",
       "      <td>-4.408703063070725</td>\n",
       "      <td>3.4731088093500855</td>\n",
       "      <td>-0.8336840546622525</td>\n",
       "      <td>-0.45426307775195673</td>\n",
       "      <td>0.08135388254536915</td>\n",
       "      <td>2.0014432433952085</td>\n",
       "      <td>0.6317503120796889</td>\n",
       "      <td>5.91531061956587</td>\n",
       "      <td>-2.804864347560359</td>\n",
       "      <td>5.045459342060153</td>\n",
       "      <td>-0.20542189922195633</td>\n",
       "      <td>-0.36398641628325556</td>\n",
       "      <td>2.5909247213031943</td>\n",
       "      <td>-0.5008246489966467</td>\n",
       "      <td>11.465985173802945</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.5990896536953398</td>\n",
       "      <td>1.4473620648246939</td>\n",
       "      <td>-0.23301014252627214</td>\n",
       "      <td>0.9061777403720076</td>\n",
       "      <td>-0.9769792825162696</td>\n",
       "      <td>0.7405836563253125</td>\n",
       "      <td>-0.21455959979054104</td>\n",
       "      <td>-0.9043525559235933</td>\n",
       "      <td>0.4335708171628694</td>\n",
       "      <td>-0.4240813778399951</td>\n",
       "      <td>-1.2577073889798733</td>\n",
       "      <td>0.5034622899916964</td>\n",
       "      <td>-1.9053300352452838</td>\n",
       "      <td>-0.3058320266979141</td>\n",
       "      <td>2.771583580245775</td>\n",
       "      <td>0.7353841609280962</td>\n",
       "      <td>-1.6880012423279134</td>\n",
       "      <td>1.3842611347741567</td>\n",
       "      <td>0.1806280850746561</td>\n",
       "      <td>-1.5715664849937567</td>\n",
       "      <td>-0.8219619826737724</td>\n",
       "      <td>1.104092968405503</td>\n",
       "      <td>0.8197035997197306</td>\n",
       "      <td>-0.5788022427307223</td>\n",
       "      <td>1.554276724873589</td>\n",
       "      <td>-0.07952293199052315</td>\n",
       "      <td>-0.0332699311968469</td>\n",
       "      <td>-4.348792076074895</td>\n",
       "      <td>0.9306531675027093</td>\n",
       "      <td>2.6560934699502337</td>\n",
       "      <td>-0.12894249709012146</td>\n",
       "      <td>0.07671673878010135</td>\n",
       "      <td>2.2429631694575307</td>\n",
       "      <td>-0.4849647887635882</td>\n",
       "      <td>-0.2420768884561871</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.9225368078052741</td>\n",
       "      <td>0.7912550203598914</td>\n",
       "      <td>1.585116806695354</td>\n",
       "      <td>6.049549847277739</td>\n",
       "      <td>7.988317730999925</td>\n",
       "      <td>2.397171061419061</td>\n",
       "      <td>-1.0070473030379696</td>\n",
       "      <td>-0.28901957150780244</td>\n",
       "      <td>0.7827018162124598</td>\n",
       "      <td>-0.3898989798377507</td>\n",
       "      <td>-0.7654992680619259</td>\n",
       "      <td>-5.02730274943831</td>\n",
       "      <td>-1.1428223814810738</td>\n",
       "      <td>-0.9283781063301043</td>\n",
       "      <td>-0.9964377150124951</td>\n",
       "      <td>0.7562460348614661</td>\n",
       "      <td>0.16854553995602686</td>\n",
       "      <td>0.7067486420847652</td>\n",
       "      <td>0.9212623184825482</td>\n",
       "      <td>-2.597130728617354</td>\n",
       "      <td>-2.616044387078954</td>\n",
       "      <td>-2.948227038366142</td>\n",
       "      <td>0.6873920680141749</td>\n",
       "      <td>-1.4553550562241782</td>\n",
       "      <td>2.890133059871754</td>\n",
       "      <td>3.1034909624328355</td>\n",
       "      <td>-0.16753346342461659</td>\n",
       "      <td>-5.101868263929028</td>\n",
       "      <td>0.6203073733554962</td>\n",
       "      <td>-1.0207243359919402</td>\n",
       "      <td>-0.45407958291068573</td>\n",
       "      <td>0.3036156869582973</td>\n",
       "      <td>-1.071500799009482</td>\n",
       "      <td>1.8151328582344022</td>\n",
       "      <td>-2.659020875050966</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0011822649698808585</td>\n",
       "      <td>-1.2391908578441815</td>\n",
       "      <td>1.7320220324447413</td>\n",
       "      <td>-0.44368355308630936</td>\n",
       "      <td>-0.7079141818255753</td>\n",
       "      <td>0.9560940659093461</td>\n",
       "      <td>2.241665713409718</td>\n",
       "      <td>1.03460834229193</td>\n",
       "      <td>0.27367762159942144</td>\n",
       "      <td>-0.03442626482166486</td>\n",
       "      <td>-0.33352591341379445</td>\n",
       "      <td>2.6268775634791734</td>\n",
       "      <td>-2.0100465281749567</td>\n",
       "      <td>1.2987248939888258</td>\n",
       "      <td>-0.02238385011968899</td>\n",
       "      <td>0.5830178509656607</td>\n",
       "      <td>-0.1375156063423236</td>\n",
       "      <td>-0.03156288810640315</td>\n",
       "      <td>1.071406697458374</td>\n",
       "      <td>2.9798753196690733</td>\n",
       "      <td>3.3082957721258666</td>\n",
       "      <td>-2.1320151715044244</td>\n",
       "      <td>0.10630130979398261</td>\n",
       "      <td>-0.03210430562467975</td>\n",
       "      <td>-0.5495903248638354</td>\n",
       "      <td>-1.0489827758132027</td>\n",
       "      <td>-0.004341291297629821</td>\n",
       "      <td>2.4013787650307754</td>\n",
       "      <td>0.45827142016911127</td>\n",
       "      <td>1.0075411892989439</td>\n",
       "      <td>0.624335975522604</td>\n",
       "      <td>0.037032451176477674</td>\n",
       "      <td>-1.64140876418702</td>\n",
       "      <td>-1.1771539846232675</td>\n",
       "      <td>0.8612641060571655</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6105241043590869</td>\n",
       "      <td>-0.7335019619006894</td>\n",
       "      <td>0.5440939964427246</td>\n",
       "      <td>-8.603653176241078</td>\n",
       "      <td>3.142106781126477</td>\n",
       "      <td>-0.2621913384664125</td>\n",
       "      <td>3.6547062900338636</td>\n",
       "      <td>1.2718664075633053</td>\n",
       "      <td>-0.20753209459264896</td>\n",
       "      <td>-0.14925364194552665</td>\n",
       "      <td>1.2000579327207528</td>\n",
       "      <td>-2.826627627750431</td>\n",
       "      <td>0.3617126702745833</td>\n",
       "      <td>0.2550872662485728</td>\n",
       "      <td>-1.004669892330396</td>\n",
       "      <td>0.5244377928091789</td>\n",
       "      <td>1.4422073317693438</td>\n",
       "      <td>1.169864280656566</td>\n",
       "      <td>1.0291192042873833</td>\n",
       "      <td>5.32660490385566</td>\n",
       "      <td>0.2423598218467308</td>\n",
       "      <td>-0.31765719577961216</td>\n",
       "      <td>2.694093035067453</td>\n",
       "      <td>-0.8200569879381142</td>\n",
       "      <td>0.4323748324386488</td>\n",
       "      <td>-2.52801195193798</td>\n",
       "      <td>0.4361646453024043</td>\n",
       "      <td>-4.385683942407756</td>\n",
       "      <td>2.387121786719825</td>\n",
       "      <td>0.8889567630927903</td>\n",
       "      <td>0.23686320369214414</td>\n",
       "      <td>-1.7843004506231512</td>\n",
       "      <td>2.450991981406149</td>\n",
       "      <td>-1.1719382248460153</td>\n",
       "      <td>5.273762630312087</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3517882469718479</td>\n",
       "      <td>0.8486183547859842</td>\n",
       "      <td>2.0185193810849547</td>\n",
       "      <td>2.8631750782749306</td>\n",
       "      <td>-0.8382521390386972</td>\n",
       "      <td>-1.8323113845798258</td>\n",
       "      <td>0.11508048070939716</td>\n",
       "      <td>-0.8871182794103027</td>\n",
       "      <td>-0.45069963880020203</td>\n",
       "      <td>0.7600506392527846</td>\n",
       "      <td>-0.8258693607712766</td>\n",
       "      <td>-4.11292077950246</td>\n",
       "      <td>0.46667267210749463</td>\n",
       "      <td>-1.1122433489030028</td>\n",
       "      <td>2.291544530132535</td>\n",
       "      <td>1.0244512184239927</td>\n",
       "      <td>3.7576492386602447</td>\n",
       "      <td>0.5630644178350591</td>\n",
       "      <td>0.18506120750058808</td>\n",
       "      <td>-6.6180137842633995</td>\n",
       "      <td>-5.640659894215979</td>\n",
       "      <td>0.830811712698912</td>\n",
       "      <td>-1.2329901149896603</td>\n",
       "      <td>-0.12277449697994541</td>\n",
       "      <td>2.3561344758337586</td>\n",
       "      <td>2.6151970649031973</td>\n",
       "      <td>1.8953063443289595</td>\n",
       "      <td>-8.202601941572349</td>\n",
       "      <td>-1.5405878360742586</td>\n",
       "      <td>-1.4554362459532166</td>\n",
       "      <td>-0.24754099013395833</td>\n",
       "      <td>-0.8399495746478683</td>\n",
       "      <td>2.0578413248771374</td>\n",
       "      <td>0.2363877791045608</td>\n",
       "      <td>2.3344976843492478</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2.075348061479394</td>\n",
       "      <td>0.45137226512892303</td>\n",
       "      <td>0.20452182450387804</td>\n",
       "      <td>1.7401028232402376</td>\n",
       "      <td>-1.7215842711723055</td>\n",
       "      <td>-1.0180485185951156</td>\n",
       "      <td>-1.491830380602376</td>\n",
       "      <td>-0.7612264180543937</td>\n",
       "      <td>2.018325375209947</td>\n",
       "      <td>0.9775990876772271</td>\n",
       "      <td>1.4404295920075487</td>\n",
       "      <td>-1.2483396743352393</td>\n",
       "      <td>1.9162494382646522</td>\n",
       "      <td>-1.1483875112963726</td>\n",
       "      <td>-2.8173094288780374</td>\n",
       "      <td>-0.4832005881506155</td>\n",
       "      <td>-3.1638094878762293</td>\n",
       "      <td>1.8337000180829752</td>\n",
       "      <td>0.29706380660044074</td>\n",
       "      <td>1.1699700149567436</td>\n",
       "      <td>-1.7562170501156156</td>\n",
       "      <td>1.3289615292329304</td>\n",
       "      <td>0.14539468951797108</td>\n",
       "      <td>0.5106626274528202</td>\n",
       "      <td>-2.1738107040543637</td>\n",
       "      <td>2.808996826486217</td>\n",
       "      <td>0.08961499714775413</td>\n",
       "      <td>5.027466791875707</td>\n",
       "      <td>-0.2947972572478632</td>\n",
       "      <td>3.5527522844723007</td>\n",
       "      <td>0.4472670134842496</td>\n",
       "      <td>0.5622997875910306</td>\n",
       "      <td>0.6757493786006363</td>\n",
       "      <td>-0.4324510417452522</td>\n",
       "      <td>6.029251456160692</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.2356055068732668</td>\n",
       "      <td>2.220742892838994</td>\n",
       "      <td>0.9797728904364547</td>\n",
       "      <td>-3.285030864295107</td>\n",
       "      <td>-10.029073310899754</td>\n",
       "      <td>0.910847563272922</td>\n",
       "      <td>0.5660318316947659</td>\n",
       "      <td>-0.0339944898463751</td>\n",
       "      <td>0.03639010947505865</td>\n",
       "      <td>0.44978700195448607</td>\n",
       "      <td>-0.22711594614665162</td>\n",
       "      <td>2.161842158976029</td>\n",
       "      <td>0.5060259938882976</td>\n",
       "      <td>1.5609124655693671</td>\n",
       "      <td>-0.9042494192497191</td>\n",
       "      <td>0.12964406135995646</td>\n",
       "      <td>4.438044645675577</td>\n",
       "      <td>0.3585423886335166</td>\n",
       "      <td>0.7508876610445616</td>\n",
       "      <td>3.0724571086440644</td>\n",
       "      <td>0.9465505554545279</td>\n",
       "      <td>2.9315718426126627</td>\n",
       "      <td>1.382102128940184</td>\n",
       "      <td>0.2602232472771473</td>\n",
       "      <td>-1.4023993909327195</td>\n",
       "      <td>-0.3642747911633514</td>\n",
       "      <td>1.9283496233241848</td>\n",
       "      <td>2.2021909037590564</td>\n",
       "      <td>-0.0028010284623413984</td>\n",
       "      <td>3.449943848663294</td>\n",
       "      <td>1.0330499769520591</td>\n",
       "      <td>-2.389514456146183</td>\n",
       "      <td>0.6819515844656311</td>\n",
       "      <td>0.9838665068182113</td>\n",
       "      <td>10.587192812945524</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.8506443631142822</td>\n",
       "      <td>0.2676320311652304</td>\n",
       "      <td>-1.7784854931142369</td>\n",
       "      <td>-5.159883702810021</td>\n",
       "      <td>-4.500460175730115</td>\n",
       "      <td>-0.4708488458459037</td>\n",
       "      <td>2.242517877620033</td>\n",
       "      <td>-0.0343640596895049</td>\n",
       "      <td>0.4122493470237777</td>\n",
       "      <td>-1.675435836504585</td>\n",
       "      <td>0.8762991351698909</td>\n",
       "      <td>2.9018961364973053</td>\n",
       "      <td>-0.03430656904330251</td>\n",
       "      <td>0.2877939344588698</td>\n",
       "      <td>-0.9504647913647432</td>\n",
       "      <td>-0.783600094163428</td>\n",
       "      <td>-0.6000273323364559</td>\n",
       "      <td>0.6668295583976607</td>\n",
       "      <td>0.2972718774740335</td>\n",
       "      <td>-4.061484708183912</td>\n",
       "      <td>-3.8161535814270278</td>\n",
       "      <td>1.020592044797977</td>\n",
       "      <td>0.04183905048114886</td>\n",
       "      <td>-0.956999397611258</td>\n",
       "      <td>3.4582840682130036</td>\n",
       "      <td>1.16583145166447</td>\n",
       "      <td>0.6200418419990028</td>\n",
       "      <td>-7.627836218548753</td>\n",
       "      <td>-1.0761111326176778</td>\n",
       "      <td>-2.7596185235532267</td>\n",
       "      <td>-1.2904629033006747</td>\n",
       "      <td>0.349524685448047</td>\n",
       "      <td>3.4423838457341236</td>\n",
       "      <td>0.7866478283816569</td>\n",
       "      <td>1.019582181337215</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.4232998032414208</td>\n",
       "      <td>-0.06992772099837025</td>\n",
       "      <td>-0.055740171042023196</td>\n",
       "      <td>-2.33369130705232</td>\n",
       "      <td>4.1565842905535035</td>\n",
       "      <td>2.160041518271852</td>\n",
       "      <td>5.028983171380723</td>\n",
       "      <td>0.0504333487618344</td>\n",
       "      <td>-0.11628677493089992</td>\n",
       "      <td>0.14257505201261444</td>\n",
       "      <td>0.14534871059104174</td>\n",
       "      <td>-3.763840517121795</td>\n",
       "      <td>0.49834069908836076</td>\n",
       "      <td>0.267904171387497</td>\n",
       "      <td>-0.07868024996720654</td>\n",
       "      <td>-1.0414680243647731</td>\n",
       "      <td>0.31696304031917577</td>\n",
       "      <td>-0.09150902457057936</td>\n",
       "      <td>-0.8395421433969492</td>\n",
       "      <td>3.1530355479593957</td>\n",
       "      <td>1.1984182560716286</td>\n",
       "      <td>-1.138748806293747</td>\n",
       "      <td>-0.6019524165338564</td>\n",
       "      <td>-0.4364001946206655</td>\n",
       "      <td>3.2806741714050163</td>\n",
       "      <td>-0.5402327389263891</td>\n",
       "      <td>0.18295947848235028</td>\n",
       "      <td>-9.576190030044943</td>\n",
       "      <td>0.8929141082090849</td>\n",
       "      <td>0.6788067663023037</td>\n",
       "      <td>0.8256240109477097</td>\n",
       "      <td>-0.386437822463787</td>\n",
       "      <td>-0.49093534043740317</td>\n",
       "      <td>-0.37513063608497854</td>\n",
       "      <td>2.2098246072607286</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6885516573395772</td>\n",
       "      <td>-0.16967863286453064</td>\n",
       "      <td>0.21475374735087452</td>\n",
       "      <td>-3.079421072563564</td>\n",
       "      <td>0.449819739273847</td>\n",
       "      <td>0.8280376202786597</td>\n",
       "      <td>2.347304679943285</td>\n",
       "      <td>-1.5363421868195597</td>\n",
       "      <td>0.34112632731886783</td>\n",
       "      <td>0.8358327248714632</td>\n",
       "      <td>-0.5468934619594901</td>\n",
       "      <td>3.2738661698150766</td>\n",
       "      <td>0.8517548079055359</td>\n",
       "      <td>1.1353836285809635</td>\n",
       "      <td>1.5544245948295474</td>\n",
       "      <td>1.5492510639146593</td>\n",
       "      <td>-1.7898767749577034</td>\n",
       "      <td>0.8019263211421492</td>\n",
       "      <td>0.5893586372961818</td>\n",
       "      <td>0.8714076333537533</td>\n",
       "      <td>2.0639319534441234</td>\n",
       "      <td>-0.7802189615943217</td>\n",
       "      <td>1.1254523933306169</td>\n",
       "      <td>-0.041385377295665854</td>\n",
       "      <td>-0.7049180176098095</td>\n",
       "      <td>-1.9709212978851007</td>\n",
       "      <td>0.8206452841951103</td>\n",
       "      <td>0.9950091510754292</td>\n",
       "      <td>0.6344723722949713</td>\n",
       "      <td>-1.6201498623991966</td>\n",
       "      <td>-0.9118674672790038</td>\n",
       "      <td>0.065658251362515</td>\n",
       "      <td>0.47085114364991354</td>\n",
       "      <td>-0.39598623045043346</td>\n",
       "      <td>-2.9860747258321156</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1480764254455367</td>\n",
       "      <td>1.4302456845560418</td>\n",
       "      <td>-0.24886097526230308</td>\n",
       "      <td>5.138150920083797</td>\n",
       "      <td>-4.291069058532566</td>\n",
       "      <td>0.5320102297531657</td>\n",
       "      <td>-2.826461738470837</td>\n",
       "      <td>1.0120503932465885</td>\n",
       "      <td>-0.7227957075164658</td>\n",
       "      <td>0.04816494909359469</td>\n",
       "      <td>-1.2047823859480709</td>\n",
       "      <td>-0.4354775834134156</td>\n",
       "      <td>-1.0335247818034208</td>\n",
       "      <td>-0.14210148978187756</td>\n",
       "      <td>0.4457393539325195</td>\n",
       "      <td>1.62591257207219</td>\n",
       "      <td>-4.136359099203966</td>\n",
       "      <td>0.559724898619284</td>\n",
       "      <td>0.1326138357511536</td>\n",
       "      <td>-7.079904562540768</td>\n",
       "      <td>0.04627877158046548</td>\n",
       "      <td>-2.351252037122795</td>\n",
       "      <td>0.2709036968720826</td>\n",
       "      <td>-0.07785897864927996</td>\n",
       "      <td>-3.619565503884099</td>\n",
       "      <td>0.6856896239480154</td>\n",
       "      <td>1.150583564046325</td>\n",
       "      <td>5.391982332168546</td>\n",
       "      <td>-3.366979246956951</td>\n",
       "      <td>3.135943948404593</td>\n",
       "      <td>0.46443470421847655</td>\n",
       "      <td>0.5877902350444172</td>\n",
       "      <td>2.1444274678642614</td>\n",
       "      <td>0.15025608808119673</td>\n",
       "      <td>-0.1960447385307176</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.2685465874545536</td>\n",
       "      <td>-0.4740789212188733</td>\n",
       "      <td>-1.9876485047410841</td>\n",
       "      <td>-0.6148569175390369</td>\n",
       "      <td>-2.720829633662395</td>\n",
       "      <td>-0.3328382554945878</td>\n",
       "      <td>0.6489593864605747</td>\n",
       "      <td>0.16368348470149532</td>\n",
       "      <td>-0.08771676942070941</td>\n",
       "      <td>1.2612051459413292</td>\n",
       "      <td>-2.2045661587637486</td>\n",
       "      <td>1.917620368257194</td>\n",
       "      <td>0.27770785412319443</td>\n",
       "      <td>3.1111689457439486</td>\n",
       "      <td>-0.7099038090744612</td>\n",
       "      <td>0.9517396066983568</td>\n",
       "      <td>-0.8964712090146774</td>\n",
       "      <td>1.0248301327553673</td>\n",
       "      <td>-1.0606312052010969</td>\n",
       "      <td>-1.4653800280407734</td>\n",
       "      <td>3.060001357096573</td>\n",
       "      <td>-1.2009171904334712</td>\n",
       "      <td>-0.6595108049607487</td>\n",
       "      <td>1.8324457937069425</td>\n",
       "      <td>-0.6611942340666496</td>\n",
       "      <td>-0.8740919282363157</td>\n",
       "      <td>0.21394254249916894</td>\n",
       "      <td>0.6041910174061222</td>\n",
       "      <td>-0.8745671279210103</td>\n",
       "      <td>-0.7552202301472909</td>\n",
       "      <td>0.06284668868374386</td>\n",
       "      <td>-0.1913500417873185</td>\n",
       "      <td>-0.0685923327637595</td>\n",
       "      <td>-2.1589869907471293</td>\n",
       "      <td>-1.667757094766148</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2.189586909706462</td>\n",
       "      <td>-0.7882054152142215</td>\n",
       "      <td>0.11330495573366375</td>\n",
       "      <td>-1.2862466268364217</td>\n",
       "      <td>4.412170599605036</td>\n",
       "      <td>-0.5833395186322218</td>\n",
       "      <td>-1.7776450823803942</td>\n",
       "      <td>-1.6656088011105596</td>\n",
       "      <td>0.25633058788256313</td>\n",
       "      <td>0.7009240720468797</td>\n",
       "      <td>-0.2141944399380312</td>\n",
       "      <td>-2.854722427278214</td>\n",
       "      <td>0.18916948994144783</td>\n",
       "      <td>-0.6375885177028534</td>\n",
       "      <td>-0.009694862409361765</td>\n",
       "      <td>1.7457069657602675</td>\n",
       "      <td>1.9635767124129062</td>\n",
       "      <td>-1.395698709828056</td>\n",
       "      <td>0.3474081355843538</td>\n",
       "      <td>4.942007291668665</td>\n",
       "      <td>-1.8230557979405526</td>\n",
       "      <td>5.0632411773972805</td>\n",
       "      <td>-0.37732465705119456</td>\n",
       "      <td>-0.08322514271200768</td>\n",
       "      <td>-0.7169011218076725</td>\n",
       "      <td>-1.896702240844149</td>\n",
       "      <td>0.9793548550360717</td>\n",
       "      <td>5.603374217084929</td>\n",
       "      <td>-0.806632446644984</td>\n",
       "      <td>-0.9891387456710531</td>\n",
       "      <td>0.0788363888486702</td>\n",
       "      <td>-1.4420875900408243</td>\n",
       "      <td>-1.2846877775694432</td>\n",
       "      <td>-0.9008855831633328</td>\n",
       "      <td>-0.7781574435379659</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0586558917064257</td>\n",
       "      <td>-0.5556677841685063</td>\n",
       "      <td>-1.6340988843933264</td>\n",
       "      <td>1.6015076980387664</td>\n",
       "      <td>-2.954000377046785</td>\n",
       "      <td>-0.4120832400623331</td>\n",
       "      <td>-2.562667670764183</td>\n",
       "      <td>0.9000180080633978</td>\n",
       "      <td>1.1281976882881328</td>\n",
       "      <td>-0.5842564487049725</td>\n",
       "      <td>0.5014531906489859</td>\n",
       "      <td>-0.3622884473532183</td>\n",
       "      <td>-0.805747072300128</td>\n",
       "      <td>1.273831153113548</td>\n",
       "      <td>0.3470369932742986</td>\n",
       "      <td>-0.20778328404424076</td>\n",
       "      <td>-0.5266931843797311</td>\n",
       "      <td>1.4879872508427707</td>\n",
       "      <td>-0.04984446028648007</td>\n",
       "      <td>-4.2175837300685375</td>\n",
       "      <td>-0.6115109246846203</td>\n",
       "      <td>-1.559086855296752</td>\n",
       "      <td>-1.5223901430805777</td>\n",
       "      <td>2.354279402197481</td>\n",
       "      <td>-0.34002696888041606</td>\n",
       "      <td>1.3067092962675573</td>\n",
       "      <td>-0.44564814913122686</td>\n",
       "      <td>-0.3871145261858975</td>\n",
       "      <td>-0.46307238242099713</td>\n",
       "      <td>3.384635915910931</td>\n",
       "      <td>-1.554200072922378</td>\n",
       "      <td>0.24810410193904675</td>\n",
       "      <td>2.57721780037592</td>\n",
       "      <td>1.1550149273869594</td>\n",
       "      <td>1.1497009299556398</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1.7862597621132181</td>\n",
       "      <td>0.035625438631779194</td>\n",
       "      <td>4.755263293263475</td>\n",
       "      <td>-4.340625828706606</td>\n",
       "      <td>6.250562786967742</td>\n",
       "      <td>-0.1691700244130309</td>\n",
       "      <td>4.600069133023698</td>\n",
       "      <td>0.4498200623097326</td>\n",
       "      <td>0.7733429649637811</td>\n",
       "      <td>1.4395527904642587</td>\n",
       "      <td>0.39872122018129363</td>\n",
       "      <td>-2.5884145456634835</td>\n",
       "      <td>1.713477792008218</td>\n",
       "      <td>-1.3103932981658883</td>\n",
       "      <td>0.6397205434078544</td>\n",
       "      <td>0.5398050186212242</td>\n",
       "      <td>-3.364655440372976</td>\n",
       "      <td>-0.8252966787252827</td>\n",
       "      <td>-1.3946081912148496</td>\n",
       "      <td>-0.1569810903067758</td>\n",
       "      <td>-6.639339571496608</td>\n",
       "      <td>-1.3533996403294009</td>\n",
       "      <td>-0.8240387261147878</td>\n",
       "      <td>-0.5250701934123424</td>\n",
       "      <td>1.7886465425371672</td>\n",
       "      <td>-1.0258268786749287</td>\n",
       "      <td>-0.06313180457126966</td>\n",
       "      <td>-3.129167856770784</td>\n",
       "      <td>-2.5080646670282163</td>\n",
       "      <td>-1.3873607248775532</td>\n",
       "      <td>-0.12496766088635478</td>\n",
       "      <td>0.5144160590471033</td>\n",
       "      <td>2.6742601798485186</td>\n",
       "      <td>-0.048643902649866094</td>\n",
       "      <td>1.00301490597781</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.466806485758727</td>\n",
       "      <td>0.4864596137186106</td>\n",
       "      <td>1.7433666534125747</td>\n",
       "      <td>-3.4172819304629565</td>\n",
       "      <td>6.178526281539781</td>\n",
       "      <td>1.407651185118566</td>\n",
       "      <td>3.7109553245869447</td>\n",
       "      <td>-1.0374663293519626</td>\n",
       "      <td>1.804587244994382</td>\n",
       "      <td>0.3755931589522066</td>\n",
       "      <td>1.5169038902157035</td>\n",
       "      <td>-1.286837809862283</td>\n",
       "      <td>-0.10989885379219791</td>\n",
       "      <td>-0.4084880659074859</td>\n",
       "      <td>-0.40707085673290677</td>\n",
       "      <td>0.3708379768752844</td>\n",
       "      <td>0.1554783109674808</td>\n",
       "      <td>-0.7994244073710132</td>\n",
       "      <td>-0.0014377351024276452</td>\n",
       "      <td>0.9953317805193768</td>\n",
       "      <td>-1.2656431346334291</td>\n",
       "      <td>-2.246148435498638</td>\n",
       "      <td>0.2671804919323004</td>\n",
       "      <td>0.3719100469096662</td>\n",
       "      <td>0.25026758768378055</td>\n",
       "      <td>0.8307510351343125</td>\n",
       "      <td>0.9255106073203831</td>\n",
       "      <td>-2.2401558273972353</td>\n",
       "      <td>1.22908647174581</td>\n",
       "      <td>-4.3423971600575495</td>\n",
       "      <td>1.4322225501404204</td>\n",
       "      <td>2.5746926762331763</td>\n",
       "      <td>-0.018909775772514154</td>\n",
       "      <td>-0.8521257950923476</td>\n",
       "      <td>-1.0090983135716574</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2.013715452751589</td>\n",
       "      <td>-0.18587430999356616</td>\n",
       "      <td>0.12048236108714586</td>\n",
       "      <td>-2.432144578918184</td>\n",
       "      <td>7.131578810811659</td>\n",
       "      <td>1.2152603616334323</td>\n",
       "      <td>-1.8570265213916781</td>\n",
       "      <td>1.0830374393631774</td>\n",
       "      <td>1.2601309440474435</td>\n",
       "      <td>0.03514561318007499</td>\n",
       "      <td>-0.01807748286282923</td>\n",
       "      <td>2.562931566693169</td>\n",
       "      <td>0.41006067200414287</td>\n",
       "      <td>0.029653047797607024</td>\n",
       "      <td>-2.7715510226316575</td>\n",
       "      <td>-0.1549628150906113</td>\n",
       "      <td>-0.015656866701561162</td>\n",
       "      <td>-0.10921189522577932</td>\n",
       "      <td>-0.42654940840973143</td>\n",
       "      <td>6.457858953762559</td>\n",
       "      <td>-1.4333922673871338</td>\n",
       "      <td>0.5646811622983611</td>\n",
       "      <td>-1.3045441785644938</td>\n",
       "      <td>-0.9948492136822421</td>\n",
       "      <td>2.862954639826133</td>\n",
       "      <td>-0.09121882400241507</td>\n",
       "      <td>0.3400443886058067</td>\n",
       "      <td>3.7648959503264985</td>\n",
       "      <td>2.5206537501932806</td>\n",
       "      <td>-1.253923825796075</td>\n",
       "      <td>0.5141657294567784</td>\n",
       "      <td>0.9641480693023237</td>\n",
       "      <td>-1.647895210361943</td>\n",
       "      <td>-0.25989309220852763</td>\n",
       "      <td>-3.8857150968431786</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.9813103976404522</td>\n",
       "      <td>-0.48277264163287914</td>\n",
       "      <td>1.2100176444856294</td>\n",
       "      <td>6.034800240645673</td>\n",
       "      <td>-13.474822590493725</td>\n",
       "      <td>0.6952896417298624</td>\n",
       "      <td>1.8687765336227389</td>\n",
       "      <td>1.0085428044850617</td>\n",
       "      <td>-0.0920766886161262</td>\n",
       "      <td>0.9235949575964124</td>\n",
       "      <td>0.14585088058234022</td>\n",
       "      <td>2.472282580554513</td>\n",
       "      <td>-0.3205317765448241</td>\n",
       "      <td>-1.3285805391421979</td>\n",
       "      <td>2.461557693405206</td>\n",
       "      <td>-1.001888306288588</td>\n",
       "      <td>1.177932054624348</td>\n",
       "      <td>-2.2890454464000953</td>\n",
       "      <td>-0.42073025749244775</td>\n",
       "      <td>-6.455470074885587</td>\n",
       "      <td>-2.6007154718500973</td>\n",
       "      <td>3.5889221551317494</td>\n",
       "      <td>1.147380639731575</td>\n",
       "      <td>-1.1784768908744205</td>\n",
       "      <td>0.641460277748327</td>\n",
       "      <td>5.423985635669208</td>\n",
       "      <td>0.27326500476344073</td>\n",
       "      <td>-4.79813101972001</td>\n",
       "      <td>-2.8415039186047513</td>\n",
       "      <td>1.6216202470875107</td>\n",
       "      <td>0.24770092445222053</td>\n",
       "      <td>0.23791289918411776</td>\n",
       "      <td>0.08164770210929465</td>\n",
       "      <td>-0.7060728495741068</td>\n",
       "      <td>8.209852007425441</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8903384311382999</td>\n",
       "      <td>0.6986974580770456</td>\n",
       "      <td>-3.200728509371963</td>\n",
       "      <td>-2.1556075086697324</td>\n",
       "      <td>-4.779074332137604</td>\n",
       "      <td>0.5844230624459069</td>\n",
       "      <td>2.9027082006114924</td>\n",
       "      <td>1.2704679384290523</td>\n",
       "      <td>-0.5401266123537254</td>\n",
       "      <td>-2.063037858185228</td>\n",
       "      <td>0.7903295406091944</td>\n",
       "      <td>0.4169802204854367</td>\n",
       "      <td>1.926703914834181</td>\n",
       "      <td>2.1480623386104476</td>\n",
       "      <td>1.4455094658778886</td>\n",
       "      <td>1.2706416265329488</td>\n",
       "      <td>-1.0973750429191869</td>\n",
       "      <td>-0.5417091182913282</td>\n",
       "      <td>1.6659316237203494</td>\n",
       "      <td>-1.9329764779195173</td>\n",
       "      <td>0.41818539868004667</td>\n",
       "      <td>2.0982118175513342</td>\n",
       "      <td>1.505075163484111</td>\n",
       "      <td>0.8906083476658132</td>\n",
       "      <td>0.6653811619719613</td>\n",
       "      <td>1.5792276809614492</td>\n",
       "      <td>-0.13972283280533276</td>\n",
       "      <td>-6.441037274929541</td>\n",
       "      <td>-0.02394248164985399</td>\n",
       "      <td>0.4311903549757613</td>\n",
       "      <td>0.4680647463651283</td>\n",
       "      <td>-1.3581526916322613</td>\n",
       "      <td>1.81660061990504</td>\n",
       "      <td>-1.013014300051717</td>\n",
       "      <td>2.791671065052714</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.3100718715580255</td>\n",
       "      <td>-0.8829662574155899</td>\n",
       "      <td>-0.7046373205748303</td>\n",
       "      <td>-0.7068768086846816</td>\n",
       "      <td>-7.781644244055439</td>\n",
       "      <td>1.0615469512372824</td>\n",
       "      <td>1.3976856186302782</td>\n",
       "      <td>0.10920882732590623</td>\n",
       "      <td>0.7748772149266269</td>\n",
       "      <td>1.007049225269684</td>\n",
       "      <td>-2.598550376477518</td>\n",
       "      <td>-0.15755296933968008</td>\n",
       "      <td>0.09961631292645666</td>\n",
       "      <td>-0.3824212326968105</td>\n",
       "      <td>-1.1189424681420006</td>\n",
       "      <td>-0.31312012262965516</td>\n",
       "      <td>2.4068822588636807</td>\n",
       "      <td>-0.11261226091407048</td>\n",
       "      <td>-0.5616981467221734</td>\n",
       "      <td>-1.6308427528615626</td>\n",
       "      <td>1.5865632500099391</td>\n",
       "      <td>0.05066117727284772</td>\n",
       "      <td>-0.6095846348800507</td>\n",
       "      <td>-0.4049324672230167</td>\n",
       "      <td>-1.126107200809527</td>\n",
       "      <td>-0.5484863648620234</td>\n",
       "      <td>0.9026717713030781</td>\n",
       "      <td>0.7020963885626733</td>\n",
       "      <td>-3.595560533973377</td>\n",
       "      <td>1.2922431587749987</td>\n",
       "      <td>-0.6189210153474656</td>\n",
       "      <td>-1.0482285682325367</td>\n",
       "      <td>-0.15088628999624906</td>\n",
       "      <td>0.155673263976993</td>\n",
       "      <td>5.378694758171784</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.597222086109405</td>\n",
       "      <td>1.1334442475129294</td>\n",
       "      <td>-0.1330998162918787</td>\n",
       "      <td>-0.28371349011563496</td>\n",
       "      <td>2.1121770904527506</td>\n",
       "      <td>-0.4010725866177972</td>\n",
       "      <td>2.165614697729135</td>\n",
       "      <td>2.1074922048297973</td>\n",
       "      <td>0.033722718466987794</td>\n",
       "      <td>-1.9912838386154217</td>\n",
       "      <td>-0.5034523462491987</td>\n",
       "      <td>-1.445496018331568</td>\n",
       "      <td>-0.004194218704708624</td>\n",
       "      <td>0.3829481007684855</td>\n",
       "      <td>1.4111659491075945</td>\n",
       "      <td>0.6203733899191238</td>\n",
       "      <td>-3.1535924476574837</td>\n",
       "      <td>-0.3741970934455955</td>\n",
       "      <td>-0.6891605179480794</td>\n",
       "      <td>-0.7575420574679261</td>\n",
       "      <td>-2.0908328156534406</td>\n",
       "      <td>-0.9616963946761988</td>\n",
       "      <td>-0.07196910582067989</td>\n",
       "      <td>0.2393313406777689</td>\n",
       "      <td>2.994192625705967</td>\n",
       "      <td>1.1907391280019706</td>\n",
       "      <td>0.9679227523174557</td>\n",
       "      <td>-7.38264969518866</td>\n",
       "      <td>0.4284836002298431</td>\n",
       "      <td>3.0971901448289665</td>\n",
       "      <td>0.5046859142838822</td>\n",
       "      <td>1.971076587744378</td>\n",
       "      <td>2.1615971158775236</td>\n",
       "      <td>1.212945738781197</td>\n",
       "      <td>1.17331180091785</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8280317690188265</td>\n",
       "      <td>-0.48889609362200304</td>\n",
       "      <td>0.31765145723535726</td>\n",
       "      <td>2.654649827446791</td>\n",
       "      <td>-6.948958209834084</td>\n",
       "      <td>2.1265988133740406</td>\n",
       "      <td>-1.5816028034207332</td>\n",
       "      <td>-0.2734707628490536</td>\n",
       "      <td>-1.645680234409779</td>\n",
       "      <td>-0.6407942894058258</td>\n",
       "      <td>0.6969489656752149</td>\n",
       "      <td>0.9929224933340147</td>\n",
       "      <td>-0.7590530539748105</td>\n",
       "      <td>1.0714019100059533</td>\n",
       "      <td>2.314722643317221</td>\n",
       "      <td>1.4371002634430396</td>\n",
       "      <td>3.5936623101404703</td>\n",
       "      <td>-0.8860713366733567</td>\n",
       "      <td>0.13058487014132097</td>\n",
       "      <td>0.613508597000374</td>\n",
       "      <td>2.815877108325811</td>\n",
       "      <td>1.8857827466803032</td>\n",
       "      <td>0.24098612179396378</td>\n",
       "      <td>-1.6439399226423361</td>\n",
       "      <td>-2.663042099611651</td>\n",
       "      <td>0.6803182639819778</td>\n",
       "      <td>0.6492100115348427</td>\n",
       "      <td>3.2574888306785765</td>\n",
       "      <td>1.0732076273334608</td>\n",
       "      <td>4.039487072132357</td>\n",
       "      <td>0.14569233726882358</td>\n",
       "      <td>-1.6192022608409509</td>\n",
       "      <td>-0.36772664507595965</td>\n",
       "      <td>1.228658488757667</td>\n",
       "      <td>5.435992194737465</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7592531994726568</td>\n",
       "      <td>-1.1182774198582695</td>\n",
       "      <td>-0.7400768946572848</td>\n",
       "      <td>-5.632356240677991</td>\n",
       "      <td>-3.355104685892391</td>\n",
       "      <td>1.0875928297870379</td>\n",
       "      <td>0.9416692407251334</td>\n",
       "      <td>1.8375143240159362</td>\n",
       "      <td>1.508355120266887</td>\n",
       "      <td>0.15544075181361283</td>\n",
       "      <td>-1.315693573827952</td>\n",
       "      <td>0.20775798390808345</td>\n",
       "      <td>-0.5809770435838787</td>\n",
       "      <td>0.21010192871626804</td>\n",
       "      <td>-2.8197940934744894</td>\n",
       "      <td>1.3933333979147235</td>\n",
       "      <td>-1.0918223138471748</td>\n",
       "      <td>-1.2553092131369086</td>\n",
       "      <td>1.0755233919356098</td>\n",
       "      <td>-0.30822852052276106</td>\n",
       "      <td>-2.947666321302357</td>\n",
       "      <td>0.8734491290142373</td>\n",
       "      <td>-0.24341912318410117</td>\n",
       "      <td>-1.4883429909113477</td>\n",
       "      <td>-0.9396887169247236</td>\n",
       "      <td>3.3431419480905884</td>\n",
       "      <td>1.2122576512447376</td>\n",
       "      <td>-3.082666953618551</td>\n",
       "      <td>3.1624006587275346</td>\n",
       "      <td>0.500655085041283</td>\n",
       "      <td>1.1782390571266803</td>\n",
       "      <td>0.6961958341223696</td>\n",
       "      <td>4.52156280376291</td>\n",
       "      <td>1.0595201867106254</td>\n",
       "      <td>8.33233879728386</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3940024028691185</td>\n",
       "      <td>-0.33445539878392805</td>\n",
       "      <td>2.027188946507427</td>\n",
       "      <td>2.261478787655949</td>\n",
       "      <td>-1.7231651056388275</td>\n",
       "      <td>0.1597903772525295</td>\n",
       "      <td>0.7669667022051557</td>\n",
       "      <td>1.417592562803678</td>\n",
       "      <td>0.7276736852596607</td>\n",
       "      <td>0.002439261226376138</td>\n",
       "      <td>0.8424081986778317</td>\n",
       "      <td>-0.34871066047306487</td>\n",
       "      <td>0.15726930851059437</td>\n",
       "      <td>-0.07015062935661162</td>\n",
       "      <td>1.0158330652920038</td>\n",
       "      <td>0.8120744904572086</td>\n",
       "      <td>0.9051740724523839</td>\n",
       "      <td>-1.3610658999297116</td>\n",
       "      <td>-1.0463004749334435</td>\n",
       "      <td>0.059444197770049764</td>\n",
       "      <td>0.0775409623523731</td>\n",
       "      <td>0.9464512597325877</td>\n",
       "      <td>0.6425258556490333</td>\n",
       "      <td>0.5353710219745076</td>\n",
       "      <td>-1.4624124933541212</td>\n",
       "      <td>2.1845000587690397</td>\n",
       "      <td>0.890118632529928</td>\n",
       "      <td>-0.07746465900430452</td>\n",
       "      <td>1.6408316699427252</td>\n",
       "      <td>0.4605310325983655</td>\n",
       "      <td>-1.0256097170909406</td>\n",
       "      <td>0.36297228256529623</td>\n",
       "      <td>-0.21220464600118424</td>\n",
       "      <td>-0.5307194153703407</td>\n",
       "      <td>4.029525138125566</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5148789918733742</td>\n",
       "      <td>-0.9138218249492241</td>\n",
       "      <td>0.40744485384645357</td>\n",
       "      <td>2.437279398835636</td>\n",
       "      <td>5.5002250319942085</td>\n",
       "      <td>-0.6445128260432557</td>\n",
       "      <td>2.6712093344595957</td>\n",
       "      <td>0.2359651938081096</td>\n",
       "      <td>-1.2564765451339706</td>\n",
       "      <td>1.497879673804852</td>\n",
       "      <td>-1.0646800563354735</td>\n",
       "      <td>-4.943882473907434</td>\n",
       "      <td>0.12212184647245884</td>\n",
       "      <td>0.1513733743125254</td>\n",
       "      <td>3.628274431825756</td>\n",
       "      <td>0.25195461332781827</td>\n",
       "      <td>-2.6253761643457865</td>\n",
       "      <td>-0.05998943148713057</td>\n",
       "      <td>0.7587803996075067</td>\n",
       "      <td>-5.462940781912974</td>\n",
       "      <td>-6.260739158950308</td>\n",
       "      <td>0.41542894546421294</td>\n",
       "      <td>0.015534066028263474</td>\n",
       "      <td>0.6330171485528483</td>\n",
       "      <td>4.865544473209683</td>\n",
       "      <td>0.35445390996927517</td>\n",
       "      <td>-0.4537599089433162</td>\n",
       "      <td>-9.410709508728909</td>\n",
       "      <td>-5.011719983891007</td>\n",
       "      <td>-0.9469126451506257</td>\n",
       "      <td>1.4862056106481196</td>\n",
       "      <td>-0.12542944122533664</td>\n",
       "      <td>1.2397466590289297</td>\n",
       "      <td>1.010092736770287</td>\n",
       "      <td>-4.62157143796051</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1170</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.9343885717308088</td>\n",
       "      <td>1.5271086405761165</td>\n",
       "      <td>-0.4435803340155843</td>\n",
       "      <td>-5.9337637890235095</td>\n",
       "      <td>-0.9616637278789445</td>\n",
       "      <td>-0.17279663799306935</td>\n",
       "      <td>4.9485797478162015</td>\n",
       "      <td>0.5567235640732643</td>\n",
       "      <td>0.8976313155683101</td>\n",
       "      <td>-0.31703987488417973</td>\n",
       "      <td>-0.8745022769774832</td>\n",
       "      <td>3.2190876548974106</td>\n",
       "      <td>1.7679814920512915</td>\n",
       "      <td>1.5994377639953254</td>\n",
       "      <td>-0.32822654301588805</td>\n",
       "      <td>1.8704785658619376</td>\n",
       "      <td>-1.2968564257886397</td>\n",
       "      <td>-0.04798919578834268</td>\n",
       "      <td>0.20658789102264802</td>\n",
       "      <td>2.6088546979336082</td>\n",
       "      <td>3.8392669146567373</td>\n",
       "      <td>-0.5172889072828292</td>\n",
       "      <td>-0.3502459667363689</td>\n",
       "      <td>-0.4905618925528823</td>\n",
       "      <td>-0.9527474220684239</td>\n",
       "      <td>-0.9422054036750068</td>\n",
       "      <td>-0.36374559425956154</td>\n",
       "      <td>-2.999746006816707</td>\n",
       "      <td>3.233247508721702</td>\n",
       "      <td>-3.0461663541450665</td>\n",
       "      <td>1.4885951744762722</td>\n",
       "      <td>-0.8417279864979065</td>\n",
       "      <td>0.6741085179044058</td>\n",
       "      <td>-1.1316165313462891</td>\n",
       "      <td>0.928810487974822</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>1171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8395911804295536</td>\n",
       "      <td>-1.168388034096431</td>\n",
       "      <td>0.7218414779595365</td>\n",
       "      <td>5.579491785516002</td>\n",
       "      <td>-3.3599702849269355</td>\n",
       "      <td>-0.7341527069333492</td>\n",
       "      <td>-0.5024380396725944</td>\n",
       "      <td>1.1925647791554592</td>\n",
       "      <td>-1.6713478134411535</td>\n",
       "      <td>0.9116232828082541</td>\n",
       "      <td>1.517650315304898</td>\n",
       "      <td>-0.33959340397101434</td>\n",
       "      <td>-1.1065237769479985</td>\n",
       "      <td>0.14831133004977215</td>\n",
       "      <td>1.773549237075518</td>\n",
       "      <td>0.5294697741134579</td>\n",
       "      <td>-2.200681857803975</td>\n",
       "      <td>-0.09005738913774403</td>\n",
       "      <td>0.5116671387793217</td>\n",
       "      <td>-2.16385196883732</td>\n",
       "      <td>1.15417355961463</td>\n",
       "      <td>-0.1511143750754613</td>\n",
       "      <td>0.5733821899190933</td>\n",
       "      <td>-0.41024783051540026</td>\n",
       "      <td>-2.1491245789132782</td>\n",
       "      <td>1.1623223957566267</td>\n",
       "      <td>-0.3203555175230647</td>\n",
       "      <td>3.3678611076317053</td>\n",
       "      <td>-1.9710507897923804</td>\n",
       "      <td>3.6645312947360176</td>\n",
       "      <td>-0.18758587987494516</td>\n",
       "      <td>0.5375661437236665</td>\n",
       "      <td>-0.5902419322633622</td>\n",
       "      <td>0.0615232658263072</td>\n",
       "      <td>1.7135041753730722</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1172</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4070003517804133</td>\n",
       "      <td>0.062108110144113715</td>\n",
       "      <td>-0.6912463640611776</td>\n",
       "      <td>-7.676674455605033</td>\n",
       "      <td>-1.1157112760666261</td>\n",
       "      <td>0.8190765362973288</td>\n",
       "      <td>4.444972268213096</td>\n",
       "      <td>1.539244817802371</td>\n",
       "      <td>-0.16068257940158517</td>\n",
       "      <td>-0.7986866278935425</td>\n",
       "      <td>-0.8830285528488023</td>\n",
       "      <td>5.441334362552395</td>\n",
       "      <td>-0.04371642368474271</td>\n",
       "      <td>0.8578527904940038</td>\n",
       "      <td>0.1645615431220804</td>\n",
       "      <td>0.4705634311050222</td>\n",
       "      <td>0.10382443153643509</td>\n",
       "      <td>-0.8986591447171908</td>\n",
       "      <td>-0.1225003679299076</td>\n",
       "      <td>5.4983254565611075</td>\n",
       "      <td>-0.5287277944767154</td>\n",
       "      <td>-0.11668337678389559</td>\n",
       "      <td>-0.7867659220684182</td>\n",
       "      <td>-0.05690055303298877</td>\n",
       "      <td>5.185219222745553</td>\n",
       "      <td>-2.073045100611649</td>\n",
       "      <td>-0.10661541671084931</td>\n",
       "      <td>-3.132837406005032</td>\n",
       "      <td>-1.3217134605781062</td>\n",
       "      <td>1.5685251131096432</td>\n",
       "      <td>-0.025732128748592514</td>\n",
       "      <td>-0.20640119980647673</td>\n",
       "      <td>-0.28887015185244425</td>\n",
       "      <td>-0.9109585404926773</td>\n",
       "      <td>0.39234548759550336</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.5097668155359814</td>\n",
       "      <td>-2.248926407043649</td>\n",
       "      <td>2.4676065050911338</td>\n",
       "      <td>-4.758676565480606</td>\n",
       "      <td>1.2035829492444905</td>\n",
       "      <td>0.7703443315699603</td>\n",
       "      <td>0.7754141968544236</td>\n",
       "      <td>0.8467339640103299</td>\n",
       "      <td>0.4430117792380237</td>\n",
       "      <td>0.4534299417404605</td>\n",
       "      <td>-1.1677908231581569</td>\n",
       "      <td>0.618170200596676</td>\n",
       "      <td>-0.09085463866265511</td>\n",
       "      <td>-0.21075473350188842</td>\n",
       "      <td>-3.4446056258032005</td>\n",
       "      <td>-0.3701809909948538</td>\n",
       "      <td>1.6107987781120632</td>\n",
       "      <td>1.6630900574498795</td>\n",
       "      <td>-0.04369903955902052</td>\n",
       "      <td>7.168841804875031</td>\n",
       "      <td>0.25392543610773743</td>\n",
       "      <td>0.17588077723926746</td>\n",
       "      <td>-0.7344894500547824</td>\n",
       "      <td>0.1911851654682032</td>\n",
       "      <td>0.4543033336476663</td>\n",
       "      <td>-3.473350091870199</td>\n",
       "      <td>-0.7803410180485866</td>\n",
       "      <td>5.232744900173913</td>\n",
       "      <td>-0.8287102346401033</td>\n",
       "      <td>1.253693346689272</td>\n",
       "      <td>0.27359933121174995</td>\n",
       "      <td>-0.9966156655049195</td>\n",
       "      <td>-1.0983497779321312</td>\n",
       "      <td>0.11140745134835475</td>\n",
       "      <td>3.523976843010388</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>1174</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.285147739860543</td>\n",
       "      <td>1.010889584045589</td>\n",
       "      <td>-4.710336727361581</td>\n",
       "      <td>-4.949070099407876</td>\n",
       "      <td>0.3166388494703435</td>\n",
       "      <td>-0.7475031584943358</td>\n",
       "      <td>2.742278390212407</td>\n",
       "      <td>1.2820406201717525</td>\n",
       "      <td>-1.136898536378983</td>\n",
       "      <td>0.302487179610313</td>\n",
       "      <td>-0.8576396156209122</td>\n",
       "      <td>-0.39527493063536745</td>\n",
       "      <td>-0.4505777841517975</td>\n",
       "      <td>-0.14186875049607517</td>\n",
       "      <td>0.28718343843631666</td>\n",
       "      <td>3.107393897873795</td>\n",
       "      <td>-2.8068849922584627</td>\n",
       "      <td>0.4523159663943246</td>\n",
       "      <td>0.8254876299113421</td>\n",
       "      <td>0.6484722146967312</td>\n",
       "      <td>1.611839361148677</td>\n",
       "      <td>2.7164397705838916</td>\n",
       "      <td>-0.13551810623672847</td>\n",
       "      <td>-0.8793058582543944</td>\n",
       "      <td>0.6950269419695012</td>\n",
       "      <td>0.18523428556903135</td>\n",
       "      <td>-1.0403904314211643</td>\n",
       "      <td>-7.46213282096412</td>\n",
       "      <td>2.807925986558534</td>\n",
       "      <td>-1.5173159357105397</td>\n",
       "      <td>1.5454919362855155</td>\n",
       "      <td>1.6337186215031823</td>\n",
       "      <td>2.4243348895779038</td>\n",
       "      <td>-0.4855176968345315</td>\n",
       "      <td>-0.35548022210897756</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>1175</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.2755590478870014</td>\n",
       "      <td>2.028400829727336</td>\n",
       "      <td>0.07563808645012751</td>\n",
       "      <td>9.173940417393867</td>\n",
       "      <td>1.5500203094679565</td>\n",
       "      <td>-3.0087677417812393</td>\n",
       "      <td>0.14950577389569744</td>\n",
       "      <td>0.9061701969680805</td>\n",
       "      <td>1.39782593082273</td>\n",
       "      <td>1.077573547898431</td>\n",
       "      <td>-0.5305936941620435</td>\n",
       "      <td>1.8854023322068052</td>\n",
       "      <td>0.12485899994861317</td>\n",
       "      <td>-1.0547796721533536</td>\n",
       "      <td>5.710290841815588</td>\n",
       "      <td>-0.3392036769370268</td>\n",
       "      <td>-4.400349299549187</td>\n",
       "      <td>-0.4684783477168748</td>\n",
       "      <td>-1.0542451400733732</td>\n",
       "      <td>-6.92949937364045</td>\n",
       "      <td>-1.7136818426314355</td>\n",
       "      <td>0.04215985095240107</td>\n",
       "      <td>0.3225850634827796</td>\n",
       "      <td>-0.6358997981161512</td>\n",
       "      <td>3.6945263304256333</td>\n",
       "      <td>3.48054050765064</td>\n",
       "      <td>-1.5013616422848837</td>\n",
       "      <td>-7.583622170759153</td>\n",
       "      <td>0.0006612452223352661</td>\n",
       "      <td>-0.28323804216496995</td>\n",
       "      <td>1.1847127558323403</td>\n",
       "      <td>0.43849439821266845</td>\n",
       "      <td>-0.2886388954201039</td>\n",
       "      <td>1.0802157525822464</td>\n",
       "      <td>-7.880462353590589</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>1176</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1742311001490733</td>\n",
       "      <td>-0.9156767520472041</td>\n",
       "      <td>-2.0817337873846644</td>\n",
       "      <td>7.501884852634827</td>\n",
       "      <td>3.257319706662676</td>\n",
       "      <td>0.35474673599673145</td>\n",
       "      <td>-0.7856021368881816</td>\n",
       "      <td>0.8256207749000923</td>\n",
       "      <td>0.9466983329354152</td>\n",
       "      <td>0.20756420125298924</td>\n",
       "      <td>-0.323115108421562</td>\n",
       "      <td>0.2538087589362772</td>\n",
       "      <td>-0.3436394963180437</td>\n",
       "      <td>-0.8647656453532908</td>\n",
       "      <td>3.0770896405116566</td>\n",
       "      <td>-1.8836858755351107</td>\n",
       "      <td>-3.9447494065054465</td>\n",
       "      <td>-0.5551711613853215</td>\n",
       "      <td>0.7982087057273198</td>\n",
       "      <td>-0.5363786585608814</td>\n",
       "      <td>4.157478775449823</td>\n",
       "      <td>0.8009199443901573</td>\n",
       "      <td>-1.7309246916445078</td>\n",
       "      <td>-1.4201358748782489</td>\n",
       "      <td>0.2914013381071281</td>\n",
       "      <td>0.4681932192268057</td>\n",
       "      <td>0.9798767828783856</td>\n",
       "      <td>-0.49639672883641206</td>\n",
       "      <td>0.9770737348591859</td>\n",
       "      <td>0.6972072661827226</td>\n",
       "      <td>-0.4891005238723114</td>\n",
       "      <td>0.286950675823082</td>\n",
       "      <td>-2.697097495463281</td>\n",
       "      <td>0.9842372868381063</td>\n",
       "      <td>-7.5310253802046265</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>1177</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.631598837828945</td>\n",
       "      <td>-0.3251869982665415</td>\n",
       "      <td>3.6317078857933813</td>\n",
       "      <td>1.1334624416146055</td>\n",
       "      <td>-7.501099171567779</td>\n",
       "      <td>1.117066470694563</td>\n",
       "      <td>1.905787485975863</td>\n",
       "      <td>-1.567214693616842</td>\n",
       "      <td>-2.1846922331218104</td>\n",
       "      <td>0.9501961584766616</td>\n",
       "      <td>-0.5950071375396341</td>\n",
       "      <td>3.0746386973069204</td>\n",
       "      <td>-0.19108995444383123</td>\n",
       "      <td>-0.4486716284181457</td>\n",
       "      <td>-0.7574053969399407</td>\n",
       "      <td>0.3567017604671636</td>\n",
       "      <td>0.7520002074264485</td>\n",
       "      <td>2.217920622936063</td>\n",
       "      <td>-0.4802502387912909</td>\n",
       "      <td>5.269317648418774</td>\n",
       "      <td>0.19563636274939325</td>\n",
       "      <td>3.197123118624416</td>\n",
       "      <td>1.2728868496894543</td>\n",
       "      <td>-0.22206224224085916</td>\n",
       "      <td>-0.34146429211857565</td>\n",
       "      <td>0.7501068504421241</td>\n",
       "      <td>-0.009303098055854164</td>\n",
       "      <td>4.086263274244224</td>\n",
       "      <td>-0.9733416641563307</td>\n",
       "      <td>4.344466269491614</td>\n",
       "      <td>1.9584961063681623</td>\n",
       "      <td>1.424045371779212</td>\n",
       "      <td>-2.096262454711408</td>\n",
       "      <td>1.344993820313971</td>\n",
       "      <td>9.573596199615139</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>1178</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.501999615495122</td>\n",
       "      <td>0.1844090034148628</td>\n",
       "      <td>-2.100063380558122</td>\n",
       "      <td>-1.7810923384709374</td>\n",
       "      <td>-3.207870198374697</td>\n",
       "      <td>0.5320642623573143</td>\n",
       "      <td>1.8956191014065429</td>\n",
       "      <td>0.3779393007703429</td>\n",
       "      <td>0.5285223381199159</td>\n",
       "      <td>-0.7255727741729106</td>\n",
       "      <td>-0.98087970760598</td>\n",
       "      <td>-1.9107613459254056</td>\n",
       "      <td>1.2965542513769552</td>\n",
       "      <td>-0.7026111854966822</td>\n",
       "      <td>0.5831288659612176</td>\n",
       "      <td>1.8500732588426205</td>\n",
       "      <td>2.1569532974067305</td>\n",
       "      <td>0.771323973482618</td>\n",
       "      <td>0.26390809689502115</td>\n",
       "      <td>-1.332551699484598</td>\n",
       "      <td>0.7344140042034013</td>\n",
       "      <td>1.8929119164492225</td>\n",
       "      <td>-0.30018639677761494</td>\n",
       "      <td>2.3507564630097377</td>\n",
       "      <td>0.7016612633909126</td>\n",
       "      <td>-1.4799858201181846</td>\n",
       "      <td>0.7076003897006045</td>\n",
       "      <td>-3.6709842954254985</td>\n",
       "      <td>-2.821301113555421</td>\n",
       "      <td>-0.8110749185916141</td>\n",
       "      <td>-0.4787850260814426</td>\n",
       "      <td>-0.3990563746704292</td>\n",
       "      <td>0.18085087991487883</td>\n",
       "      <td>0.30481925433822266</td>\n",
       "      <td>1.1486171393884104</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>1179</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3549601192871137</td>\n",
       "      <td>0.5093663447647194</td>\n",
       "      <td>0.870628211799776</td>\n",
       "      <td>6.839038756055286</td>\n",
       "      <td>6.244724369179147</td>\n",
       "      <td>1.036326174867773</td>\n",
       "      <td>-3.4005230552311088</td>\n",
       "      <td>-1.264071607647027</td>\n",
       "      <td>2.066097290412573</td>\n",
       "      <td>0.5406876506228713</td>\n",
       "      <td>1.2068698860452327</td>\n",
       "      <td>-2.3080747502335917</td>\n",
       "      <td>1.3960888105914664</td>\n",
       "      <td>0.8992406128911524</td>\n",
       "      <td>3.6383788085526763</td>\n",
       "      <td>0.5716145069176645</td>\n",
       "      <td>-1.0116457817083477</td>\n",
       "      <td>-1.6798892052328613</td>\n",
       "      <td>0.7251096758267004</td>\n",
       "      <td>-4.766880390837934</td>\n",
       "      <td>-4.3227454549937105</td>\n",
       "      <td>-0.20944529515690222</td>\n",
       "      <td>-1.6721884962117863</td>\n",
       "      <td>0.06633932279885599</td>\n",
       "      <td>2.638043563753404</td>\n",
       "      <td>2.0650588396666834</td>\n",
       "      <td>0.6775389574302096</td>\n",
       "      <td>-2.668754940790324</td>\n",
       "      <td>-0.15420480739908368</td>\n",
       "      <td>0.3458259073350073</td>\n",
       "      <td>-2.254917614549761</td>\n",
       "      <td>0.1682578038866202</td>\n",
       "      <td>0.6356762035436628</td>\n",
       "      <td>-0.5741744993929256</td>\n",
       "      <td>-6.648817829603208</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2757699071676885</td>\n",
       "      <td>0.3606846574028201</td>\n",
       "      <td>2.6328238482106956</td>\n",
       "      <td>3.485539825620019</td>\n",
       "      <td>12.202579133530863</td>\n",
       "      <td>2.7556865859815507</td>\n",
       "      <td>-3.4655100072209386</td>\n",
       "      <td>-1.1435347617525058</td>\n",
       "      <td>-0.6164970340708004</td>\n",
       "      <td>-0.024971807504163085</td>\n",
       "      <td>-0.45416620045024747</td>\n",
       "      <td>-0.2949092991340999</td>\n",
       "      <td>0.6543345126296674</td>\n",
       "      <td>-1.1315097611901677</td>\n",
       "      <td>4.547168422816</td>\n",
       "      <td>0.7613780771045985</td>\n",
       "      <td>0.1497505187396261</td>\n",
       "      <td>-0.27481145451137456</td>\n",
       "      <td>0.5709358316798155</td>\n",
       "      <td>-0.38920623199588655</td>\n",
       "      <td>-2.0901177902896775</td>\n",
       "      <td>-2.496758589514485</td>\n",
       "      <td>-0.6321984127415937</td>\n",
       "      <td>-0.8634197858922311</td>\n",
       "      <td>2.5325837748171933</td>\n",
       "      <td>-1.812824334651895</td>\n",
       "      <td>-1.0936137676644242</td>\n",
       "      <td>1.0650951962138904</td>\n",
       "      <td>1.9780666975451489</td>\n",
       "      <td>-0.9492018698243917</td>\n",
       "      <td>0.10668027699532659</td>\n",
       "      <td>1.007085316087386</td>\n",
       "      <td>-0.15925911135486878</td>\n",
       "      <td>-0.06636545511788838</td>\n",
       "      <td>-11.818504194414974</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>1181</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0869037393644914</td>\n",
       "      <td>0.3669177217748294</td>\n",
       "      <td>-0.020739883750392124</td>\n",
       "      <td>0.004005818030604319</td>\n",
       "      <td>2.925164421662623</td>\n",
       "      <td>-0.9877572767886712</td>\n",
       "      <td>3.700225720248489</td>\n",
       "      <td>0.5742893176828693</td>\n",
       "      <td>0.411309551030833</td>\n",
       "      <td>-0.027638249570701046</td>\n",
       "      <td>-0.2963547026992954</td>\n",
       "      <td>2.4335367324992077</td>\n",
       "      <td>-0.05987898731290267</td>\n",
       "      <td>1.889175609061096</td>\n",
       "      <td>3.215582182144786</td>\n",
       "      <td>0.3352501674849724</td>\n",
       "      <td>-0.46328314385542957</td>\n",
       "      <td>-0.9927201613296553</td>\n",
       "      <td>0.11446695549453945</td>\n",
       "      <td>4.510088399098215</td>\n",
       "      <td>6.911653115484638</td>\n",
       "      <td>1.247801935761855</td>\n",
       "      <td>1.218737737261192</td>\n",
       "      <td>-0.4602838589532928</td>\n",
       "      <td>-1.4569160399066596</td>\n",
       "      <td>-2.1164246789172045</td>\n",
       "      <td>-1.668745775418122</td>\n",
       "      <td>-0.2821166385601971</td>\n",
       "      <td>3.673722535956881</td>\n",
       "      <td>-3.4131436183933355</td>\n",
       "      <td>-0.15784945828762456</td>\n",
       "      <td>-0.6727397460191185</td>\n",
       "      <td>-3.1476555367824006</td>\n",
       "      <td>-0.43474105957744624</td>\n",
       "      <td>-4.977556003641755</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1182</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6263599205771264</td>\n",
       "      <td>1.1258149991248585</td>\n",
       "      <td>3.701070213937797</td>\n",
       "      <td>2.9873339762390723</td>\n",
       "      <td>-2.864835489485059</td>\n",
       "      <td>1.3946783101356994</td>\n",
       "      <td>-0.6305853027186651</td>\n",
       "      <td>0.3922432007246808</td>\n",
       "      <td>-1.073998237883779</td>\n",
       "      <td>-1.5914545525178225</td>\n",
       "      <td>0.9618468556898917</td>\n",
       "      <td>5.074992500157688</td>\n",
       "      <td>0.26934073538821207</td>\n",
       "      <td>-1.0537310487250304</td>\n",
       "      <td>3.213724879732177</td>\n",
       "      <td>0.40308574389718643</td>\n",
       "      <td>-0.7169527775060137</td>\n",
       "      <td>0.9249819291346241</td>\n",
       "      <td>0.6523556195231468</td>\n",
       "      <td>-2.1226421724295106</td>\n",
       "      <td>-0.6993142218365305</td>\n",
       "      <td>-2.6413021252474738</td>\n",
       "      <td>-0.6634639134840192</td>\n",
       "      <td>1.0167203846556607</td>\n",
       "      <td>-0.6635971066343154</td>\n",
       "      <td>0.21434969453182898</td>\n",
       "      <td>0.16376978937060832</td>\n",
       "      <td>5.524448613748559</td>\n",
       "      <td>-1.3436608737117721</td>\n",
       "      <td>2.462453852531733</td>\n",
       "      <td>0.23449855730270497</td>\n",
       "      <td>0.7442535953253104</td>\n",
       "      <td>0.040776023392586375</td>\n",
       "      <td>0.8759850562175853</td>\n",
       "      <td>-1.1692579562905703</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1183</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6129179389642732</td>\n",
       "      <td>-0.39217834055598655</td>\n",
       "      <td>-5.119979617969202</td>\n",
       "      <td>2.033431108916127</td>\n",
       "      <td>-3.8965870845354034</td>\n",
       "      <td>-0.8406356012977525</td>\n",
       "      <td>-1.0269406801685665</td>\n",
       "      <td>-0.36776948636391504</td>\n",
       "      <td>-0.9110671731204575</td>\n",
       "      <td>-0.05681405001318976</td>\n",
       "      <td>0.9160254682046123</td>\n",
       "      <td>1.733680949956994</td>\n",
       "      <td>-1.0780195505343118</td>\n",
       "      <td>-1.2937515231532881</td>\n",
       "      <td>0.9193441463558475</td>\n",
       "      <td>-0.9005873631610734</td>\n",
       "      <td>4.04405323236104</td>\n",
       "      <td>-0.701448880467059</td>\n",
       "      <td>0.11182185312890804</td>\n",
       "      <td>-0.75233463097153</td>\n",
       "      <td>3.731560222721702</td>\n",
       "      <td>1.3619104221217557</td>\n",
       "      <td>0.8727273900930504</td>\n",
       "      <td>0.6567275034869384</td>\n",
       "      <td>1.6435358724284725</td>\n",
       "      <td>2.1459656008476338</td>\n",
       "      <td>1.2468781094614638</td>\n",
       "      <td>-3.013708121787551</td>\n",
       "      <td>0.8610187201244269</td>\n",
       "      <td>-0.26546462432717177</td>\n",
       "      <td>-0.29737452693623334</td>\n",
       "      <td>0.45564689645966105</td>\n",
       "      <td>-2.2307804306654684</td>\n",
       "      <td>-0.3617303041969676</td>\n",
       "      <td>-1.8082835630458323</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.10794931518822878</td>\n",
       "      <td>0.007381890887349932</td>\n",
       "      <td>-0.1395438321142489</td>\n",
       "      <td>-5.442056573292989</td>\n",
       "      <td>-0.8290715358039483</td>\n",
       "      <td>0.44831037907607785</td>\n",
       "      <td>-2.4207315993525658</td>\n",
       "      <td>-0.5422398170639251</td>\n",
       "      <td>-0.8358364280442306</td>\n",
       "      <td>-0.004574777695534396</td>\n",
       "      <td>-0.40191599746417705</td>\n",
       "      <td>0.06616607398009858</td>\n",
       "      <td>0.643722984977031</td>\n",
       "      <td>1.2391240866363762</td>\n",
       "      <td>-2.5003865457409264</td>\n",
       "      <td>0.8051192812646061</td>\n",
       "      <td>-1.8561248274543916</td>\n",
       "      <td>0.784829030068205</td>\n",
       "      <td>-0.10630803220350596</td>\n",
       "      <td>-4.566878352637057</td>\n",
       "      <td>-3.6342209957805913</td>\n",
       "      <td>-2.038703384586579</td>\n",
       "      <td>0.29815531215049373</td>\n",
       "      <td>0.3250880877977148</td>\n",
       "      <td>-2.0016859279283237</td>\n",
       "      <td>0.8471471607256565</td>\n",
       "      <td>-1.9113186590375766</td>\n",
       "      <td>-0.45327462186108824</td>\n",
       "      <td>2.712898613959792</td>\n",
       "      <td>-0.5287944645420087</td>\n",
       "      <td>-0.16530994276743585</td>\n",
       "      <td>0.7994906149107759</td>\n",
       "      <td>6.986343916630998</td>\n",
       "      <td>1.039525825188455</td>\n",
       "      <td>2.8408139142385753</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.3134127493167909</td>\n",
       "      <td>-1.4629020736620388</td>\n",
       "      <td>0.4590293576941443</td>\n",
       "      <td>-9.418486467672105</td>\n",
       "      <td>-0.47476248637845087</td>\n",
       "      <td>-0.0012087709476195363</td>\n",
       "      <td>2.7655670200857014</td>\n",
       "      <td>1.2291371776220976</td>\n",
       "      <td>1.1078863371188161</td>\n",
       "      <td>0.2588890724753168</td>\n",
       "      <td>-1.7303112703176096</td>\n",
       "      <td>0.656849879250122</td>\n",
       "      <td>-1.051650843220058</td>\n",
       "      <td>0.27230937164348823</td>\n",
       "      <td>-1.2887618933099945</td>\n",
       "      <td>-0.8978165965785845</td>\n",
       "      <td>-3.2737538828922004</td>\n",
       "      <td>0.004971631516108813</td>\n",
       "      <td>2.72135233442499</td>\n",
       "      <td>6.09378861544378</td>\n",
       "      <td>-0.331141112574712</td>\n",
       "      <td>1.4241705735610837</td>\n",
       "      <td>2.3438966741840406</td>\n",
       "      <td>-0.9251915008697665</td>\n",
       "      <td>-0.017020612806081997</td>\n",
       "      <td>-2.4404898742421506</td>\n",
       "      <td>1.2332735142643654</td>\n",
       "      <td>-2.9490242559843782</td>\n",
       "      <td>3.752660920524056</td>\n",
       "      <td>4.170543427769195</td>\n",
       "      <td>0.048222579290120686</td>\n",
       "      <td>0.5889719273060283</td>\n",
       "      <td>4.807809304648286</td>\n",
       "      <td>-2.0719350171579785</td>\n",
       "      <td>7.503542537475342</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1186</td>\n",
       "      <td>2</td>\n",
       "      <td>4.727886928421089</td>\n",
       "      <td>-1.2045072072202163</td>\n",
       "      <td>-2.0926864575980364</td>\n",
       "      <td>-6.0051461410617835</td>\n",
       "      <td>0.5446052812813532</td>\n",
       "      <td>0.23335765841680634</td>\n",
       "      <td>1.210581307473026</td>\n",
       "      <td>1.4788767979210948</td>\n",
       "      <td>-0.10643602148366063</td>\n",
       "      <td>-1.1135306934174836</td>\n",
       "      <td>-1.273002670907794</td>\n",
       "      <td>1.6827692082079075</td>\n",
       "      <td>0.3435779323908643</td>\n",
       "      <td>0.6783880961589769</td>\n",
       "      <td>-0.2473708871877985</td>\n",
       "      <td>-0.9448376458392073</td>\n",
       "      <td>0.7893012604655022</td>\n",
       "      <td>0.9745168711703224</td>\n",
       "      <td>-0.19551595592651602</td>\n",
       "      <td>3.834309207302429</td>\n",
       "      <td>1.5320701384360136</td>\n",
       "      <td>-1.0026642525333656</td>\n",
       "      <td>-1.704105852339399</td>\n",
       "      <td>-1.976278420981215</td>\n",
       "      <td>0.5221872815082007</td>\n",
       "      <td>-2.6982309807096816</td>\n",
       "      <td>0.7929185029539482</td>\n",
       "      <td>2.9469008522214186</td>\n",
       "      <td>-1.3931795725578817</td>\n",
       "      <td>1.4707929916835887</td>\n",
       "      <td>0.8389363333401786</td>\n",
       "      <td>0.2534108321372878</td>\n",
       "      <td>0.02913805849390219</td>\n",
       "      <td>-0.48355581037101736</td>\n",
       "      <td>-0.7632400969616091</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>1187</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5436124423771385</td>\n",
       "      <td>-1.9257092098903543</td>\n",
       "      <td>0.5910381786166532</td>\n",
       "      <td>1.3003490176196895</td>\n",
       "      <td>10.182788918078685</td>\n",
       "      <td>-0.5739208587329149</td>\n",
       "      <td>0.9824116401290722</td>\n",
       "      <td>-0.2198577725892268</td>\n",
       "      <td>1.4920885245879245</td>\n",
       "      <td>0.0483561033140756</td>\n",
       "      <td>1.0986894553411708</td>\n",
       "      <td>-3.0388218008568786</td>\n",
       "      <td>0.783766684056295</td>\n",
       "      <td>-2.0659613788392512</td>\n",
       "      <td>-2.4168674433540267</td>\n",
       "      <td>0.7457628044046148</td>\n",
       "      <td>-1.8546119644506052</td>\n",
       "      <td>-1.2692438313872676</td>\n",
       "      <td>-0.3567429824070466</td>\n",
       "      <td>2.2590663366804598</td>\n",
       "      <td>-1.0753958081300472</td>\n",
       "      <td>-2.5533965166302646</td>\n",
       "      <td>0.2669263969436074</td>\n",
       "      <td>0.7941307155015527</td>\n",
       "      <td>1.219377009243913</td>\n",
       "      <td>1.1226837503264984</td>\n",
       "      <td>-0.5046974910966262</td>\n",
       "      <td>1.6425325094740166</td>\n",
       "      <td>-0.6383728696394436</td>\n",
       "      <td>-2.7778601930431153</td>\n",
       "      <td>-0.8935679788559023</td>\n",
       "      <td>-1.4816970917897987</td>\n",
       "      <td>-2.7015660093680043</td>\n",
       "      <td>0.36079302337131447</td>\n",
       "      <td>-4.585715924482174</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>1.2329251653272348</td>\n",
       "      <td>-0.06524200816444947</td>\n",
       "      <td>1.4553915011708614</td>\n",
       "      <td>-0.7865973982630422</td>\n",
       "      <td>15.215766440681762</td>\n",
       "      <td>-0.9473903076113371</td>\n",
       "      <td>3.4309569213117235</td>\n",
       "      <td>-0.03506816997180939</td>\n",
       "      <td>-1.0325629394558722</td>\n",
       "      <td>-1.1045617287918557</td>\n",
       "      <td>0.941934111648276</td>\n",
       "      <td>-0.2697439724566898</td>\n",
       "      <td>-1.3226550898756675</td>\n",
       "      <td>2.0874826651506586</td>\n",
       "      <td>1.4192642585320496</td>\n",
       "      <td>-0.9348092570568097</td>\n",
       "      <td>-5.283253340398339</td>\n",
       "      <td>2.1727422010940383</td>\n",
       "      <td>1.2070365208119884</td>\n",
       "      <td>7.102413363528593</td>\n",
       "      <td>2.3108473107010803</td>\n",
       "      <td>-0.1497635744618204</td>\n",
       "      <td>0.4335647884378009</td>\n",
       "      <td>-1.6009769736667907</td>\n",
       "      <td>1.0635982156080506</td>\n",
       "      <td>-3.8571078639721565</td>\n",
       "      <td>-1.187706713979012</td>\n",
       "      <td>2.625428501308728</td>\n",
       "      <td>1.244394720323839</td>\n",
       "      <td>-4.901062322408849</td>\n",
       "      <td>0.6168911396487382</td>\n",
       "      <td>-0.6647731742814764</td>\n",
       "      <td>-3.8246440258682415</td>\n",
       "      <td>0.22861391160381067</td>\n",
       "      <td>-12.03845256597095</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.2936105795324857</td>\n",
       "      <td>0.3011025738075046</td>\n",
       "      <td>1.3755795832114068</td>\n",
       "      <td>1.0254488071741743</td>\n",
       "      <td>-12.31030182317135</td>\n",
       "      <td>-1.6456159580145735</td>\n",
       "      <td>-2.617102329758365</td>\n",
       "      <td>2.9868575246001967</td>\n",
       "      <td>0.03783236475091617</td>\n",
       "      <td>-0.07233283283812192</td>\n",
       "      <td>0.6622595637412135</td>\n",
       "      <td>4.702046413497952</td>\n",
       "      <td>1.6419810290536057</td>\n",
       "      <td>-1.8223820170852811</td>\n",
       "      <td>-1.7608825316885288</td>\n",
       "      <td>-0.07964595560303947</td>\n",
       "      <td>2.7396199508260257</td>\n",
       "      <td>-1.9712738355268513</td>\n",
       "      <td>0.13246603104196525</td>\n",
       "      <td>0.34564906170647036</td>\n",
       "      <td>-1.3702761421688545</td>\n",
       "      <td>3.800944888734973</td>\n",
       "      <td>-0.1452412174491898</td>\n",
       "      <td>0.07968275664501623</td>\n",
       "      <td>-1.5796225896953313</td>\n",
       "      <td>0.7135917114346542</td>\n",
       "      <td>-0.48210931519557</td>\n",
       "      <td>8.644217707740694</td>\n",
       "      <td>-3.2497492163264075</td>\n",
       "      <td>3.196057030689897</td>\n",
       "      <td>0.049479531314287985</td>\n",
       "      <td>0.32756766242208</td>\n",
       "      <td>-0.4246741540360265</td>\n",
       "      <td>0.9105795504835837</td>\n",
       "      <td>7.8856976084311</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3454572207954528</td>\n",
       "      <td>1.36317439172573</td>\n",
       "      <td>-0.14709913185586107</td>\n",
       "      <td>-4.78043998170748</td>\n",
       "      <td>-7.353315425817431</td>\n",
       "      <td>-1.125204048127775</td>\n",
       "      <td>-1.4246462231894401</td>\n",
       "      <td>-0.37231508541002667</td>\n",
       "      <td>-1.2099176959758922</td>\n",
       "      <td>1.5102810650843936</td>\n",
       "      <td>0.24069442971470778</td>\n",
       "      <td>2.5581790691072688</td>\n",
       "      <td>0.9472677623423394</td>\n",
       "      <td>1.6798061082091258</td>\n",
       "      <td>-1.505459307596925</td>\n",
       "      <td>-1.1425541752902488</td>\n",
       "      <td>1.1611081118185376</td>\n",
       "      <td>-1.092960210997312</td>\n",
       "      <td>0.2990290396129368</td>\n",
       "      <td>-2.8613443847517583</td>\n",
       "      <td>-1.1664251625096638</td>\n",
       "      <td>-2.267771929810628</td>\n",
       "      <td>-1.3276502297225148</td>\n",
       "      <td>-1.539472140922785</td>\n",
       "      <td>-1.2338484762276147</td>\n",
       "      <td>1.081225948890778</td>\n",
       "      <td>-1.7227291803732574</td>\n",
       "      <td>-0.6690250782465482</td>\n",
       "      <td>1.9706879526535241</td>\n",
       "      <td>3.4400558968611024</td>\n",
       "      <td>0.8485837984465023</td>\n",
       "      <td>-0.33270275417937323</td>\n",
       "      <td>5.412837905399699</td>\n",
       "      <td>-0.25691706671012293</td>\n",
       "      <td>7.414895635157644</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>1191</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3336131538463962</td>\n",
       "      <td>0.3575267371928684</td>\n",
       "      <td>0.8898496495280238</td>\n",
       "      <td>3.0306291046081344</td>\n",
       "      <td>2.5535233968377815</td>\n",
       "      <td>0.3643176283709202</td>\n",
       "      <td>-0.7098041357038359</td>\n",
       "      <td>-0.8364243784707404</td>\n",
       "      <td>-0.5123697580052905</td>\n",
       "      <td>0.14848195415350673</td>\n",
       "      <td>-0.23185691638913644</td>\n",
       "      <td>-0.9680642551593782</td>\n",
       "      <td>1.4752653354322334</td>\n",
       "      <td>-0.266479361487191</td>\n",
       "      <td>2.5927848947635703</td>\n",
       "      <td>-0.37922446587097036</td>\n",
       "      <td>-1.4729464617219161</td>\n",
       "      <td>0.7625369766034309</td>\n",
       "      <td>-0.9210888134392853</td>\n",
       "      <td>0.9187425316433875</td>\n",
       "      <td>-0.7073597544254319</td>\n",
       "      <td>2.1831480651196746</td>\n",
       "      <td>1.5609570430227322</td>\n",
       "      <td>0.22327717700077215</td>\n",
       "      <td>-0.9653564344849971</td>\n",
       "      <td>1.0407556529556043</td>\n",
       "      <td>0.8679319019031883</td>\n",
       "      <td>1.7464643788787737</td>\n",
       "      <td>1.1439874560707757</td>\n",
       "      <td>1.1221210988456465</td>\n",
       "      <td>0.5671564526096289</td>\n",
       "      <td>0.9622644491994143</td>\n",
       "      <td>-0.2641290188665565</td>\n",
       "      <td>-0.7913604870429379</td>\n",
       "      <td>-0.8068225621897305</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>1192</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3461236018617098</td>\n",
       "      <td>0.6189160112596074</td>\n",
       "      <td>2.8209106645397326</td>\n",
       "      <td>2.7100196926066564</td>\n",
       "      <td>7.06454805210008</td>\n",
       "      <td>-0.0483687615953205</td>\n",
       "      <td>-0.4498798772250102</td>\n",
       "      <td>0.4505304408012383</td>\n",
       "      <td>-1.4650124931154587</td>\n",
       "      <td>0.5003312681163836</td>\n",
       "      <td>2.2759060522456185</td>\n",
       "      <td>0.3803421498066083</td>\n",
       "      <td>-1.2216076611838673</td>\n",
       "      <td>-0.9708798934946584</td>\n",
       "      <td>2.2720380564641416</td>\n",
       "      <td>-1.2314991150820267</td>\n",
       "      <td>-1.0181128351666389</td>\n",
       "      <td>0.9481913020013338</td>\n",
       "      <td>0.1704408958479472</td>\n",
       "      <td>-1.5791776268888102</td>\n",
       "      <td>-3.573303072355796</td>\n",
       "      <td>-1.3449942030746962</td>\n",
       "      <td>0.5703483722373734</td>\n",
       "      <td>-1.665215706139557</td>\n",
       "      <td>1.8531390276740485</td>\n",
       "      <td>2.8854935495834817</td>\n",
       "      <td>1.0969729779935657</td>\n",
       "      <td>-2.7249737524633146</td>\n",
       "      <td>3.2300080371224307</td>\n",
       "      <td>-2.2014573890129943</td>\n",
       "      <td>0.8386560296136905</td>\n",
       "      <td>1.3861260217912657</td>\n",
       "      <td>0.6252100235865369</td>\n",
       "      <td>0.8597417148779872</td>\n",
       "      <td>-4.274870765460767</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>1193</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9968516942940568</td>\n",
       "      <td>0.07052153931092947</td>\n",
       "      <td>-1.3516144546157016</td>\n",
       "      <td>-1.0773998720154385</td>\n",
       "      <td>-1.8301128895352943</td>\n",
       "      <td>-0.3630482291372321</td>\n",
       "      <td>2.633758746656935</td>\n",
       "      <td>-0.7593446770612872</td>\n",
       "      <td>-1.7726014536212062</td>\n",
       "      <td>1.2897123082740756</td>\n",
       "      <td>-0.3856107062563472</td>\n",
       "      <td>1.103607149769532</td>\n",
       "      <td>0.027953894675313493</td>\n",
       "      <td>-0.5227695331457697</td>\n",
       "      <td>2.500161010986847</td>\n",
       "      <td>0.990015805244922</td>\n",
       "      <td>-0.41523933676732117</td>\n",
       "      <td>-0.7764808465473477</td>\n",
       "      <td>-0.525933577208267</td>\n",
       "      <td>1.5067776670395305</td>\n",
       "      <td>0.6370073034618512</td>\n",
       "      <td>2.4199252090460557</td>\n",
       "      <td>-0.6927820363145551</td>\n",
       "      <td>0.9331499487319751</td>\n",
       "      <td>1.1817385465989874</td>\n",
       "      <td>0.3605993844196098</td>\n",
       "      <td>1.4173850138234776</td>\n",
       "      <td>-3.0752627695835355</td>\n",
       "      <td>-0.12983821166654272</td>\n",
       "      <td>1.077457789294931</td>\n",
       "      <td>-0.5935339738952637</td>\n",
       "      <td>-0.0107556146062327</td>\n",
       "      <td>-0.27802236500228383</td>\n",
       "      <td>1.0355773742424188</td>\n",
       "      <td>0.9321275704383634</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1194</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0970396507461195</td>\n",
       "      <td>-0.32527013565851515</td>\n",
       "      <td>0.586032452524004</td>\n",
       "      <td>4.344560953590231</td>\n",
       "      <td>-8.078979286738608</td>\n",
       "      <td>0.4435231957955683</td>\n",
       "      <td>-1.5553635109706</td>\n",
       "      <td>-0.9110572421045012</td>\n",
       "      <td>-0.3856580008631578</td>\n",
       "      <td>1.3153691810754826</td>\n",
       "      <td>1.2101943081458408</td>\n",
       "      <td>3.316580916552174</td>\n",
       "      <td>-1.2147134689335644</td>\n",
       "      <td>-0.09146320371261416</td>\n",
       "      <td>1.9712728620274473</td>\n",
       "      <td>0.2514737313369846</td>\n",
       "      <td>-0.4329999389812048</td>\n",
       "      <td>-0.6812807409545232</td>\n",
       "      <td>0.5586241748349468</td>\n",
       "      <td>-4.225673409780895</td>\n",
       "      <td>0.5952473789089366</td>\n",
       "      <td>-1.3575544584443353</td>\n",
       "      <td>1.573345663180804</td>\n",
       "      <td>1.001883998733843</td>\n",
       "      <td>-1.3771223817895453</td>\n",
       "      <td>1.6108351107524774</td>\n",
       "      <td>-2.5427235816259546</td>\n",
       "      <td>3.4000287343095175</td>\n",
       "      <td>-2.020270346239041</td>\n",
       "      <td>4.5254905476274505</td>\n",
       "      <td>-1.5584977224395449</td>\n",
       "      <td>0.24711891306325004</td>\n",
       "      <td>0.695333053328436</td>\n",
       "      <td>-0.507316125618687</td>\n",
       "      <td>2.6672662191010548</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.4408100384870415</td>\n",
       "      <td>-1.0595497351470051</td>\n",
       "      <td>-2.136874921146005</td>\n",
       "      <td>3.4333757647715846</td>\n",
       "      <td>-8.610958548233503</td>\n",
       "      <td>-1.5373246333469868</td>\n",
       "      <td>0.664981295929772</td>\n",
       "      <td>-0.000508050475247962</td>\n",
       "      <td>-1.2505285569243703</td>\n",
       "      <td>0.22032703776013496</td>\n",
       "      <td>-0.47571124862153125</td>\n",
       "      <td>4.509741883545093</td>\n",
       "      <td>0.15526609881212555</td>\n",
       "      <td>-0.6208064587643621</td>\n",
       "      <td>2.364672566041228</td>\n",
       "      <td>0.8323998734214928</td>\n",
       "      <td>-0.7433067045786835</td>\n",
       "      <td>0.16073457942393404</td>\n",
       "      <td>-0.9915938923044053</td>\n",
       "      <td>-2.531455004744205</td>\n",
       "      <td>4.345514496135831</td>\n",
       "      <td>2.1947044000194116</td>\n",
       "      <td>0.2309881027574295</td>\n",
       "      <td>0.5283731400228635</td>\n",
       "      <td>-1.2232420303996334</td>\n",
       "      <td>1.861636035732857</td>\n",
       "      <td>-0.6243179765849484</td>\n",
       "      <td>-1.6672759931310073</td>\n",
       "      <td>1.425215944471904</td>\n",
       "      <td>0.48605316671294285</td>\n",
       "      <td>1.2281282435023066</td>\n",
       "      <td>2.083503295227778</td>\n",
       "      <td>-0.5111341178879835</td>\n",
       "      <td>-0.2889085169181339</td>\n",
       "      <td>0.7890077247404356</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>1</td>\n",
       "      <td>3.3670063907031</td>\n",
       "      <td>-0.06417402860719307</td>\n",
       "      <td>0.2680765885924036</td>\n",
       "      <td>-8.566973810664075</td>\n",
       "      <td>-0.23042063226223206</td>\n",
       "      <td>0.41506883843577624</td>\n",
       "      <td>5.2582412876858955</td>\n",
       "      <td>-0.9906531583064887</td>\n",
       "      <td>-0.6497717529832614</td>\n",
       "      <td>-0.21433096839151625</td>\n",
       "      <td>0.40702530983324037</td>\n",
       "      <td>-0.23403552934786717</td>\n",
       "      <td>0.628647318752534</td>\n",
       "      <td>2.9470882303759285</td>\n",
       "      <td>1.2048886679880781</td>\n",
       "      <td>-0.1786262894535223</td>\n",
       "      <td>1.4730329370196436</td>\n",
       "      <td>0.15955802094577845</td>\n",
       "      <td>0.6107094311421618</td>\n",
       "      <td>-0.2235216755002389</td>\n",
       "      <td>-1.2575751976100307</td>\n",
       "      <td>-1.7804500491580915</td>\n",
       "      <td>1.4509105946426832</td>\n",
       "      <td>-0.8020784150840996</td>\n",
       "      <td>0.6903339912697565</td>\n",
       "      <td>-0.8565193285077014</td>\n",
       "      <td>-1.6515800383154506</td>\n",
       "      <td>-6.33145723464855</td>\n",
       "      <td>0.363897031914182</td>\n",
       "      <td>-1.5114293309653983</td>\n",
       "      <td>0.2418422790153796</td>\n",
       "      <td>1.1999320580998245</td>\n",
       "      <td>3.2993390440385997</td>\n",
       "      <td>1.1488389619561448</td>\n",
       "      <td>3.374621495378453</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1197</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6098839323452996</td>\n",
       "      <td>-0.5043465007241554</td>\n",
       "      <td>-0.9404131277911284</td>\n",
       "      <td>0.8054729414537122</td>\n",
       "      <td>8.493837912523224</td>\n",
       "      <td>-1.6944698810016943</td>\n",
       "      <td>-0.23420385000755037</td>\n",
       "      <td>0.2083510845388692</td>\n",
       "      <td>-0.5737421576565819</td>\n",
       "      <td>0.35198040244381146</td>\n",
       "      <td>-0.613341146916984</td>\n",
       "      <td>-1.43220746734427</td>\n",
       "      <td>0.2022161400290321</td>\n",
       "      <td>1.6347112482164654</td>\n",
       "      <td>1.7307193492606703</td>\n",
       "      <td>1.2709205990042762</td>\n",
       "      <td>-2.6688461301698676</td>\n",
       "      <td>-0.27663384206442465</td>\n",
       "      <td>-0.6271746701563826</td>\n",
       "      <td>3.4131512367980523</td>\n",
       "      <td>-1.1836704733467376</td>\n",
       "      <td>2.0331659077545883</td>\n",
       "      <td>0.3998898211061982</td>\n",
       "      <td>-2.086268004125758</td>\n",
       "      <td>2.2417337542669467</td>\n",
       "      <td>1.00244011733166</td>\n",
       "      <td>0.8312945044540551</td>\n",
       "      <td>-2.7449339056720556</td>\n",
       "      <td>3.236162784324399</td>\n",
       "      <td>0.09461717277185713</td>\n",
       "      <td>1.4497082383345115</td>\n",
       "      <td>-0.17725146918042853</td>\n",
       "      <td>-0.2843975661069681</td>\n",
       "      <td>0.4265312809270644</td>\n",
       "      <td>-4.407640584452011</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1198</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552730026132185</td>\n",
       "      <td>-0.8293254796874954</td>\n",
       "      <td>-1.2507092269394087</td>\n",
       "      <td>1.135719470395597</td>\n",
       "      <td>7.618186079185134</td>\n",
       "      <td>-0.18924111158630744</td>\n",
       "      <td>-0.8394234315931267</td>\n",
       "      <td>1.1759756057550312</td>\n",
       "      <td>0.5528234868525234</td>\n",
       "      <td>-0.5743057341622545</td>\n",
       "      <td>-1.7454375533232127</td>\n",
       "      <td>0.13776404233392694</td>\n",
       "      <td>0.055445344919623496</td>\n",
       "      <td>0.24221469346730456</td>\n",
       "      <td>3.477546997369153</td>\n",
       "      <td>-1.21730080394277</td>\n",
       "      <td>-2.262065501478789</td>\n",
       "      <td>-0.18212451317786751</td>\n",
       "      <td>1.9925622610041367</td>\n",
       "      <td>-1.0616462021853428</td>\n",
       "      <td>-0.8510370634062078</td>\n",
       "      <td>-1.69499485334055</td>\n",
       "      <td>1.8469848441814392</td>\n",
       "      <td>-0.06235366598167899</td>\n",
       "      <td>3.8761159965137204</td>\n",
       "      <td>-1.2839531083461222</td>\n",
       "      <td>0.6555497995580744</td>\n",
       "      <td>-4.037787405730562</td>\n",
       "      <td>0.48066437763530845</td>\n",
       "      <td>0.5296330432171811</td>\n",
       "      <td>-2.57180159165048</td>\n",
       "      <td>1.5677328897504648</td>\n",
       "      <td>0.6607932081267054</td>\n",
       "      <td>-0.24538740678723966</td>\n",
       "      <td>-9.34294271988492</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1199</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3333378520796635</td>\n",
       "      <td>0.2519332004238905</td>\n",
       "      <td>-0.5492246285882997</td>\n",
       "      <td>6.685126714477433</td>\n",
       "      <td>6.630892525181011</td>\n",
       "      <td>1.6412160946715382</td>\n",
       "      <td>-2.2737299003600095</td>\n",
       "      <td>-0.40261655741432434</td>\n",
       "      <td>0.5826740951134363</td>\n",
       "      <td>0.4992268641896448</td>\n",
       "      <td>-0.09667516186692057</td>\n",
       "      <td>1.2551926190025722</td>\n",
       "      <td>-0.4999794907831364</td>\n",
       "      <td>-2.5646597034124747</td>\n",
       "      <td>1.5171895615504303</td>\n",
       "      <td>-0.08706333580876527</td>\n",
       "      <td>-3.455732985711092</td>\n",
       "      <td>0.39660291230667444</td>\n",
       "      <td>0.912636353184481</td>\n",
       "      <td>0.5451092388804373</td>\n",
       "      <td>1.9940396621027274</td>\n",
       "      <td>-0.30577487946294035</td>\n",
       "      <td>-0.2847791896104451</td>\n",
       "      <td>-1.4654551059727134</td>\n",
       "      <td>1.2242305811562204</td>\n",
       "      <td>-0.6079537787114343</td>\n",
       "      <td>-0.09159479217414465</td>\n",
       "      <td>3.904210372925601</td>\n",
       "      <td>-0.3723720122381162</td>\n",
       "      <td>-0.7674918991666885</td>\n",
       "      <td>0.893122346457367</td>\n",
       "      <td>-0.8657537538131193</td>\n",
       "      <td>-3.461871473181544</td>\n",
       "      <td>0.9442083140957253</td>\n",
       "      <td>-10.526892625446957</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 label          num.feature 1         num.feature 2  \\\n",
       "0             0     3      5.655293710354379     2.014054833232026   \n",
       "1             1     0    -1.7984256395878813    1.6919170435177888   \n",
       "2             2     1    -3.0391516448049285   0.04324375221837573   \n",
       "3             3     0     2.5921064476513824   -0.5829057500291885   \n",
       "4             4     0     -1.183333848599415   -0.8720288566362178   \n",
       "5             5     0    -1.5990896536953398    1.4473620648246939   \n",
       "6             6     1    -0.9225368078052741    0.7912550203598914   \n",
       "7             7     3  0.0011822649698808585   -1.2391908578441815   \n",
       "8             8     0     0.6105241043590869   -0.7335019619006894   \n",
       "9             9     1    -1.3517882469718479    0.8486183547859842   \n",
       "10           10     3      2.075348061479394   0.45137226512892303   \n",
       "11           11     0    -1.2356055068732668     2.220742892838994   \n",
       "12           12     3    -1.8506443631142822    0.2676320311652304   \n",
       "13           13     0    -1.4232998032414208  -0.06992772099837025   \n",
       "14           14     3     0.6885516573395772  -0.16967863286453064   \n",
       "15           15     0     0.1480764254455367    1.4302456845560418   \n",
       "16           16     3    -1.2685465874545536   -0.4740789212188733   \n",
       "17           17     2      2.189586909706462   -0.7882054152142215   \n",
       "18           18     0     1.0586558917064257   -0.5556677841685063   \n",
       "19           19     3     1.7862597621132181  0.035625438631779194   \n",
       "20           20     0      3.466806485758727    0.4864596137186106   \n",
       "21           21     0      2.013715452751589  -0.18587430999356616   \n",
       "22           22     0    -1.9813103976404522  -0.48277264163287914   \n",
       "23           23     2     0.8903384311382999    0.6986974580770456   \n",
       "24           24     0    -1.3100718715580255   -0.8829662574155899   \n",
       "25           25     0      0.597222086109405    1.1334442475129294   \n",
       "26           26     0     0.8280317690188265  -0.48889609362200304   \n",
       "27           27     0     1.7592531994726568   -1.1182774198582695   \n",
       "28           28     0     0.3940024028691185  -0.33445539878392805   \n",
       "29           29     1     0.5148789918733742   -0.9138218249492241   \n",
       "...         ...   ...                    ...                   ...   \n",
       "1170       1170     0    -0.9343885717308088    1.5271086405761165   \n",
       "1171       1171     0     0.8395911804295536    -1.168388034096431   \n",
       "1172       1172     3     2.4070003517804133  0.062108110144113715   \n",
       "1173       1173     0    -1.5097668155359814    -2.248926407043649   \n",
       "1174       1174     2     -0.285147739860543     1.010889584045589   \n",
       "1175       1175     0    -1.2755590478870014     2.028400829727336   \n",
       "1176       1176     1    -1.1742311001490733   -0.9156767520472041   \n",
       "1177       1177     0     -1.631598837828945   -0.3251869982665415   \n",
       "1178       1178     3     -1.501999615495122    0.1844090034148628   \n",
       "1179       1179     0     1.3549601192871137    0.5093663447647194   \n",
       "1180       1180     0     2.2757699071676885    0.3606846574028201   \n",
       "1181       1181     3    -1.0869037393644914    0.3669177217748294   \n",
       "1182       1182     0     2.6263599205771264    1.1258149991248585   \n",
       "1183       1183     1     1.6129179389642732  -0.39217834055598655   \n",
       "1184       1184     0   -0.10794931518822878  0.007381890887349932   \n",
       "1185       1185     0    -1.3134127493167909   -1.4629020736620388   \n",
       "1186       1186     2      4.727886928421089   -1.2045072072202163   \n",
       "1187       1187     1     3.5436124423771385   -1.9257092098903543   \n",
       "1188       1188     3     1.2329251653272348  -0.06524200816444947   \n",
       "1189       1189     0    -1.2936105795324857    0.3011025738075046   \n",
       "1190       1190     0     0.3454572207954528      1.36317439172573   \n",
       "1191       1191     0     2.3336131538463962    0.3575267371928684   \n",
       "1192       1192     0     2.3461236018617098    0.6189160112596074   \n",
       "1193       1193     2     1.9968516942940568   0.07052153931092947   \n",
       "1194       1194     0     1.0970396507461195  -0.32527013565851515   \n",
       "1195       1195     1    -2.4408100384870415   -1.0595497351470051   \n",
       "1196       1196     1        3.3670063907031  -0.06417402860719307   \n",
       "1197       1197     0     2.6098839323452996   -0.5043465007241554   \n",
       "1198       1198     0      1.552730026132185   -0.8293254796874954   \n",
       "1199       1199     1    -0.3333378520796635    0.2519332004238905   \n",
       "\n",
       "              num.feature 3         num.feature 4         num.feature 5  \\\n",
       "0        1.5312187343606998   -0.9373708832551105    -8.469637272851203   \n",
       "1        1.0482356462149778     5.480638906415307    -3.819468279526532   \n",
       "2         2.215229914095391     4.953622286722998  -0.46074817959280046   \n",
       "3        1.6713136588869428   -1.7808474667492604   -3.9707821371419914   \n",
       "4         1.771248550898655   -1.3240326709389698    -17.33915954056416   \n",
       "5      -0.23301014252627214    0.9061777403720076   -0.9769792825162696   \n",
       "6         1.585116806695354     6.049549847277739     7.988317730999925   \n",
       "7        1.7320220324447413  -0.44368355308630936   -0.7079141818255753   \n",
       "8        0.5440939964427246    -8.603653176241078     3.142106781126477   \n",
       "9        2.0185193810849547    2.8631750782749306   -0.8382521390386972   \n",
       "10      0.20452182450387804    1.7401028232402376   -1.7215842711723055   \n",
       "11       0.9797728904364547    -3.285030864295107   -10.029073310899754   \n",
       "12      -1.7784854931142369    -5.159883702810021    -4.500460175730115   \n",
       "13    -0.055740171042023196     -2.33369130705232    4.1565842905535035   \n",
       "14      0.21475374735087452    -3.079421072563564     0.449819739273847   \n",
       "15     -0.24886097526230308     5.138150920083797    -4.291069058532566   \n",
       "16      -1.9876485047410841   -0.6148569175390369    -2.720829633662395   \n",
       "17      0.11330495573366375   -1.2862466268364217     4.412170599605036   \n",
       "18      -1.6340988843933264    1.6015076980387664    -2.954000377046785   \n",
       "19        4.755263293263475    -4.340625828706606     6.250562786967742   \n",
       "20       1.7433666534125747   -3.4172819304629565     6.178526281539781   \n",
       "21      0.12048236108714586    -2.432144578918184     7.131578810811659   \n",
       "22       1.2100176444856294     6.034800240645673   -13.474822590493725   \n",
       "23       -3.200728509371963   -2.1556075086697324    -4.779074332137604   \n",
       "24      -0.7046373205748303   -0.7068768086846816    -7.781644244055439   \n",
       "25      -0.1330998162918787  -0.28371349011563496    2.1121770904527506   \n",
       "26      0.31765145723535726     2.654649827446791    -6.948958209834084   \n",
       "27      -0.7400768946572848    -5.632356240677991    -3.355104685892391   \n",
       "28        2.027188946507427     2.261478787655949   -1.7231651056388275   \n",
       "29      0.40744485384645357     2.437279398835636    5.5002250319942085   \n",
       "...                     ...                   ...                   ...   \n",
       "1170    -0.4435803340155843   -5.9337637890235095   -0.9616637278789445   \n",
       "1171     0.7218414779595365     5.579491785516002   -3.3599702849269355   \n",
       "1172    -0.6912463640611776    -7.676674455605033   -1.1157112760666261   \n",
       "1173     2.4676065050911338    -4.758676565480606    1.2035829492444905   \n",
       "1174     -4.710336727361581    -4.949070099407876    0.3166388494703435   \n",
       "1175    0.07563808645012751     9.173940417393867    1.5500203094679565   \n",
       "1176    -2.0817337873846644     7.501884852634827     3.257319706662676   \n",
       "1177     3.6317078857933813    1.1334624416146055    -7.501099171567779   \n",
       "1178     -2.100063380558122   -1.7810923384709374    -3.207870198374697   \n",
       "1179      0.870628211799776     6.839038756055286     6.244724369179147   \n",
       "1180     2.6328238482106956     3.485539825620019    12.202579133530863   \n",
       "1181  -0.020739883750392124  0.004005818030604319     2.925164421662623   \n",
       "1182      3.701070213937797    2.9873339762390723    -2.864835489485059   \n",
       "1183     -5.119979617969202     2.033431108916127   -3.8965870845354034   \n",
       "1184    -0.1395438321142489    -5.442056573292989   -0.8290715358039483   \n",
       "1185     0.4590293576941443    -9.418486467672105  -0.47476248637845087   \n",
       "1186    -2.0926864575980364   -6.0051461410617835    0.5446052812813532   \n",
       "1187     0.5910381786166532    1.3003490176196895    10.182788918078685   \n",
       "1188     1.4553915011708614   -0.7865973982630422    15.215766440681762   \n",
       "1189     1.3755795832114068    1.0254488071741743    -12.31030182317135   \n",
       "1190   -0.14709913185586107     -4.78043998170748    -7.353315425817431   \n",
       "1191     0.8898496495280238    3.0306291046081344    2.5535233968377815   \n",
       "1192     2.8209106645397326    2.7100196926066564      7.06454805210008   \n",
       "1193    -1.3516144546157016   -1.0773998720154385   -1.8301128895352943   \n",
       "1194      0.586032452524004     4.344560953590231    -8.078979286738608   \n",
       "1195     -2.136874921146005    3.4333757647715846    -8.610958548233503   \n",
       "1196     0.2680765885924036    -8.566973810664075  -0.23042063226223206   \n",
       "1197    -0.9404131277911284    0.8054729414537122     8.493837912523224   \n",
       "1198    -1.2507092269394087     1.135719470395597     7.618186079185134   \n",
       "1199    -0.5492246285882997     6.685126714477433     6.630892525181011   \n",
       "\n",
       "               num.feature 6         num.feature 7          num.feature 8  \\\n",
       "0          1.113903439986374  -0.42905952145040327     0.6427846330833655   \n",
       "1        -0.5861169375133876  -0.46316164430857876      1.391275736650314   \n",
       "2         -1.099156946118343     0.528542562842154     0.8383435021509663   \n",
       "3        -0.3083914226602489   0.08230135499113403    -0.7718830352052108   \n",
       "4        0.36202581349779717    -3.351701763089841    -0.0548368003767373   \n",
       "5         0.7405836563253125  -0.21455959979054104    -0.9043525559235933   \n",
       "6          2.397171061419061   -1.0070473030379696   -0.28901957150780244   \n",
       "7         0.9560940659093461     2.241665713409718       1.03460834229193   \n",
       "8        -0.2621913384664125    3.6547062900338636     1.2718664075633053   \n",
       "9        -1.8323113845798258   0.11508048070939716    -0.8871182794103027   \n",
       "10       -1.0180485185951156    -1.491830380602376    -0.7612264180543937   \n",
       "11         0.910847563272922    0.5660318316947659    -0.0339944898463751   \n",
       "12       -0.4708488458459037     2.242517877620033    -0.0343640596895049   \n",
       "13         2.160041518271852     5.028983171380723     0.0504333487618344   \n",
       "14        0.8280376202786597     2.347304679943285    -1.5363421868195597   \n",
       "15        0.5320102297531657    -2.826461738470837     1.0120503932465885   \n",
       "16       -0.3328382554945878    0.6489593864605747    0.16368348470149532   \n",
       "17       -0.5833395186322218   -1.7776450823803942    -1.6656088011105596   \n",
       "18       -0.4120832400623331    -2.562667670764183     0.9000180080633978   \n",
       "19       -0.1691700244130309     4.600069133023698     0.4498200623097326   \n",
       "20         1.407651185118566    3.7109553245869447    -1.0374663293519626   \n",
       "21        1.2152603616334323   -1.8570265213916781     1.0830374393631774   \n",
       "22        0.6952896417298624    1.8687765336227389     1.0085428044850617   \n",
       "23        0.5844230624459069    2.9027082006114924     1.2704679384290523   \n",
       "24        1.0615469512372824    1.3976856186302782    0.10920882732590623   \n",
       "25       -0.4010725866177972     2.165614697729135     2.1074922048297973   \n",
       "26        2.1265988133740406   -1.5816028034207332    -0.2734707628490536   \n",
       "27        1.0875928297870379    0.9416692407251334     1.8375143240159362   \n",
       "28        0.1597903772525295    0.7669667022051557      1.417592562803678   \n",
       "29       -0.6445128260432557    2.6712093344595957     0.2359651938081096   \n",
       "...                      ...                   ...                    ...   \n",
       "1170    -0.17279663799306935    4.9485797478162015     0.5567235640732643   \n",
       "1171     -0.7341527069333492   -0.5024380396725944     1.1925647791554592   \n",
       "1172      0.8190765362973288     4.444972268213096      1.539244817802371   \n",
       "1173      0.7703443315699603    0.7754141968544236     0.8467339640103299   \n",
       "1174     -0.7475031584943358     2.742278390212407     1.2820406201717525   \n",
       "1175     -3.0087677417812393   0.14950577389569744     0.9061701969680805   \n",
       "1176     0.35474673599673145   -0.7856021368881816     0.8256207749000923   \n",
       "1177       1.117066470694563     1.905787485975863     -1.567214693616842   \n",
       "1178      0.5320642623573143    1.8956191014065429     0.3779393007703429   \n",
       "1179       1.036326174867773   -3.4005230552311088     -1.264071607647027   \n",
       "1180      2.7556865859815507   -3.4655100072209386    -1.1435347617525058   \n",
       "1181     -0.9877572767886712     3.700225720248489     0.5742893176828693   \n",
       "1182      1.3946783101356994   -0.6305853027186651     0.3922432007246808   \n",
       "1183     -0.8406356012977525   -1.0269406801685665   -0.36776948636391504   \n",
       "1184     0.44831037907607785   -2.4207315993525658    -0.5422398170639251   \n",
       "1185  -0.0012087709476195363    2.7655670200857014     1.2291371776220976   \n",
       "1186     0.23335765841680634     1.210581307473026     1.4788767979210948   \n",
       "1187     -0.5739208587329149    0.9824116401290722    -0.2198577725892268   \n",
       "1188     -0.9473903076113371    3.4309569213117235   -0.03506816997180939   \n",
       "1189     -1.6456159580145735    -2.617102329758365     2.9868575246001967   \n",
       "1190      -1.125204048127775   -1.4246462231894401   -0.37231508541002667   \n",
       "1191      0.3643176283709202   -0.7098041357038359    -0.8364243784707404   \n",
       "1192     -0.0483687615953205   -0.4498798772250102     0.4505304408012383   \n",
       "1193     -0.3630482291372321     2.633758746656935    -0.7593446770612872   \n",
       "1194      0.4435231957955683      -1.5553635109706    -0.9110572421045012   \n",
       "1195     -1.5373246333469868     0.664981295929772  -0.000508050475247962   \n",
       "1196     0.41506883843577624    5.2582412876858955    -0.9906531583064887   \n",
       "1197     -1.6944698810016943  -0.23420385000755037     0.2083510845388692   \n",
       "1198    -0.18924111158630744   -0.8394234315931267     1.1759756057550312   \n",
       "1199      1.6412160946715382   -2.2737299003600095   -0.40261655741432434   \n",
       "\n",
       "             num.feature 9         num.feature 10        num.feature 11  \\\n",
       "0      -0.9785354410320942    -0.5434369150743821    0.5004595994572275   \n",
       "1       0.6458916220445187    -2.6107190116090493    2.7712434705333826   \n",
       "2     -0.43736465142855263     0.8292222734847019  -0.07646348234231906   \n",
       "3      0.38717443073067614    -0.5760098910106438      1.90776030729306   \n",
       "4       0.2681452236265388     1.1355369238529915   0.08094436964186573   \n",
       "5       0.4335708171628694    -0.4240813778399951   -1.2577073889798733   \n",
       "6       0.7827018162124598    -0.3898989798377507   -0.7654992680619259   \n",
       "7      0.27367762159942144   -0.03442626482166486  -0.33352591341379445   \n",
       "8     -0.20753209459264896   -0.14925364194552665    1.2000579327207528   \n",
       "9     -0.45069963880020203     0.7600506392527846   -0.8258693607712766   \n",
       "10       2.018325375209947     0.9775990876772271    1.4404295920075487   \n",
       "11     0.03639010947505865    0.44978700195448607  -0.22711594614665162   \n",
       "12      0.4122493470237777     -1.675435836504585    0.8762991351698909   \n",
       "13    -0.11628677493089992    0.14257505201261444   0.14534871059104174   \n",
       "14     0.34112632731886783     0.8358327248714632   -0.5468934619594901   \n",
       "15     -0.7227957075164658    0.04816494909359469   -1.2047823859480709   \n",
       "16    -0.08771676942070941     1.2612051459413292   -2.2045661587637486   \n",
       "17     0.25633058788256313     0.7009240720468797   -0.2141944399380312   \n",
       "18      1.1281976882881328    -0.5842564487049725    0.5014531906489859   \n",
       "19      0.7733429649637811     1.4395527904642587   0.39872122018129363   \n",
       "20       1.804587244994382     0.3755931589522066    1.5169038902157035   \n",
       "21      1.2601309440474435    0.03514561318007499  -0.01807748286282923   \n",
       "22     -0.0920766886161262     0.9235949575964124   0.14585088058234022   \n",
       "23     -0.5401266123537254     -2.063037858185228    0.7903295406091944   \n",
       "24      0.7748772149266269      1.007049225269684    -2.598550376477518   \n",
       "25    0.033722718466987794    -1.9912838386154217   -0.5034523462491987   \n",
       "26      -1.645680234409779    -0.6407942894058258    0.6969489656752149   \n",
       "27       1.508355120266887    0.15544075181361283    -1.315693573827952   \n",
       "28      0.7276736852596607   0.002439261226376138    0.8424081986778317   \n",
       "29     -1.2564765451339706      1.497879673804852   -1.0646800563354735   \n",
       "...                    ...                    ...                   ...   \n",
       "1170    0.8976313155683101   -0.31703987488417973   -0.8745022769774832   \n",
       "1171   -1.6713478134411535     0.9116232828082541     1.517650315304898   \n",
       "1172  -0.16068257940158517    -0.7986866278935425   -0.8830285528488023   \n",
       "1173    0.4430117792380237     0.4534299417404605   -1.1677908231581569   \n",
       "1174    -1.136898536378983      0.302487179610313   -0.8576396156209122   \n",
       "1175      1.39782593082273      1.077573547898431   -0.5305936941620435   \n",
       "1176    0.9466983329354152    0.20756420125298924    -0.323115108421562   \n",
       "1177   -2.1846922331218104     0.9501961584766616   -0.5950071375396341   \n",
       "1178    0.5285223381199159    -0.7255727741729106     -0.98087970760598   \n",
       "1179     2.066097290412573     0.5406876506228713    1.2068698860452327   \n",
       "1180   -0.6164970340708004  -0.024971807504163085  -0.45416620045024747   \n",
       "1181     0.411309551030833  -0.027638249570701046   -0.2963547026992954   \n",
       "1182    -1.073998237883779    -1.5914545525178225    0.9618468556898917   \n",
       "1183   -0.9110671731204575   -0.05681405001318976    0.9160254682046123   \n",
       "1184   -0.8358364280442306  -0.004574777695534396  -0.40191599746417705   \n",
       "1185    1.1078863371188161     0.2588890724753168   -1.7303112703176096   \n",
       "1186  -0.10643602148366063    -1.1135306934174836    -1.273002670907794   \n",
       "1187    1.4920885245879245     0.0483561033140756    1.0986894553411708   \n",
       "1188   -1.0325629394558722    -1.1045617287918557     0.941934111648276   \n",
       "1189   0.03783236475091617   -0.07233283283812192    0.6622595637412135   \n",
       "1190   -1.2099176959758922     1.5102810650843936   0.24069442971470778   \n",
       "1191   -0.5123697580052905    0.14848195415350673  -0.23185691638913644   \n",
       "1192   -1.4650124931154587     0.5003312681163836    2.2759060522456185   \n",
       "1193   -1.7726014536212062     1.2897123082740756   -0.3856107062563472   \n",
       "1194   -0.3856580008631578     1.3153691810754826    1.2101943081458408   \n",
       "1195   -1.2505285569243703    0.22032703776013496  -0.47571124862153125   \n",
       "1196   -0.6497717529832614   -0.21433096839151625   0.40702530983324037   \n",
       "1197   -0.5737421576565819    0.35198040244381146    -0.613341146916984   \n",
       "1198    0.5528234868525234    -0.5743057341622545   -1.7454375533232127   \n",
       "1199    0.5826740951134363     0.4992268641896448  -0.09667516186692057   \n",
       "\n",
       "            num.feature 12         num.feature 13         num.feature 14  \\\n",
       "0        4.616519808018674     0.6808881962924479     0.6730755131774082   \n",
       "1        6.195669313471689   0.023358605844600047     0.7458273315641616   \n",
       "2      -6.2893705673957045       1.44246196213609     1.6162737578517006   \n",
       "3       2.1408898893771773   -0.40531870135667847    -0.8114189854928818   \n",
       "4        7.619684323987867     1.1445597193621535  -0.022261203920839176   \n",
       "5       0.5034622899916964    -1.9053300352452838    -0.3058320266979141   \n",
       "6        -5.02730274943831    -1.1428223814810738    -0.9283781063301043   \n",
       "7       2.6268775634791734    -2.0100465281749567     1.2987248939888258   \n",
       "8       -2.826627627750431     0.3617126702745833     0.2550872662485728   \n",
       "9        -4.11292077950246    0.46667267210749463    -1.1122433489030028   \n",
       "10     -1.2483396743352393     1.9162494382646522    -1.1483875112963726   \n",
       "11       2.161842158976029     0.5060259938882976     1.5609124655693671   \n",
       "12      2.9018961364973053   -0.03430656904330251     0.2877939344588698   \n",
       "13      -3.763840517121795    0.49834069908836076      0.267904171387497   \n",
       "14      3.2738661698150766     0.8517548079055359     1.1353836285809635   \n",
       "15     -0.4354775834134156    -1.0335247818034208   -0.14210148978187756   \n",
       "16       1.917620368257194    0.27770785412319443     3.1111689457439486   \n",
       "17      -2.854722427278214    0.18916948994144783    -0.6375885177028534   \n",
       "18     -0.3622884473532183     -0.805747072300128      1.273831153113548   \n",
       "19     -2.5884145456634835      1.713477792008218    -1.3103932981658883   \n",
       "20      -1.286837809862283   -0.10989885379219791    -0.4084880659074859   \n",
       "21       2.562931566693169    0.41006067200414287   0.029653047797607024   \n",
       "22       2.472282580554513    -0.3205317765448241    -1.3285805391421979   \n",
       "23      0.4169802204854367      1.926703914834181     2.1480623386104476   \n",
       "24    -0.15755296933968008    0.09961631292645666    -0.3824212326968105   \n",
       "25      -1.445496018331568  -0.004194218704708624     0.3829481007684855   \n",
       "26      0.9929224933340147    -0.7590530539748105     1.0714019100059533   \n",
       "27     0.20775798390808345    -0.5809770435838787    0.21010192871626804   \n",
       "28    -0.34871066047306487    0.15726930851059437   -0.07015062935661162   \n",
       "29      -4.943882473907434    0.12212184647245884     0.1513733743125254   \n",
       "...                    ...                    ...                    ...   \n",
       "1170    3.2190876548974106     1.7679814920512915     1.5994377639953254   \n",
       "1171  -0.33959340397101434    -1.1065237769479985    0.14831133004977215   \n",
       "1172     5.441334362552395   -0.04371642368474271     0.8578527904940038   \n",
       "1173     0.618170200596676   -0.09085463866265511   -0.21075473350188842   \n",
       "1174  -0.39527493063536745    -0.4505777841517975   -0.14186875049607517   \n",
       "1175    1.8854023322068052    0.12485899994861317    -1.0547796721533536   \n",
       "1176    0.2538087589362772    -0.3436394963180437    -0.8647656453532908   \n",
       "1177    3.0746386973069204   -0.19108995444383123    -0.4486716284181457   \n",
       "1178   -1.9107613459254056     1.2965542513769552    -0.7026111854966822   \n",
       "1179   -2.3080747502335917     1.3960888105914664     0.8992406128911524   \n",
       "1180   -0.2949092991340999     0.6543345126296674    -1.1315097611901677   \n",
       "1181    2.4335367324992077   -0.05987898731290267      1.889175609061096   \n",
       "1182     5.074992500157688    0.26934073538821207    -1.0537310487250304   \n",
       "1183     1.733680949956994    -1.0780195505343118    -1.2937515231532881   \n",
       "1184   0.06616607398009858      0.643722984977031     1.2391240866363762   \n",
       "1185     0.656849879250122     -1.051650843220058    0.27230937164348823   \n",
       "1186    1.6827692082079075     0.3435779323908643     0.6783880961589769   \n",
       "1187   -3.0388218008568786      0.783766684056295    -2.0659613788392512   \n",
       "1188   -0.2697439724566898    -1.3226550898756675     2.0874826651506586   \n",
       "1189     4.702046413497952     1.6419810290536057    -1.8223820170852811   \n",
       "1190    2.5581790691072688     0.9472677623423394     1.6798061082091258   \n",
       "1191   -0.9680642551593782     1.4752653354322334     -0.266479361487191   \n",
       "1192    0.3803421498066083    -1.2216076611838673    -0.9708798934946584   \n",
       "1193     1.103607149769532   0.027953894675313493    -0.5227695331457697   \n",
       "1194     3.316580916552174    -1.2147134689335644   -0.09146320371261416   \n",
       "1195     4.509741883545093    0.15526609881212555    -0.6208064587643621   \n",
       "1196  -0.23403552934786717      0.628647318752534     2.9470882303759285   \n",
       "1197     -1.43220746734427     0.2022161400290321     1.6347112482164654   \n",
       "1198   0.13776404233392694   0.055445344919623496    0.24221469346730456   \n",
       "1199    1.2551926190025722    -0.4999794907831364    -2.5646597034124747   \n",
       "\n",
       "             num.feature 15        num.feature 16         num.feature 17  \\\n",
       "0      -0.08742126693713967   -0.2651697228510659     3.1860948215502742   \n",
       "1         5.188554130471982  -0.15475539955618664     -0.416003130089185   \n",
       "2         2.094705618605038   -0.9239445521479827     1.3543952308200025   \n",
       "3       0.13282802694888865   -0.7215621772611183    -1.2725606946795458   \n",
       "4       -1.5140761508081317   -1.3092885395699658     3.9539260643442504   \n",
       "5         2.771583580245775    0.7353841609280962    -1.6880012423279134   \n",
       "6       -0.9964377150124951    0.7562460348614661    0.16854553995602686   \n",
       "7      -0.02238385011968899    0.5830178509656607    -0.1375156063423236   \n",
       "8        -1.004669892330396    0.5244377928091789     1.4422073317693438   \n",
       "9         2.291544530132535    1.0244512184239927     3.7576492386602447   \n",
       "10      -2.8173094288780374   -0.4832005881506155    -3.1638094878762293   \n",
       "11      -0.9042494192497191   0.12964406135995646      4.438044645675577   \n",
       "12      -0.9504647913647432    -0.783600094163428    -0.6000273323364559   \n",
       "13     -0.07868024996720654   -1.0414680243647731    0.31696304031917577   \n",
       "14       1.5544245948295474    1.5492510639146593    -1.7898767749577034   \n",
       "15       0.4457393539325195      1.62591257207219     -4.136359099203966   \n",
       "16      -0.7099038090744612    0.9517396066983568    -0.8964712090146774   \n",
       "17    -0.009694862409361765    1.7457069657602675     1.9635767124129062   \n",
       "18       0.3470369932742986  -0.20778328404424076    -0.5266931843797311   \n",
       "19       0.6397205434078544    0.5398050186212242     -3.364655440372976   \n",
       "20     -0.40707085673290677    0.3708379768752844     0.1554783109674808   \n",
       "21      -2.7715510226316575   -0.1549628150906113  -0.015656866701561162   \n",
       "22        2.461557693405206    -1.001888306288588      1.177932054624348   \n",
       "23       1.4455094658778886    1.2706416265329488    -1.0973750429191869   \n",
       "24      -1.1189424681420006  -0.31312012262965516     2.4068822588636807   \n",
       "25       1.4111659491075945    0.6203733899191238    -3.1535924476574837   \n",
       "26        2.314722643317221    1.4371002634430396     3.5936623101404703   \n",
       "27      -2.8197940934744894    1.3933333979147235    -1.0918223138471748   \n",
       "28       1.0158330652920038    0.8120744904572086     0.9051740724523839   \n",
       "29        3.628274431825756   0.25195461332781827    -2.6253761643457865   \n",
       "...                     ...                   ...                    ...   \n",
       "1170   -0.32822654301588805    1.8704785658619376    -1.2968564257886397   \n",
       "1171      1.773549237075518    0.5294697741134579     -2.200681857803975   \n",
       "1172     0.1645615431220804    0.4705634311050222    0.10382443153643509   \n",
       "1173    -3.4446056258032005   -0.3701809909948538     1.6107987781120632   \n",
       "1174    0.28718343843631666     3.107393897873795    -2.8068849922584627   \n",
       "1175      5.710290841815588   -0.3392036769370268     -4.400349299549187   \n",
       "1176     3.0770896405116566   -1.8836858755351107    -3.9447494065054465   \n",
       "1177    -0.7574053969399407    0.3567017604671636     0.7520002074264485   \n",
       "1178     0.5831288659612176    1.8500732588426205     2.1569532974067305   \n",
       "1179     3.6383788085526763    0.5716145069176645    -1.0116457817083477   \n",
       "1180         4.547168422816    0.7613780771045985     0.1497505187396261   \n",
       "1181      3.215582182144786    0.3352501674849724   -0.46328314385542957   \n",
       "1182      3.213724879732177   0.40308574389718643    -0.7169527775060137   \n",
       "1183     0.9193441463558475   -0.9005873631610734       4.04405323236104   \n",
       "1184    -2.5003865457409264    0.8051192812646061    -1.8561248274543916   \n",
       "1185    -1.2887618933099945   -0.8978165965785845    -3.2737538828922004   \n",
       "1186    -0.2473708871877985   -0.9448376458392073     0.7893012604655022   \n",
       "1187    -2.4168674433540267    0.7457628044046148    -1.8546119644506052   \n",
       "1188     1.4192642585320496   -0.9348092570568097     -5.283253340398339   \n",
       "1189    -1.7608825316885288  -0.07964595560303947     2.7396199508260257   \n",
       "1190     -1.505459307596925   -1.1425541752902488     1.1611081118185376   \n",
       "1191     2.5927848947635703  -0.37922446587097036    -1.4729464617219161   \n",
       "1192     2.2720380564641416   -1.2314991150820267    -1.0181128351666389   \n",
       "1193      2.500161010986847     0.990015805244922   -0.41523933676732117   \n",
       "1194     1.9712728620274473    0.2514737313369846    -0.4329999389812048   \n",
       "1195      2.364672566041228    0.8323998734214928    -0.7433067045786835   \n",
       "1196     1.2048886679880781   -0.1786262894535223     1.4730329370196436   \n",
       "1197     1.7307193492606703    1.2709205990042762    -2.6688461301698676   \n",
       "1198      3.477546997369153     -1.21730080394277     -2.262065501478789   \n",
       "1199     1.5171895615504303  -0.08706333580876527     -3.455732985711092   \n",
       "\n",
       "            num.feature 18          num.feature 19        num.feature 20  \\\n",
       "0       0.2884074352632435      0.5738177798006181     4.190982582810352   \n",
       "1      -1.4060201020375653      0.6506604070563828    -9.614876401476089   \n",
       "2       1.3675549575475838      2.0596880344459954   -3.5218346365348845   \n",
       "3       -0.713358914048789      0.3825142723790069   -1.7828476331602812   \n",
       "4        1.450660955152561     -0.9991184296745591    -2.009372814613767   \n",
       "5       1.3842611347741567      0.1806280850746561   -1.5715664849937567   \n",
       "6       0.7067486420847652      0.9212623184825482    -2.597130728617354   \n",
       "7     -0.03156288810640315       1.071406697458374    2.9798753196690733   \n",
       "8        1.169864280656566      1.0291192042873833      5.32660490385566   \n",
       "9       0.5630644178350591     0.18506120750058808   -6.6180137842633995   \n",
       "10      1.8337000180829752     0.29706380660044074    1.1699700149567436   \n",
       "11      0.3585423886335166      0.7508876610445616    3.0724571086440644   \n",
       "12      0.6668295583976607      0.2972718774740335    -4.061484708183912   \n",
       "13    -0.09150902457057936     -0.8395421433969492    3.1530355479593957   \n",
       "14      0.8019263211421492      0.5893586372961818    0.8714076333537533   \n",
       "15       0.559724898619284      0.1326138357511536    -7.079904562540768   \n",
       "16      1.0248301327553673     -1.0606312052010969   -1.4653800280407734   \n",
       "17      -1.395698709828056      0.3474081355843538     4.942007291668665   \n",
       "18      1.4879872508427707    -0.04984446028648007   -4.2175837300685375   \n",
       "19     -0.8252966787252827     -1.3946081912148496   -0.1569810903067758   \n",
       "20     -0.7994244073710132  -0.0014377351024276452    0.9953317805193768   \n",
       "21    -0.10921189522577932    -0.42654940840973143     6.457858953762559   \n",
       "22     -2.2890454464000953    -0.42073025749244775    -6.455470074885587   \n",
       "23     -0.5417091182913282      1.6659316237203494   -1.9329764779195173   \n",
       "24    -0.11261226091407048     -0.5616981467221734   -1.6308427528615626   \n",
       "25     -0.3741970934455955     -0.6891605179480794   -0.7575420574679261   \n",
       "26     -0.8860713366733567     0.13058487014132097     0.613508597000374   \n",
       "27     -1.2553092131369086      1.0755233919356098  -0.30822852052276106   \n",
       "28     -1.3610658999297116     -1.0463004749334435  0.059444197770049764   \n",
       "29    -0.05998943148713057      0.7587803996075067    -5.462940781912974   \n",
       "...                    ...                     ...                   ...   \n",
       "1170  -0.04798919578834268     0.20658789102264802    2.6088546979336082   \n",
       "1171  -0.09005738913774403      0.5116671387793217     -2.16385196883732   \n",
       "1172   -0.8986591447171908     -0.1225003679299076    5.4983254565611075   \n",
       "1173    1.6630900574498795    -0.04369903955902052     7.168841804875031   \n",
       "1174    0.4523159663943246      0.8254876299113421    0.6484722146967312   \n",
       "1175   -0.4684783477168748     -1.0542451400733732     -6.92949937364045   \n",
       "1176   -0.5551711613853215      0.7982087057273198   -0.5363786585608814   \n",
       "1177     2.217920622936063     -0.4802502387912909     5.269317648418774   \n",
       "1178     0.771323973482618     0.26390809689502115    -1.332551699484598   \n",
       "1179   -1.6798892052328613      0.7251096758267004    -4.766880390837934   \n",
       "1180  -0.27481145451137456      0.5709358316798155  -0.38920623199588655   \n",
       "1181   -0.9927201613296553     0.11446695549453945     4.510088399098215   \n",
       "1182    0.9249819291346241      0.6523556195231468   -2.1226421724295106   \n",
       "1183    -0.701448880467059     0.11182185312890804     -0.75233463097153   \n",
       "1184     0.784829030068205    -0.10630803220350596    -4.566878352637057   \n",
       "1185  0.004971631516108813        2.72135233442499      6.09378861544378   \n",
       "1186    0.9745168711703224    -0.19551595592651602     3.834309207302429   \n",
       "1187   -1.2692438313872676     -0.3567429824070466    2.2590663366804598   \n",
       "1188    2.1727422010940383      1.2070365208119884     7.102413363528593   \n",
       "1189   -1.9712738355268513     0.13246603104196525   0.34564906170647036   \n",
       "1190    -1.092960210997312      0.2990290396129368   -2.8613443847517583   \n",
       "1191    0.7625369766034309     -0.9210888134392853    0.9187425316433875   \n",
       "1192    0.9481913020013338      0.1704408958479472   -1.5791776268888102   \n",
       "1193   -0.7764808465473477      -0.525933577208267    1.5067776670395305   \n",
       "1194   -0.6812807409545232      0.5586241748349468    -4.225673409780895   \n",
       "1195   0.16073457942393404     -0.9915938923044053    -2.531455004744205   \n",
       "1196   0.15955802094577845      0.6107094311421618   -0.2235216755002389   \n",
       "1197  -0.27663384206442465     -0.6271746701563826    3.4131512367980523   \n",
       "1198  -0.18212451317786751      1.9925622610041367   -1.0616462021853428   \n",
       "1199   0.39660291230667444       0.912636353184481    0.5451092388804373   \n",
       "\n",
       "            num.feature 21        num.feature 22        num.feature 23  \\\n",
       "0       2.9961116566301853    1.0343228168083176   -0.5895395047491798   \n",
       "1     -0.32205287764062207   -1.8570570670815592    0.8000777999684054   \n",
       "2       -2.521186677581191     2.539471007050667   -0.6872770667753036   \n",
       "3       -3.408004805995034    0.4768822067812818    1.7680094586003825   \n",
       "4       -4.408703063070725    3.4731088093500855   -0.8336840546622525   \n",
       "5      -0.8219619826737724     1.104092968405503    0.8197035997197306   \n",
       "6       -2.616044387078954    -2.948227038366142    0.6873920680141749   \n",
       "7       3.3082957721258666   -2.1320151715044244   0.10630130979398261   \n",
       "8       0.2423598218467308  -0.31765719577961216     2.694093035067453   \n",
       "9       -5.640659894215979     0.830811712698912   -1.2329901149896603   \n",
       "10     -1.7562170501156156    1.3289615292329304   0.14539468951797108   \n",
       "11      0.9465505554545279    2.9315718426126627     1.382102128940184   \n",
       "12     -3.8161535814270278     1.020592044797977   0.04183905048114886   \n",
       "13      1.1984182560716286    -1.138748806293747   -0.6019524165338564   \n",
       "14      2.0639319534441234   -0.7802189615943217    1.1254523933306169   \n",
       "15     0.04627877158046548    -2.351252037122795    0.2709036968720826   \n",
       "16       3.060001357096573   -1.2009171904334712   -0.6595108049607487   \n",
       "17     -1.8230557979405526    5.0632411773972805  -0.37732465705119456   \n",
       "18     -0.6115109246846203    -1.559086855296752   -1.5223901430805777   \n",
       "19      -6.639339571496608   -1.3533996403294009   -0.8240387261147878   \n",
       "20     -1.2656431346334291    -2.246148435498638    0.2671804919323004   \n",
       "21     -1.4333922673871338    0.5646811622983611   -1.3045441785644938   \n",
       "22     -2.6007154718500973    3.5889221551317494     1.147380639731575   \n",
       "23     0.41818539868004667    2.0982118175513342     1.505075163484111   \n",
       "24      1.5865632500099391   0.05066117727284772   -0.6095846348800507   \n",
       "25     -2.0908328156534406   -0.9616963946761988  -0.07196910582067989   \n",
       "26       2.815877108325811    1.8857827466803032   0.24098612179396378   \n",
       "27      -2.947666321302357    0.8734491290142373  -0.24341912318410117   \n",
       "28      0.0775409623523731    0.9464512597325877    0.6425258556490333   \n",
       "29      -6.260739158950308   0.41542894546421294  0.015534066028263474   \n",
       "...                    ...                   ...                   ...   \n",
       "1170    3.8392669146567373   -0.5172889072828292   -0.3502459667363689   \n",
       "1171      1.15417355961463   -0.1511143750754613    0.5733821899190933   \n",
       "1172   -0.5287277944767154  -0.11668337678389559   -0.7867659220684182   \n",
       "1173   0.25392543610773743   0.17588077723926746   -0.7344894500547824   \n",
       "1174     1.611839361148677    2.7164397705838916  -0.13551810623672847   \n",
       "1175   -1.7136818426314355   0.04215985095240107    0.3225850634827796   \n",
       "1176     4.157478775449823    0.8009199443901573   -1.7309246916445078   \n",
       "1177   0.19563636274939325     3.197123118624416    1.2728868496894543   \n",
       "1178    0.7344140042034013    1.8929119164492225  -0.30018639677761494   \n",
       "1179   -4.3227454549937105  -0.20944529515690222   -1.6721884962117863   \n",
       "1180   -2.0901177902896775    -2.496758589514485   -0.6321984127415937   \n",
       "1181     6.911653115484638     1.247801935761855     1.218737737261192   \n",
       "1182   -0.6993142218365305   -2.6413021252474738   -0.6634639134840192   \n",
       "1183     3.731560222721702    1.3619104221217557    0.8727273900930504   \n",
       "1184   -3.6342209957805913    -2.038703384586579   0.29815531215049373   \n",
       "1185    -0.331141112574712    1.4241705735610837    2.3438966741840406   \n",
       "1186    1.5320701384360136   -1.0026642525333656    -1.704105852339399   \n",
       "1187   -1.0753958081300472   -2.5533965166302646    0.2669263969436074   \n",
       "1188    2.3108473107010803   -0.1497635744618204    0.4335647884378009   \n",
       "1189   -1.3702761421688545     3.800944888734973   -0.1452412174491898   \n",
       "1190   -1.1664251625096638    -2.267771929810628   -1.3276502297225148   \n",
       "1191   -0.7073597544254319    2.1831480651196746    1.5609570430227322   \n",
       "1192    -3.573303072355796   -1.3449942030746962    0.5703483722373734   \n",
       "1193    0.6370073034618512    2.4199252090460557   -0.6927820363145551   \n",
       "1194    0.5952473789089366   -1.3575544584443353     1.573345663180804   \n",
       "1195     4.345514496135831    2.1947044000194116    0.2309881027574295   \n",
       "1196   -1.2575751976100307   -1.7804500491580915    1.4509105946426832   \n",
       "1197   -1.1836704733467376    2.0331659077545883    0.3998898211061982   \n",
       "1198   -0.8510370634062078     -1.69499485334055    1.8469848441814392   \n",
       "1199    1.9940396621027274  -0.30577487946294035   -0.2847791896104451   \n",
       "\n",
       "             num.feature 24         num.feature 25        num.feature 26  \\\n",
       "0        1.0498511724513335      -4.98876034321755   0.24448168911127613   \n",
       "1       0.08218760403803332     0.9954991307626876    2.8747424812395947   \n",
       "2        -2.234257003388035   -0.45277656602690397     2.161038370562584   \n",
       "3       -0.3625841676276228    -1.3400761818240001     1.984914518287928   \n",
       "4      -0.45426307775195673    0.08135388254536915    2.0014432433952085   \n",
       "5       -0.5788022427307223      1.554276724873589  -0.07952293199052315   \n",
       "6       -1.4553550562241782      2.890133059871754    3.1034909624328355   \n",
       "7      -0.03210430562467975    -0.5495903248638354   -1.0489827758132027   \n",
       "8       -0.8200569879381142     0.4323748324386488     -2.52801195193798   \n",
       "9      -0.12277449697994541     2.3561344758337586    2.6151970649031973   \n",
       "10       0.5106626274528202    -2.1738107040543637     2.808996826486217   \n",
       "11       0.2602232472771473    -1.4023993909327195   -0.3642747911633514   \n",
       "12       -0.956999397611258     3.4582840682130036      1.16583145166447   \n",
       "13      -0.4364001946206655     3.2806741714050163   -0.5402327389263891   \n",
       "14    -0.041385377295665854    -0.7049180176098095   -1.9709212978851007   \n",
       "15     -0.07785897864927996     -3.619565503884099    0.6856896239480154   \n",
       "16       1.8324457937069425    -0.6611942340666496   -0.8740919282363157   \n",
       "17     -0.08322514271200768    -0.7169011218076725    -1.896702240844149   \n",
       "18        2.354279402197481   -0.34002696888041606    1.3067092962675573   \n",
       "19      -0.5250701934123424     1.7886465425371672   -1.0258268786749287   \n",
       "20       0.3719100469096662    0.25026758768378055    0.8307510351343125   \n",
       "21      -0.9948492136822421      2.862954639826133  -0.09121882400241507   \n",
       "22      -1.1784768908744205      0.641460277748327     5.423985635669208   \n",
       "23       0.8906083476658132     0.6653811619719613    1.5792276809614492   \n",
       "24      -0.4049324672230167     -1.126107200809527   -0.5484863648620234   \n",
       "25       0.2393313406777689      2.994192625705967    1.1907391280019706   \n",
       "26      -1.6439399226423361     -2.663042099611651    0.6803182639819778   \n",
       "27      -1.4883429909113477    -0.9396887169247236    3.3431419480905884   \n",
       "28       0.5353710219745076    -1.4624124933541212    2.1845000587690397   \n",
       "29       0.6330171485528483      4.865544473209683   0.35445390996927517   \n",
       "...                     ...                    ...                   ...   \n",
       "1170    -0.4905618925528823    -0.9527474220684239   -0.9422054036750068   \n",
       "1171   -0.41024783051540026    -2.1491245789132782    1.1623223957566267   \n",
       "1172   -0.05690055303298877      5.185219222745553    -2.073045100611649   \n",
       "1173     0.1911851654682032     0.4543033336476663    -3.473350091870199   \n",
       "1174    -0.8793058582543944     0.6950269419695012   0.18523428556903135   \n",
       "1175    -0.6358997981161512     3.6945263304256333      3.48054050765064   \n",
       "1176    -1.4201358748782489     0.2914013381071281    0.4681932192268057   \n",
       "1177   -0.22206224224085916   -0.34146429211857565    0.7501068504421241   \n",
       "1178     2.3507564630097377     0.7016612633909126   -1.4799858201181846   \n",
       "1179    0.06633932279885599      2.638043563753404    2.0650588396666834   \n",
       "1180    -0.8634197858922311     2.5325837748171933    -1.812824334651895   \n",
       "1181    -0.4602838589532928    -1.4569160399066596   -2.1164246789172045   \n",
       "1182     1.0167203846556607    -0.6635971066343154   0.21434969453182898   \n",
       "1183     0.6567275034869384     1.6435358724284725    2.1459656008476338   \n",
       "1184     0.3250880877977148    -2.0016859279283237    0.8471471607256565   \n",
       "1185    -0.9251915008697665  -0.017020612806081997   -2.4404898742421506   \n",
       "1186     -1.976278420981215     0.5221872815082007   -2.6982309807096816   \n",
       "1187     0.7941307155015527      1.219377009243913    1.1226837503264984   \n",
       "1188    -1.6009769736667907     1.0635982156080506   -3.8571078639721565   \n",
       "1189    0.07968275664501623    -1.5796225896953313    0.7135917114346542   \n",
       "1190     -1.539472140922785    -1.2338484762276147     1.081225948890778   \n",
       "1191    0.22327717700077215    -0.9653564344849971    1.0407556529556043   \n",
       "1192     -1.665215706139557     1.8531390276740485    2.8854935495834817   \n",
       "1193     0.9331499487319751     1.1817385465989874    0.3605993844196098   \n",
       "1194      1.001883998733843    -1.3771223817895453    1.6108351107524774   \n",
       "1195     0.5283731400228635    -1.2232420303996334     1.861636035732857   \n",
       "1196    -0.8020784150840996     0.6903339912697565   -0.8565193285077014   \n",
       "1197     -2.086268004125758     2.2417337542669467      1.00244011733166   \n",
       "1198   -0.06235366598167899     3.8761159965137204   -1.2839531083461222   \n",
       "1199    -1.4654551059727134     1.2242305811562204   -0.6079537787114343   \n",
       "\n",
       "             num.feature 27        num.feature 28          num.feature 29  \\\n",
       "0        0.1383888203646912    12.733733188446942     -0.8323277554390629   \n",
       "1      0.024378006410350678    -4.526591825701548      1.4579170472956164   \n",
       "2       -0.8736782865731195    -6.071563370820246    -0.06540227028915366   \n",
       "3       0.03431028675969114     2.025872914529067  -0.0013047645060802748   \n",
       "4        0.6317503120796889      5.91531061956587      -2.804864347560359   \n",
       "5       -0.0332699311968469    -4.348792076074895      0.9306531675027093   \n",
       "6      -0.16753346342461659    -5.101868263929028      0.6203073733554962   \n",
       "7     -0.004341291297629821    2.4013787650307754     0.45827142016911127   \n",
       "8        0.4361646453024043    -4.385683942407756       2.387121786719825   \n",
       "9        1.8953063443289595    -8.202601941572349     -1.5405878360742586   \n",
       "10      0.08961499714775413     5.027466791875707     -0.2947972572478632   \n",
       "11       1.9283496233241848    2.2021909037590564  -0.0028010284623413984   \n",
       "12       0.6200418419990028    -7.627836218548753     -1.0761111326176778   \n",
       "13      0.18295947848235028    -9.576190030044943      0.8929141082090849   \n",
       "14       0.8206452841951103    0.9950091510754292      0.6344723722949713   \n",
       "15        1.150583564046325     5.391982332168546      -3.366979246956951   \n",
       "16      0.21394254249916894    0.6041910174061222     -0.8745671279210103   \n",
       "17       0.9793548550360717     5.603374217084929      -0.806632446644984   \n",
       "18     -0.44564814913122686   -0.3871145261858975    -0.46307238242099713   \n",
       "19     -0.06313180457126966    -3.129167856770784     -2.5080646670282163   \n",
       "20       0.9255106073203831   -2.2401558273972353        1.22908647174581   \n",
       "21       0.3400443886058067    3.7648959503264985      2.5206537501932806   \n",
       "22      0.27326500476344073     -4.79813101972001     -2.8415039186047513   \n",
       "23     -0.13972283280533276    -6.441037274929541    -0.02394248164985399   \n",
       "24       0.9026717713030781    0.7020963885626733      -3.595560533973377   \n",
       "25       0.9679227523174557     -7.38264969518866      0.4284836002298431   \n",
       "26       0.6492100115348427    3.2574888306785765      1.0732076273334608   \n",
       "27       1.2122576512447376    -3.082666953618551      3.1624006587275346   \n",
       "28        0.890118632529928  -0.07746465900430452      1.6408316699427252   \n",
       "29      -0.4537599089433162    -9.410709508728909      -5.011719983891007   \n",
       "...                     ...                   ...                     ...   \n",
       "1170   -0.36374559425956154    -2.999746006816707       3.233247508721702   \n",
       "1171    -0.3203555175230647    3.3678611076317053     -1.9710507897923804   \n",
       "1172   -0.10661541671084931    -3.132837406005032     -1.3217134605781062   \n",
       "1173    -0.7803410180485866     5.232744900173913     -0.8287102346401033   \n",
       "1174    -1.0403904314211643     -7.46213282096412       2.807925986558534   \n",
       "1175    -1.5013616422848837    -7.583622170759153   0.0006612452223352661   \n",
       "1176     0.9798767828783856  -0.49639672883641206      0.9770737348591859   \n",
       "1177  -0.009303098055854164     4.086263274244224     -0.9733416641563307   \n",
       "1178     0.7076003897006045   -3.6709842954254985      -2.821301113555421   \n",
       "1179     0.6775389574302096    -2.668754940790324    -0.15420480739908368   \n",
       "1180    -1.0936137676644242    1.0650951962138904      1.9780666975451489   \n",
       "1181     -1.668745775418122   -0.2821166385601971       3.673722535956881   \n",
       "1182    0.16376978937060832     5.524448613748559     -1.3436608737117721   \n",
       "1183     1.2468781094614638    -3.013708121787551      0.8610187201244269   \n",
       "1184    -1.9113186590375766  -0.45327462186108824       2.712898613959792   \n",
       "1185     1.2332735142643654   -2.9490242559843782       3.752660920524056   \n",
       "1186     0.7929185029539482    2.9469008522214186     -1.3931795725578817   \n",
       "1187    -0.5046974910966262    1.6425325094740166     -0.6383728696394436   \n",
       "1188     -1.187706713979012     2.625428501308728       1.244394720323839   \n",
       "1189      -0.48210931519557     8.644217707740694     -3.2497492163264075   \n",
       "1190    -1.7227291803732574   -0.6690250782465482      1.9706879526535241   \n",
       "1191     0.8679319019031883    1.7464643788787737      1.1439874560707757   \n",
       "1192     1.0969729779935657   -2.7249737524633146      3.2300080371224307   \n",
       "1193     1.4173850138234776   -3.0752627695835355    -0.12983821166654272   \n",
       "1194    -2.5427235816259546    3.4000287343095175      -2.020270346239041   \n",
       "1195    -0.6243179765849484   -1.6672759931310073       1.425215944471904   \n",
       "1196    -1.6515800383154506     -6.33145723464855       0.363897031914182   \n",
       "1197     0.8312945044540551   -2.7449339056720556       3.236162784324399   \n",
       "1198     0.6555497995580744    -4.037787405730562     0.48066437763530845   \n",
       "1199   -0.09159479217414465     3.904210372925601     -0.3723720122381162   \n",
       "\n",
       "            num.feature 30         num.feature 31        num.feature 32  \\\n",
       "0        3.144878740193841    0.12085391702036802  -0.02292218197000398   \n",
       "1      -3.9400304528827994     0.9550817747742019      1.46598584281507   \n",
       "2        0.644367853055584     0.9017071775335213    0.6687721887419774   \n",
       "3        1.163716371655199   -0.34438202044532085   -0.5874840931411275   \n",
       "4        5.045459342060153   -0.20542189922195633  -0.36398641628325556   \n",
       "5       2.6560934699502337   -0.12894249709012146   0.07671673878010135   \n",
       "6      -1.0207243359919402   -0.45407958291068573    0.3036156869582973   \n",
       "7       1.0075411892989439      0.624335975522604  0.037032451176477674   \n",
       "8       0.8889567630927903    0.23686320369214414   -1.7843004506231512   \n",
       "9      -1.4554362459532166   -0.24754099013395833   -0.8399495746478683   \n",
       "10      3.5527522844723007     0.4472670134842496    0.5622997875910306   \n",
       "11       3.449943848663294     1.0330499769520591    -2.389514456146183   \n",
       "12     -2.7596185235532267    -1.2904629033006747     0.349524685448047   \n",
       "13      0.6788067663023037     0.8256240109477097    -0.386437822463787   \n",
       "14     -1.6201498623991966    -0.9118674672790038     0.065658251362515   \n",
       "15       3.135943948404593    0.46443470421847655    0.5877902350444172   \n",
       "16     -0.7552202301472909    0.06284668868374386   -0.1913500417873185   \n",
       "17     -0.9891387456710531     0.0788363888486702   -1.4420875900408243   \n",
       "18       3.384635915910931     -1.554200072922378   0.24810410193904675   \n",
       "19     -1.3873607248775532   -0.12496766088635478    0.5144160590471033   \n",
       "20     -4.3423971600575495     1.4322225501404204    2.5746926762331763   \n",
       "21      -1.253923825796075     0.5141657294567784    0.9641480693023237   \n",
       "22      1.6216202470875107    0.24770092445222053   0.23791289918411776   \n",
       "23      0.4311903549757613     0.4680647463651283   -1.3581526916322613   \n",
       "24      1.2922431587749987    -0.6189210153474656   -1.0482285682325367   \n",
       "25      3.0971901448289665     0.5046859142838822     1.971076587744378   \n",
       "26       4.039487072132357    0.14569233726882358   -1.6192022608409509   \n",
       "27       0.500655085041283     1.1782390571266803    0.6961958341223696   \n",
       "28      0.4605310325983655    -1.0256097170909406   0.36297228256529623   \n",
       "29     -0.9469126451506257     1.4862056106481196  -0.12542944122533664   \n",
       "...                    ...                    ...                   ...   \n",
       "1170   -3.0461663541450665     1.4885951744762722   -0.8417279864979065   \n",
       "1171    3.6645312947360176   -0.18758587987494516    0.5375661437236665   \n",
       "1172    1.5685251131096432  -0.025732128748592514  -0.20640119980647673   \n",
       "1173     1.253693346689272    0.27359933121174995   -0.9966156655049195   \n",
       "1174   -1.5173159357105397     1.5454919362855155    1.6337186215031823   \n",
       "1175  -0.28323804216496995     1.1847127558323403   0.43849439821266845   \n",
       "1176    0.6972072661827226    -0.4891005238723114     0.286950675823082   \n",
       "1177     4.344466269491614     1.9584961063681623     1.424045371779212   \n",
       "1178   -0.8110749185916141    -0.4787850260814426   -0.3990563746704292   \n",
       "1179    0.3458259073350073     -2.254917614549761    0.1682578038866202   \n",
       "1180   -0.9492018698243917    0.10668027699532659     1.007085316087386   \n",
       "1181   -3.4131436183933355   -0.15784945828762456   -0.6727397460191185   \n",
       "1182     2.462453852531733    0.23449855730270497    0.7442535953253104   \n",
       "1183  -0.26546462432717177   -0.29737452693623334   0.45564689645966105   \n",
       "1184   -0.5287944645420087   -0.16530994276743585    0.7994906149107759   \n",
       "1185     4.170543427769195   0.048222579290120686    0.5889719273060283   \n",
       "1186    1.4707929916835887     0.8389363333401786    0.2534108321372878   \n",
       "1187   -2.7778601930431153    -0.8935679788559023   -1.4816970917897987   \n",
       "1188    -4.901062322408849     0.6168911396487382   -0.6647731742814764   \n",
       "1189     3.196057030689897   0.049479531314287985      0.32756766242208   \n",
       "1190    3.4400558968611024     0.8485837984465023  -0.33270275417937323   \n",
       "1191    1.1221210988456465     0.5671564526096289    0.9622644491994143   \n",
       "1192   -2.2014573890129943     0.8386560296136905    1.3861260217912657   \n",
       "1193     1.077457789294931    -0.5935339738952637   -0.0107556146062327   \n",
       "1194    4.5254905476274505    -1.5584977224395449   0.24711891306325004   \n",
       "1195   0.48605316671294285     1.2281282435023066     2.083503295227778   \n",
       "1196   -1.5114293309653983     0.2418422790153796    1.1999320580998245   \n",
       "1197   0.09461717277185713     1.4497082383345115  -0.17725146918042853   \n",
       "1198    0.5296330432171811      -2.57180159165048    1.5677328897504648   \n",
       "1199   -0.7674918991666885      0.893122346457367   -0.8657537538131193   \n",
       "\n",
       "             num.feature 33         num.feature 34        num.feature 35  \\\n",
       "0       -1.9719619919503475    0.21237607040740009     7.124590112731301   \n",
       "1        1.2647753222284333     3.3286913493038273    -7.046315021752932   \n",
       "2        1.1048654203483457     1.0166929319060005     4.950654571548156   \n",
       "3        2.9165003844150528     -0.386704638315042    4.7163399882388815   \n",
       "4        2.5909247213031943    -0.5008246489966467    11.465985173802945   \n",
       "5        2.2429631694575307    -0.4849647887635882   -0.2420768884561871   \n",
       "6        -1.071500799009482     1.8151328582344022    -2.659020875050966   \n",
       "7         -1.64140876418702    -1.1771539846232675    0.8612641060571655   \n",
       "8         2.450991981406149    -1.1719382248460153     5.273762630312087   \n",
       "9        2.0578413248771374     0.2363877791045608    2.3344976843492478   \n",
       "10       0.6757493786006363    -0.4324510417452522     6.029251456160692   \n",
       "11       0.6819515844656311     0.9838665068182113    10.587192812945524   \n",
       "12       3.4423838457341236     0.7866478283816569     1.019582181337215   \n",
       "13     -0.49093534043740317   -0.37513063608497854    2.2098246072607286   \n",
       "14      0.47085114364991354   -0.39598623045043346   -2.9860747258321156   \n",
       "15       2.1444274678642614    0.15025608808119673   -0.1960447385307176   \n",
       "16      -0.0685923327637595    -2.1589869907471293    -1.667757094766148   \n",
       "17      -1.2846877775694432    -0.9008855831633328   -0.7781574435379659   \n",
       "18         2.57721780037592     1.1550149273869594    1.1497009299556398   \n",
       "19       2.6742601798485186  -0.048643902649866094      1.00301490597781   \n",
       "20    -0.018909775772514154    -0.8521257950923476   -1.0090983135716574   \n",
       "21       -1.647895210361943   -0.25989309220852763   -3.8857150968431786   \n",
       "22      0.08164770210929465    -0.7060728495741068     8.209852007425441   \n",
       "23         1.81660061990504     -1.013014300051717     2.791671065052714   \n",
       "24     -0.15088628999624906      0.155673263976993     5.378694758171784   \n",
       "25       2.1615971158775236      1.212945738781197      1.17331180091785   \n",
       "26     -0.36772664507595965      1.228658488757667     5.435992194737465   \n",
       "27         4.52156280376291     1.0595201867106254      8.33233879728386   \n",
       "28     -0.21220464600118424    -0.5307194153703407     4.029525138125566   \n",
       "29       1.2397466590289297      1.010092736770287     -4.62157143796051   \n",
       "...                     ...                    ...                   ...   \n",
       "1170     0.6741085179044058    -1.1316165313462891     0.928810487974822   \n",
       "1171    -0.5902419322633622     0.0615232658263072    1.7135041753730722   \n",
       "1172   -0.28887015185244425    -0.9109585404926773   0.39234548759550336   \n",
       "1173    -1.0983497779321312    0.11140745134835475     3.523976843010388   \n",
       "1174     2.4243348895779038    -0.4855176968345315  -0.35548022210897756   \n",
       "1175    -0.2886388954201039     1.0802157525822464    -7.880462353590589   \n",
       "1176     -2.697097495463281     0.9842372868381063   -7.5310253802046265   \n",
       "1177     -2.096262454711408      1.344993820313971     9.573596199615139   \n",
       "1178    0.18085087991487883    0.30481925433822266    1.1486171393884104   \n",
       "1179     0.6356762035436628    -0.5741744993929256    -6.648817829603208   \n",
       "1180   -0.15925911135486878   -0.06636545511788838   -11.818504194414974   \n",
       "1181    -3.1476555367824006   -0.43474105957744624    -4.977556003641755   \n",
       "1182   0.040776023392586375     0.8759850562175853   -1.1692579562905703   \n",
       "1183    -2.2307804306654684    -0.3617303041969676   -1.8082835630458323   \n",
       "1184      6.986343916630998      1.039525825188455    2.8408139142385753   \n",
       "1185      4.807809304648286    -2.0719350171579785     7.503542537475342   \n",
       "1186    0.02913805849390219   -0.48355581037101736   -0.7632400969616091   \n",
       "1187    -2.7015660093680043    0.36079302337131447    -4.585715924482174   \n",
       "1188    -3.8246440258682415    0.22861391160381067    -12.03845256597095   \n",
       "1189    -0.4246741540360265     0.9105795504835837       7.8856976084311   \n",
       "1190      5.412837905399699   -0.25691706671012293     7.414895635157644   \n",
       "1191    -0.2641290188665565    -0.7913604870429379   -0.8068225621897305   \n",
       "1192     0.6252100235865369     0.8597417148779872    -4.274870765460767   \n",
       "1193   -0.27802236500228383     1.0355773742424188    0.9321275704383634   \n",
       "1194      0.695333053328436     -0.507316125618687    2.6672662191010548   \n",
       "1195    -0.5111341178879835    -0.2889085169181339    0.7890077247404356   \n",
       "1196     3.2993390440385997     1.1488389619561448     3.374621495378453   \n",
       "1197    -0.2843975661069681     0.4265312809270644    -4.407640584452011   \n",
       "1198     0.6607932081267054   -0.24538740678723966     -9.34294271988492   \n",
       "1199     -3.461871473181544     0.9442083140957253   -10.526892625446957   \n",
       "\n",
       "     cat.feature 1  \n",
       "0                A  \n",
       "1                C  \n",
       "2                B  \n",
       "3                A  \n",
       "4                A  \n",
       "5                A  \n",
       "6                A  \n",
       "7                A  \n",
       "8                A  \n",
       "9                A  \n",
       "10               A  \n",
       "11               A  \n",
       "12               B  \n",
       "13               A  \n",
       "14               C  \n",
       "15               A  \n",
       "16               A  \n",
       "17               A  \n",
       "18               A  \n",
       "19               C  \n",
       "20               A  \n",
       "21               A  \n",
       "22               A  \n",
       "23               B  \n",
       "24               C  \n",
       "25               A  \n",
       "26               A  \n",
       "27               A  \n",
       "28               A  \n",
       "29               A  \n",
       "...            ...  \n",
       "1170             A  \n",
       "1171             A  \n",
       "1172             C  \n",
       "1173             C  \n",
       "1174             C  \n",
       "1175             A  \n",
       "1176             A  \n",
       "1177             A  \n",
       "1178             A  \n",
       "1179             A  \n",
       "1180             A  \n",
       "1181             A  \n",
       "1182             A  \n",
       "1183             C  \n",
       "1184             A  \n",
       "1185             C  \n",
       "1186             C  \n",
       "1187             C  \n",
       "1188             A  \n",
       "1189             A  \n",
       "1190             A  \n",
       "1191             A  \n",
       "1192             A  \n",
       "1193             A  \n",
       "1194             A  \n",
       "1195             A  \n",
       "1196             B  \n",
       "1197             B  \n",
       "1198             A  \n",
       "1199             C  \n",
       "\n",
       "[1200 rows x 38 columns]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first superficial analysis we can see that while importing this csv data, it has created a column called \"Unnamed: 0\" which consists only on the index of the data that we already know. As a result, we're going to drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're goin to gather more information of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 37 columns):\n",
      "label             1200 non-null object\n",
      "num.feature 1     1200 non-null object\n",
      "num.feature 2     1200 non-null object\n",
      "num.feature 3     1200 non-null object\n",
      "num.feature 4     1200 non-null object\n",
      "num.feature 5     1200 non-null object\n",
      "num.feature 6     1200 non-null object\n",
      "num.feature 7     1200 non-null object\n",
      "num.feature 8     1200 non-null object\n",
      "num.feature 9     1200 non-null object\n",
      "num.feature 10    1200 non-null object\n",
      "num.feature 11    1200 non-null object\n",
      "num.feature 12    1200 non-null object\n",
      "num.feature 13    1200 non-null object\n",
      "num.feature 14    1200 non-null object\n",
      "num.feature 15    1200 non-null object\n",
      "num.feature 16    1200 non-null object\n",
      "num.feature 17    1200 non-null object\n",
      "num.feature 18    1200 non-null object\n",
      "num.feature 19    1200 non-null object\n",
      "num.feature 20    1200 non-null object\n",
      "num.feature 21    1200 non-null object\n",
      "num.feature 22    1200 non-null object\n",
      "num.feature 23    1200 non-null object\n",
      "num.feature 24    1200 non-null object\n",
      "num.feature 25    1200 non-null object\n",
      "num.feature 26    1200 non-null object\n",
      "num.feature 27    1200 non-null object\n",
      "num.feature 28    1200 non-null object\n",
      "num.feature 29    1200 non-null object\n",
      "num.feature 30    1200 non-null object\n",
      "num.feature 31    1200 non-null object\n",
      "num.feature 32    1200 non-null object\n",
      "num.feature 33    1200 non-null object\n",
      "num.feature 34    1200 non-null object\n",
      "num.feature 35    1200 non-null object\n",
      "cat.feature 1     1200 non-null object\n",
      "dtypes: object(37)\n",
      "memory usage: 347.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that comes in our mind is that all variables have no missing values (NaNs). This is important because we won't need to spend time and computer efficiency while trying to reach those missing values. Nevertheless, notice that all variables are defined as object (the continuous ones and also the categorical one). The first thing that we're going to do is converting the numeric variables into floats. Then, we're going to transform the categorical variable into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_var = df.drop(['cat.feature 1'], axis = 1)\n",
    "num_var_col = [num_var.columns]\n",
    "for i in num_var_col:\n",
    "    df[i]=df[i].astype(float)\n",
    "df['cat.feature 1'] = df['cat.feature 1'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 37 columns):\n",
      "label             1200 non-null float64\n",
      "num.feature 1     1200 non-null float64\n",
      "num.feature 2     1200 non-null float64\n",
      "num.feature 3     1200 non-null float64\n",
      "num.feature 4     1200 non-null float64\n",
      "num.feature 5     1200 non-null float64\n",
      "num.feature 6     1200 non-null float64\n",
      "num.feature 7     1200 non-null float64\n",
      "num.feature 8     1200 non-null float64\n",
      "num.feature 9     1200 non-null float64\n",
      "num.feature 10    1200 non-null float64\n",
      "num.feature 11    1200 non-null float64\n",
      "num.feature 12    1200 non-null float64\n",
      "num.feature 13    1200 non-null float64\n",
      "num.feature 14    1200 non-null float64\n",
      "num.feature 15    1200 non-null float64\n",
      "num.feature 16    1200 non-null float64\n",
      "num.feature 17    1200 non-null float64\n",
      "num.feature 18    1200 non-null float64\n",
      "num.feature 19    1200 non-null float64\n",
      "num.feature 20    1200 non-null float64\n",
      "num.feature 21    1200 non-null float64\n",
      "num.feature 22    1200 non-null float64\n",
      "num.feature 23    1200 non-null float64\n",
      "num.feature 24    1200 non-null float64\n",
      "num.feature 25    1200 non-null float64\n",
      "num.feature 26    1200 non-null float64\n",
      "num.feature 27    1200 non-null float64\n",
      "num.feature 28    1200 non-null float64\n",
      "num.feature 29    1200 non-null float64\n",
      "num.feature 30    1200 non-null float64\n",
      "num.feature 31    1200 non-null float64\n",
      "num.feature 32    1200 non-null float64\n",
      "num.feature 33    1200 non-null float64\n",
      "num.feature 34    1200 non-null float64\n",
      "num.feature 35    1200 non-null float64\n",
      "cat.feature 1     1200 non-null category\n",
      "dtypes: category(1), float64(36)\n",
      "memory usage: 338.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to create a function to perform the transformation of the categoric variable into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A', 'B', 'C'], dtype='object')"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cat.feature 1'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_int(column):\n",
    "    column_int = 'int.' + column\n",
    "    cat=df[column].cat.categories\n",
    "    num_cat=len(cat)\n",
    "    df[column_int]=df[column]\n",
    "    df[column_int].cat.categories=range(num_cat)\n",
    "    df[column_int]=df[column_int].astype(int)\n",
    "    return(df[column_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       2\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "5       0\n",
       "6       0\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      0\n",
       "11      0\n",
       "12      1\n",
       "13      0\n",
       "14      2\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      2\n",
       "20      0\n",
       "21      0\n",
       "22      0\n",
       "23      1\n",
       "24      2\n",
       "25      0\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      0\n",
       "       ..\n",
       "1170    0\n",
       "1171    0\n",
       "1172    2\n",
       "1173    2\n",
       "1174    2\n",
       "1175    0\n",
       "1176    0\n",
       "1177    0\n",
       "1178    0\n",
       "1179    0\n",
       "1180    0\n",
       "1181    0\n",
       "1182    0\n",
       "1183    2\n",
       "1184    0\n",
       "1185    2\n",
       "1186    2\n",
       "1187    2\n",
       "1188    0\n",
       "1189    0\n",
       "1190    0\n",
       "1191    0\n",
       "1192    0\n",
       "1193    0\n",
       "1194    0\n",
       "1195    0\n",
       "1196    1\n",
       "1197    1\n",
       "1198    0\n",
       "1199    2\n",
       "Name: int.cat.feature 1, Length: 1200, dtype: int32"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_int('cat.feature 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['cat.feature 1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 37 columns):\n",
      "label                1200 non-null float64\n",
      "num.feature 1        1200 non-null float64\n",
      "num.feature 2        1200 non-null float64\n",
      "num.feature 3        1200 non-null float64\n",
      "num.feature 4        1200 non-null float64\n",
      "num.feature 5        1200 non-null float64\n",
      "num.feature 6        1200 non-null float64\n",
      "num.feature 7        1200 non-null float64\n",
      "num.feature 8        1200 non-null float64\n",
      "num.feature 9        1200 non-null float64\n",
      "num.feature 10       1200 non-null float64\n",
      "num.feature 11       1200 non-null float64\n",
      "num.feature 12       1200 non-null float64\n",
      "num.feature 13       1200 non-null float64\n",
      "num.feature 14       1200 non-null float64\n",
      "num.feature 15       1200 non-null float64\n",
      "num.feature 16       1200 non-null float64\n",
      "num.feature 17       1200 non-null float64\n",
      "num.feature 18       1200 non-null float64\n",
      "num.feature 19       1200 non-null float64\n",
      "num.feature 20       1200 non-null float64\n",
      "num.feature 21       1200 non-null float64\n",
      "num.feature 22       1200 non-null float64\n",
      "num.feature 23       1200 non-null float64\n",
      "num.feature 24       1200 non-null float64\n",
      "num.feature 25       1200 non-null float64\n",
      "num.feature 26       1200 non-null float64\n",
      "num.feature 27       1200 non-null float64\n",
      "num.feature 28       1200 non-null float64\n",
      "num.feature 29       1200 non-null float64\n",
      "num.feature 30       1200 non-null float64\n",
      "num.feature 31       1200 non-null float64\n",
      "num.feature 32       1200 non-null float64\n",
      "num.feature 33       1200 non-null float64\n",
      "num.feature 34       1200 non-null float64\n",
      "num.feature 35       1200 non-null float64\n",
      "int.cat.feature 1    1200 non-null int32\n",
      "dtypes: float64(36), int32(1)\n",
      "memory usage: 342.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take analyse our data in a more accurate way as all variables are correctly defined as numeric (floats or integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Continuous Numeric Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>num.feature 1</th>\n",
       "      <th>num.feature 2</th>\n",
       "      <th>num.feature 3</th>\n",
       "      <th>num.feature 4</th>\n",
       "      <th>num.feature 5</th>\n",
       "      <th>num.feature 6</th>\n",
       "      <th>num.feature 7</th>\n",
       "      <th>num.feature 8</th>\n",
       "      <th>num.feature 9</th>\n",
       "      <th>num.feature 10</th>\n",
       "      <th>num.feature 11</th>\n",
       "      <th>num.feature 12</th>\n",
       "      <th>num.feature 13</th>\n",
       "      <th>num.feature 14</th>\n",
       "      <th>num.feature 15</th>\n",
       "      <th>num.feature 16</th>\n",
       "      <th>num.feature 17</th>\n",
       "      <th>num.feature 18</th>\n",
       "      <th>num.feature 19</th>\n",
       "      <th>num.feature 20</th>\n",
       "      <th>num.feature 21</th>\n",
       "      <th>num.feature 22</th>\n",
       "      <th>num.feature 23</th>\n",
       "      <th>num.feature 24</th>\n",
       "      <th>num.feature 25</th>\n",
       "      <th>num.feature 26</th>\n",
       "      <th>num.feature 27</th>\n",
       "      <th>num.feature 28</th>\n",
       "      <th>num.feature 29</th>\n",
       "      <th>num.feature 30</th>\n",
       "      <th>num.feature 31</th>\n",
       "      <th>num.feature 32</th>\n",
       "      <th>num.feature 33</th>\n",
       "      <th>num.feature 34</th>\n",
       "      <th>num.feature 35</th>\n",
       "      <th>int.cat.feature 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.964167</td>\n",
       "      <td>0.394995</td>\n",
       "      <td>0.098596</td>\n",
       "      <td>0.532564</td>\n",
       "      <td>0.422863</td>\n",
       "      <td>0.356033</td>\n",
       "      <td>0.082350</td>\n",
       "      <td>0.514195</td>\n",
       "      <td>0.131063</td>\n",
       "      <td>0.084885</td>\n",
       "      <td>0.125530</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.269832</td>\n",
       "      <td>0.138377</td>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.437499</td>\n",
       "      <td>0.133395</td>\n",
       "      <td>-0.315088</td>\n",
       "      <td>0.125515</td>\n",
       "      <td>0.108832</td>\n",
       "      <td>-0.261238</td>\n",
       "      <td>-0.526801</td>\n",
       "      <td>-0.178032</td>\n",
       "      <td>0.097906</td>\n",
       "      <td>0.078464</td>\n",
       "      <td>0.559184</td>\n",
       "      <td>0.590363</td>\n",
       "      <td>0.166863</td>\n",
       "      <td>-0.758658</td>\n",
       "      <td>0.093808</td>\n",
       "      <td>0.193078</td>\n",
       "      <td>0.174101</td>\n",
       "      <td>0.102982</td>\n",
       "      <td>0.200697</td>\n",
       "      <td>0.072975</td>\n",
       "      <td>0.128052</td>\n",
       "      <td>0.475833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.177159</td>\n",
       "      <td>2.252283</td>\n",
       "      <td>1.033517</td>\n",
       "      <td>1.925060</td>\n",
       "      <td>4.288232</td>\n",
       "      <td>5.833616</td>\n",
       "      <td>1.025086</td>\n",
       "      <td>2.200539</td>\n",
       "      <td>0.966456</td>\n",
       "      <td>1.020703</td>\n",
       "      <td>1.003501</td>\n",
       "      <td>1.011806</td>\n",
       "      <td>2.334242</td>\n",
       "      <td>0.982136</td>\n",
       "      <td>1.040787</td>\n",
       "      <td>2.223783</td>\n",
       "      <td>0.987633</td>\n",
       "      <td>2.269627</td>\n",
       "      <td>1.014491</td>\n",
       "      <td>1.023470</td>\n",
       "      <td>3.985565</td>\n",
       "      <td>2.921890</td>\n",
       "      <td>2.011555</td>\n",
       "      <td>1.032395</td>\n",
       "      <td>0.987187</td>\n",
       "      <td>2.162870</td>\n",
       "      <td>2.050096</td>\n",
       "      <td>1.053863</td>\n",
       "      <td>4.418153</td>\n",
       "      <td>2.284759</td>\n",
       "      <td>2.257411</td>\n",
       "      <td>1.004652</td>\n",
       "      <td>0.980702</td>\n",
       "      <td>2.140749</td>\n",
       "      <td>0.968576</td>\n",
       "      <td>5.230978</td>\n",
       "      <td>0.801010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.109836</td>\n",
       "      <td>-3.277833</td>\n",
       "      <td>-5.896827</td>\n",
       "      <td>-12.400750</td>\n",
       "      <td>-21.272713</td>\n",
       "      <td>-3.197792</td>\n",
       "      <td>-6.573275</td>\n",
       "      <td>-3.462937</td>\n",
       "      <td>-3.189302</td>\n",
       "      <td>-3.210898</td>\n",
       "      <td>-4.169358</td>\n",
       "      <td>-8.822972</td>\n",
       "      <td>-4.074314</td>\n",
       "      <td>-3.131565</td>\n",
       "      <td>-6.437534</td>\n",
       "      <td>-3.149744</td>\n",
       "      <td>-7.879758</td>\n",
       "      <td>-3.786762</td>\n",
       "      <td>-3.163815</td>\n",
       "      <td>-13.439900</td>\n",
       "      <td>-11.026807</td>\n",
       "      <td>-6.264799</td>\n",
       "      <td>-3.525819</td>\n",
       "      <td>-3.150131</td>\n",
       "      <td>-6.324259</td>\n",
       "      <td>-5.551830</td>\n",
       "      <td>-3.087317</td>\n",
       "      <td>-13.278464</td>\n",
       "      <td>-7.382085</td>\n",
       "      <td>-7.896540</td>\n",
       "      <td>-3.342949</td>\n",
       "      <td>-3.742966</td>\n",
       "      <td>-8.096125</td>\n",
       "      <td>-3.252055</td>\n",
       "      <td>-16.444731</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.139956</td>\n",
       "      <td>-0.645805</td>\n",
       "      <td>-0.729419</td>\n",
       "      <td>-2.334541</td>\n",
       "      <td>-3.758604</td>\n",
       "      <td>-0.591190</td>\n",
       "      <td>-0.981457</td>\n",
       "      <td>-0.529842</td>\n",
       "      <td>-0.596455</td>\n",
       "      <td>-0.535099</td>\n",
       "      <td>-0.596213</td>\n",
       "      <td>-1.196368</td>\n",
       "      <td>-0.547828</td>\n",
       "      <td>-0.636374</td>\n",
       "      <td>-1.029325</td>\n",
       "      <td>-0.566355</td>\n",
       "      <td>-1.794535</td>\n",
       "      <td>-0.580734</td>\n",
       "      <td>-0.590109</td>\n",
       "      <td>-2.844312</td>\n",
       "      <td>-2.502463</td>\n",
       "      <td>-1.529416</td>\n",
       "      <td>-0.617683</td>\n",
       "      <td>-0.590036</td>\n",
       "      <td>-0.771492</td>\n",
       "      <td>-0.778916</td>\n",
       "      <td>-0.568743</td>\n",
       "      <td>-3.752840</td>\n",
       "      <td>-1.355948</td>\n",
       "      <td>-1.319428</td>\n",
       "      <td>-0.501404</td>\n",
       "      <td>-0.551042</td>\n",
       "      <td>-1.207891</td>\n",
       "      <td>-0.584542</td>\n",
       "      <td>-3.722073</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473224</td>\n",
       "      <td>0.095605</td>\n",
       "      <td>0.474654</td>\n",
       "      <td>0.408786</td>\n",
       "      <td>0.433386</td>\n",
       "      <td>0.072672</td>\n",
       "      <td>0.404405</td>\n",
       "      <td>0.125349</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.164859</td>\n",
       "      <td>0.140907</td>\n",
       "      <td>0.275747</td>\n",
       "      <td>0.151903</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.449159</td>\n",
       "      <td>0.089020</td>\n",
       "      <td>-0.431272</td>\n",
       "      <td>0.160146</td>\n",
       "      <td>0.112399</td>\n",
       "      <td>-0.103744</td>\n",
       "      <td>-0.601428</td>\n",
       "      <td>-0.216418</td>\n",
       "      <td>0.074816</td>\n",
       "      <td>0.074917</td>\n",
       "      <td>0.618008</td>\n",
       "      <td>0.638462</td>\n",
       "      <td>0.162891</td>\n",
       "      <td>-0.823076</td>\n",
       "      <td>0.187761</td>\n",
       "      <td>0.217674</td>\n",
       "      <td>0.173059</td>\n",
       "      <td>0.148114</td>\n",
       "      <td>0.217796</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.384989</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.907368</td>\n",
       "      <td>0.804529</td>\n",
       "      <td>1.804912</td>\n",
       "      <td>3.158848</td>\n",
       "      <td>4.283546</td>\n",
       "      <td>0.782072</td>\n",
       "      <td>2.050532</td>\n",
       "      <td>0.819176</td>\n",
       "      <td>0.823775</td>\n",
       "      <td>0.820052</td>\n",
       "      <td>0.790880</td>\n",
       "      <td>1.809292</td>\n",
       "      <td>0.786342</td>\n",
       "      <td>0.667449</td>\n",
       "      <td>1.869683</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>1.128540</td>\n",
       "      <td>0.788149</td>\n",
       "      <td>0.795205</td>\n",
       "      <td>2.262891</td>\n",
       "      <td>1.334926</td>\n",
       "      <td>1.151154</td>\n",
       "      <td>0.790508</td>\n",
       "      <td>0.744945</td>\n",
       "      <td>1.990754</td>\n",
       "      <td>1.990474</td>\n",
       "      <td>0.901615</td>\n",
       "      <td>2.026352</td>\n",
       "      <td>1.627969</td>\n",
       "      <td>1.729993</td>\n",
       "      <td>0.842643</td>\n",
       "      <td>0.764326</td>\n",
       "      <td>1.589543</td>\n",
       "      <td>0.742191</td>\n",
       "      <td>3.884139</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.304114</td>\n",
       "      <td>3.493776</td>\n",
       "      <td>6.563185</td>\n",
       "      <td>16.040157</td>\n",
       "      <td>18.379012</td>\n",
       "      <td>3.459009</td>\n",
       "      <td>10.247385</td>\n",
       "      <td>2.986858</td>\n",
       "      <td>3.144253</td>\n",
       "      <td>3.180150</td>\n",
       "      <td>3.228582</td>\n",
       "      <td>7.619684</td>\n",
       "      <td>3.328647</td>\n",
       "      <td>3.479617</td>\n",
       "      <td>7.628459</td>\n",
       "      <td>3.438960</td>\n",
       "      <td>6.258585</td>\n",
       "      <td>2.914268</td>\n",
       "      <td>4.125161</td>\n",
       "      <td>14.851201</td>\n",
       "      <td>8.911001</td>\n",
       "      <td>7.759072</td>\n",
       "      <td>2.982282</td>\n",
       "      <td>3.139641</td>\n",
       "      <td>8.580210</td>\n",
       "      <td>8.186917</td>\n",
       "      <td>3.462005</td>\n",
       "      <td>13.992788</td>\n",
       "      <td>7.697013</td>\n",
       "      <td>7.516701</td>\n",
       "      <td>3.687075</td>\n",
       "      <td>3.086134</td>\n",
       "      <td>7.303555</td>\n",
       "      <td>3.872649</td>\n",
       "      <td>17.127994</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  num.feature 1  num.feature 2  num.feature 3  \\\n",
       "count  1200.000000    1200.000000    1200.000000    1200.000000   \n",
       "mean      0.964167       0.394995       0.098596       0.532564   \n",
       "std       1.177159       2.252283       1.033517       1.925060   \n",
       "min       0.000000      -7.109836      -3.277833      -5.896827   \n",
       "25%       0.000000      -1.139956      -0.645805      -0.729419   \n",
       "50%       0.000000       0.473224       0.095605       0.474654   \n",
       "75%       2.000000       1.907368       0.804529       1.804912   \n",
       "max       3.000000       8.304114       3.493776       6.563185   \n",
       "\n",
       "       num.feature 4  num.feature 5  num.feature 6  num.feature 7  \\\n",
       "count    1200.000000    1200.000000    1200.000000    1200.000000   \n",
       "mean        0.422863       0.356033       0.082350       0.514195   \n",
       "std         4.288232       5.833616       1.025086       2.200539   \n",
       "min       -12.400750     -21.272713      -3.197792      -6.573275   \n",
       "25%        -2.334541      -3.758604      -0.591190      -0.981457   \n",
       "50%         0.408786       0.433386       0.072672       0.404405   \n",
       "75%         3.158848       4.283546       0.782072       2.050532   \n",
       "max        16.040157      18.379012       3.459009      10.247385   \n",
       "\n",
       "       num.feature 8  num.feature 9  num.feature 10  num.feature 11  \\\n",
       "count    1200.000000    1200.000000     1200.000000     1200.000000   \n",
       "mean        0.131063       0.084885        0.125530        0.121519   \n",
       "std         0.966456       1.020703        1.003501        1.011806   \n",
       "min        -3.462937      -3.189302       -3.210898       -4.169358   \n",
       "25%        -0.529842      -0.596455       -0.535099       -0.596213   \n",
       "50%         0.125349       0.109900        0.164859        0.140907   \n",
       "75%         0.819176       0.823775        0.820052        0.790880   \n",
       "max         2.986858       3.144253        3.180150        3.228582   \n",
       "\n",
       "       num.feature 12  num.feature 13  num.feature 14  num.feature 15  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean         0.269832        0.138377        0.041717        0.437499   \n",
       "std          2.334242        0.982136        1.040787        2.223783   \n",
       "min         -8.822972       -4.074314       -3.131565       -6.437534   \n",
       "25%         -1.196368       -0.547828       -0.636374       -1.029325   \n",
       "50%          0.275747        0.151903        0.032855        0.449159   \n",
       "75%          1.809292        0.786342        0.667449        1.869683   \n",
       "max          7.619684        3.328647        3.479617        7.628459   \n",
       "\n",
       "       num.feature 16  num.feature 17  num.feature 18  num.feature 19  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean         0.133395       -0.315088        0.125515        0.108832   \n",
       "std          0.987633        2.269627        1.014491        1.023470   \n",
       "min         -3.149744       -7.879758       -3.786762       -3.163815   \n",
       "25%         -0.566355       -1.794535       -0.580734       -0.590109   \n",
       "50%          0.089020       -0.431272        0.160146        0.112399   \n",
       "75%          0.779462        1.128540        0.788149        0.795205   \n",
       "max          3.438960        6.258585        2.914268        4.125161   \n",
       "\n",
       "       num.feature 20  num.feature 21  num.feature 22  num.feature 23  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean        -0.261238       -0.526801       -0.178032        0.097906   \n",
       "std          3.985565        2.921890        2.011555        1.032395   \n",
       "min        -13.439900      -11.026807       -6.264799       -3.525819   \n",
       "25%         -2.844312       -2.502463       -1.529416       -0.617683   \n",
       "50%         -0.103744       -0.601428       -0.216418        0.074816   \n",
       "75%          2.262891        1.334926        1.151154        0.790508   \n",
       "max         14.851201        8.911001        7.759072        2.982282   \n",
       "\n",
       "       num.feature 24  num.feature 25  num.feature 26  num.feature 27  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean         0.078464        0.559184        0.590363        0.166863   \n",
       "std          0.987187        2.162870        2.050096        1.053863   \n",
       "min         -3.150131       -6.324259       -5.551830       -3.087317   \n",
       "25%         -0.590036       -0.771492       -0.778916       -0.568743   \n",
       "50%          0.074917        0.618008        0.638462        0.162891   \n",
       "75%          0.744945        1.990754        1.990474        0.901615   \n",
       "max          3.139641        8.580210        8.186917        3.462005   \n",
       "\n",
       "       num.feature 28  num.feature 29  num.feature 30  num.feature 31  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean        -0.758658        0.093808        0.193078        0.174101   \n",
       "std          4.418153        2.284759        2.257411        1.004652   \n",
       "min        -13.278464       -7.382085       -7.896540       -3.342949   \n",
       "25%         -3.752840       -1.355948       -1.319428       -0.501404   \n",
       "50%         -0.823076        0.187761        0.217674        0.173059   \n",
       "75%          2.026352        1.627969        1.729993        0.842643   \n",
       "max         13.992788        7.697013        7.516701        3.687075   \n",
       "\n",
       "       num.feature 32  num.feature 33  num.feature 34  num.feature 35  \\\n",
       "count     1200.000000     1200.000000     1200.000000     1200.000000   \n",
       "mean         0.102982        0.200697        0.072975        0.128052   \n",
       "std          0.980702        2.140749        0.968576        5.230978   \n",
       "min         -3.742966       -8.096125       -3.252055      -16.444731   \n",
       "25%         -0.551042       -1.207891       -0.584542       -3.722073   \n",
       "50%          0.148114        0.217796        0.062700        0.384989   \n",
       "75%          0.764326        1.589543        0.742191        3.884139   \n",
       "max          3.086134        7.303555        3.872649       17.127994   \n",
       "\n",
       "       int.cat.feature 1  \n",
       "count        1200.000000  \n",
       "mean            0.475833  \n",
       "std             0.801010  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%             0.000000  \n",
       "75%             1.000000  \n",
       "max             2.000000  "
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily notice the following characteristics over the table which contains some basic informationf of the continuous numeric variables:\n",
    "1. The range of the maximum and minimum values that they assume is different throughout the sample;\n",
    "2. The standard deviation of all variables is bounded between the interval [0,6];\n",
    "3. The average of those variables is boundesd between [-1,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Categorical Numeric Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEFCAYAAAD+A2xwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJMiFECLKSZALIUSU+/+G6+U19ui1+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155dbe5e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sn.distplot(df['label'], axlabel='Classes', kde_kws={\"label\":'Label'} )\n",
    "ax = sn.distplot(df['int.cat.feature 1'], axlabel='Classes', kde_kws={\"label\":'Categoric Feature'}, ax=ax )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a frequency table for both label and categoric feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_table_label = pd.value_counts(df['label']).to_frame().reset_index()\n",
    "freq_table_label['Labels in %']=round(freq_table_label['label']/sum(freq_table_label['label'])*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>Labels in %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>622</td>\n",
       "      <td>51.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>231</td>\n",
       "      <td>19.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>230</td>\n",
       "      <td>19.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>117</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  label  Labels in %\n",
       "0    0.0    622        51.83\n",
       "1    3.0    231        19.25\n",
       "2    1.0    230        19.17\n",
       "3    2.0    117         9.75"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_table_feat = pd.value_counts(df['int.cat.feature 1']).to_frame().reset_index()\n",
    "freq_table_feat['Feature Classes in %']=round(freq_table_feat['int.cat.feature 1']/sum(freq_table_feat['int.cat.feature 1'])*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>int.cat.feature 1</th>\n",
       "      <th>Feature Classes in %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>864</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>235</td>\n",
       "      <td>19.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  int.cat.feature 1  Feature Classes in %\n",
       "0      0                864                 72.00\n",
       "1      2                235                 19.58\n",
       "2      1                101                  8.42"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency table above shows surprising results for those categorical variables:\n",
    "\n",
    "**a) Labels**\n",
    " - More than 50% of the labels on this dataset are correspondent to label 0. Also, about 20% of the data is related to labels 1 and 3 and only 10% to label 2. This clearly shows that to perform a Multi Layer Perceptron algorithm over this dataset, we will need to adopt some strategy to deal with the possible bias that would be introduced into the dataset as suggests *Philippe Thomas* on *\"Perceptron Learning for Classification Problems\"* (the paper is disposable [here](https://hal.archives-ouvertes.fr/hal-01232286/document));\n",
    " \n",
    "**b) Categorical Variable**\n",
    "- On the categorical variable, the unbalance is even higher: more than 70% of the datapoints refer to the same index (0), while only 20% refer to index 2 and 10% to index 1; However, this is not a huge problem, as this is only a feature and most of the algorithms address this properly (in case of the Percetron, the weight would be adjusted in such a way to gather the information contained inside the lower level of occurency of this event, while decision trees would use this as a threshold to determine the possible nodes of it); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-preparation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dealing with the unbalance class problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, this dataset has unbalanced class on the labels. This can be a potential problem while performing both Multi-Layer Perceptron and Random Forests, as it could create a bias in direction of the label which is over represented, potentially creating misleading conclusions. On the first case, the weights of the perceptrons will absorb this bias generating misleading results as, on average, pointing out to some label will increase the chance of predicting it. On the second case, the decision trees that form a random forest are potentially affected as the thresholds of the nodes generated via the maximum mutual information criteria can creat a bias over each tree; notice that, on average, the random forest will also be biased toward the direction of the label with the highest frequency over the dataset. Having this in mind, this is a serious potential problem that needs to be addressed while performing an adequate formulation of the data modelling scheme.\n",
    "One way to solve this problem is to define a matrix of weights related to the labels with the objective to \"re-balance\" them according to their  frequency. To do this, we will set following equation: firstly, define $g$ as the fraction between $s$ (the size of the sample, in this case it is $2000$) and $k$ (the number of possible classes that the label can assume, for instance, it is $4$ on this case). Also, define the vector $l_{i}$ which is a line vector with size ($f_{i},1$) with $f_{i}$ being the frequency of the label (e.g. for label $0$,  $f_{0}$ will be $622$). We want to find the vector $\\bar{w_{i}}^T$ such as its product with vector $\\bar{l_{i}}$ will result in $g$, for every $i=0,...,3$. Notice that each element from the vector $\\bar{w_{i}}^T$ is equal to $w_{i}$, so this problem will resume to finding $w_{i}$ that satisfies this equation.\n",
    "\n",
    "$$g= \\bar{w_{i}}^T  \\bar{l_{i}}$$\n",
    "\n",
    "$$g =\\frac{s}{k} $$\n",
    "\n",
    "However, we cannot fit this weights while analysing a Multi-Layer Perceptron using MLPClassifier. \n",
    "\n",
    "Another way to solve this problem is to adopt a naive approach called resample. In practice, this would imply in over-sampling the classes that are under represented over the dataset until you have equally distributed data. This is a simple way to deal with unbalanced problems that could be easily implemented on our dataset. So what we're going to do is selecting randomly some observations of those classes and we will replicate them until we reach a balanced classes over the labels space. To perform this, we're going to use a package developed by imbalanced-learn called RandomOverSampler. It will perform what have been described earlier: it will over-sample the classes that are under represented with the goal to create a balanced class of labels. The documentation of this new package can be found [here](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class perm:\n",
    "    seed = 123456\n",
    "    dataset = df.as_matrix()\n",
    "    np.random.seed(seed)\n",
    "    perm_dataset = np.random.permutation(dataset)\n",
    "    features = perm_dataset[:,1:]\n",
    "    labels = perm_dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined the class perm to perform a permutation over the dataset for them slice it to perform our over sampling. This has been done to create homogenous distribution of the dataset to later on while creating both training and test set finding more robust findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=458689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_res, labels_res = ros.fit_sample(perm.features, perm.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    0\n",
       "0    2.0  622\n",
       "1    3.0  622\n",
       "2    0.0  622\n",
       "3    1.0  622"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table_label_res = pd.value_counts(labels_res).to_frame().reset_index()\n",
    "freq_table_label_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2488"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = len(labels_res)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Setting the analysis sets and pre-processment of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over this section, we want to define a class that is going to perform the following objectives:\n",
    "1. Dividing it into two subsets:\n",
    "    - Training set: consists on the fraction over our sample that is going to be used to train the models that we're defing;\n",
    "    - Validation set: is the subset of our sample that have not been presented our model before with the objective of testing its accuracy while making predictions;\n",
    "2. Defining the possibility to aplly Principal Component Analysis over the data;\n",
    "    \n",
    "As we may want to compare different approaches to address the mentioned problems, we will define a class, as Professor Carlo Baldassi have done on the practice lesson of MLP, but with some modifications:\n",
    "1. As this dataset is small (only 2488 entries after perfoming the over sampling), we've decided to divide the dataset in two sections of 70 and 30% size to represent both training and validation sets, respectively. \n",
    "2. It has been defined the possibility to apply or no Principal Component Analysis (PCA) on the data: when n_components is equal to zero, we don't perform PCA; if n_components is defined, so it will takes the n_components responsible for '100xn_components%' of the overall variance of the data. The default value of n_components has been set to be equal to 0.9 (90%);\n",
    "3. We have used the same seed over our analysis to have a static analysis while trying to tune properly the hyperparameters of the model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = len(labels_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class init_MLP:\n",
    "    def __init__(self, frac_train = 0.7, n_components = 0.9, seed = 123456):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        do_pca = n_components is None or n_components > 0\n",
    "        \n",
    "        num_train=round(frac_train*sample)\n",
    "\n",
    "        train_data = features_res[:num_train,:]\n",
    "        train_target = labels_res[:num_train]\n",
    "        valid_data = features_res[num_train:sample,:]\n",
    "        valid_target = labels_res[num_train:sample]\n",
    "        \n",
    "        if do_pca:\n",
    "            pca = PCA(n_components = n_components)\n",
    "            pca.fit(train_data)\n",
    "            train_X = pca.transform(train_data)\n",
    "            valid_X = pca.transform(valid_data)\n",
    "        else:\n",
    "            train_X, valid_X = train_data, valid_data\n",
    "        \n",
    "        nc = train_X.shape[1]\n",
    "        print('Number of Components = %i' % train_X.shape[1])\n",
    "        \n",
    "        self.train_X, self.train_y = train_X, train_target\n",
    "        self.valid_X, self.valid_y = valid_X, valid_target\n",
    "        self.pca = pca if do_pca else None\n",
    "    \n",
    "    def train(self):\n",
    "        return self.train_X, self.train_y\n",
    "    \n",
    "    def valid(self):\n",
    "        return self.valid_X, self.valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing Multi Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this phase of our project, we're going to develop a Multi-Layer Perceptron model with the purpouse to apply over our dataset. Our goal on this section is try to explore all different settings of this algorithm (including estimation of optimal parameters and hyper-parameters over their possible space as also some suggestions over how to deal with the imbalance problem over the labels) in such a way to optimize its performance over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class init:\n",
    "    def __init__(self, frac_train = 0.7, n_components = 0.9, seed = 123456):\n",
    "        \n",
    "        sample = len(labels_res)\n",
    "        np.random.seed(seed)\n",
    "        do_pca = n_components is None or n_components > 0\n",
    "        \n",
    "        num_train=round(frac_train*sample)\n",
    "\n",
    "        train_data = features_res[:num_train,:]\n",
    "        train_target = labels_res[:num_train]\n",
    "        valid_data = features_res[num_train:sample,:]\n",
    "        valid_target = labels_res[num_train:sample]\n",
    "        \n",
    "        if do_pca:\n",
    "            pca = PCA(n_components = n_components)\n",
    "            pca.fit(train_data)\n",
    "            train_X = pca.transform(train_data)\n",
    "            valid_X = pca.transform(valid_data)\n",
    "        else:\n",
    "            train_X, valid_X = train_data, valid_data\n",
    "        \n",
    "        nc = train_X.shape[1]\n",
    "        print('Number of Components = %i' % train_X.shape[1])\n",
    "        \n",
    "        self.train_X, self.train_y = train_X, train_target\n",
    "        self.valid_X, self.valid_y = valid_X, valid_target\n",
    "        self.pca = pca if do_pca else None\n",
    "    \n",
    "    def train(self):\n",
    "        return self.train_X, self.train_y\n",
    "    \n",
    "    def valid(self):\n",
    "        return self.valid_X, self.valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Creating the raw MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the init class, the next is step is defining the Multi-Layer Perceptron. The first thing that we do is creating an auxiliar function that will help us while analysing both training and validation sets and evaluating the results of the algorithm over them. Also, we would like to check the results after each iteration, so we can plot some findings creating the possibility to analyse some of the results graphically. This will be important while setting in an optimal way the hyperparameters of the model. This function has the following constituents:\n",
    "1. We define the MLP function based on:\n",
    "     1. The percentage of the total data used to perform the training;\n",
    "     2. The number of components set to perform the PCA over the dataset (which can also be set as 0 i.e. not applying it over the data);\n",
    "     3. A fixed seed;\n",
    "     4. As the MLPClassifier has a lot of parameters, we only define the ones related to the data and we use a trick (as Professor Baldassi suggested) to fix the other parameters of it passing them as a dictionary called mlp_params; \n",
    "2. It takes the data class that have been formulated earlier as the dataset for performing the supervised learning task;\n",
    "    - This class has two arguments: (i) frac_train, (ii) n_components and (iii) seed;\n",
    "3. For analytical purpouse, we want to know the evolution of the MLP scores and loss function values on the test and training set after each iteration. To do this we create three empty lists that will append the results after each iteration and we use the *warm_start* option to use the result that have been found as input to the next iteration (for the documentation of MLPClassifier, click [here](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html));\n",
    "4. We measure the running time in which this algorithm has taken to perform over this dataset. This is a proxy of computational efficiency for our analysis over the hyperparameter space; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(frac_train =0.7, n_components = 0.9, seed = 123456,\n",
    "              **mlp_params):\n",
    "    \n",
    "    data = init(frac_train, n_components, seed)\n",
    "\n",
    "    max_iter = mlp_params['max_iter']\n",
    "    mlperc = MLPClassifier(**mlp_params)\n",
    "    \n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    loss_evol = []\n",
    "    start = t.time()\n",
    "    for it in range(max_iter):\n",
    "        mlperc.set_params(max_iter=it+1)\n",
    "        mlperc.fit(*data.train())\n",
    "        tscore = mlperc.score(*data.train())\n",
    "        vscore = mlperc.score(*data.valid())\n",
    "        loss = mlperc.loss_\n",
    "        train_scores.append(tscore)\n",
    "        valid_scores.append(vscore)\n",
    "        loss_evol.append(loss)\n",
    "        mlperc.set_params(warm_start=True)\n",
    "    end = t.time()\n",
    "    rt = round(end - start, 2)\n",
    "    train_avg = sum(train_scores)/max_iter\n",
    "    valid_avg = sum(valid_scores)/max_iter\n",
    "    print(f\"Average Training Score = {train_avg}; Average Validation Score = {valid_avg}; Running Time = {rt}s\")\n",
    "    return mlperc, data, train_scores, valid_scores, train_avg, valid_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Fine tuning the MLP to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it is time to calibrate the MLP adapting its hyperparameters with the objective to optimize its performance over our dataset. We start working over the elements of the MLPClassifier with a theoretical justification over our choices:\n",
    "    1. We are going to use a Stochastic Gradient Descent algorithm proposed by Kingma, Diederik, and Jimmy Ba, i.e. the Adam solver. The reason of this choice is that it performs relatively well over large datasets (with more than a thousand entries, which is our case, resulting in a faster convergence and also an optimal running time and score over training and validation sets). Also, the learning rate chosed was constant with the objective to create a lighter algorithm starting at 1e-2;\n",
    "    2. We prefer to use a regularizer term over our optimization problem i.e. setting a value over alpha;\n",
    "    3. The tolerance set to this algorithm is rather small as our objective is to produce an accurate result over our dataset. This we're going to set at its default value ($1e-4$) as suggests sklearn;\n",
    "    4. The batch size is related to the size of the sample that is going to be presented while performing the gradient descent algorithm (the batch algorithm) and adjusting its gradient towards the minimum;\n",
    "    5. The hidden layer size is the number of hidden layers that are going to be presented over our dataset (in terms of perceptrons and in terms of width of the neural network;\n",
    "    6. As we're going to use a Adam sover, we don't need to specify the following variables:\n",
    "        1. momentum (for gradient descent update over each iteration);\n",
    "        2. nesterovs_momentum (whether using a Nesterov's momentum or no);\n",
    "- The other hyperparameters such as batch_size, hidden layer size, alpha and tolerance are going to be choosen based on tests over the dataset that are going to be plotted graphically and evaluated with its justification in terms of time and performance in terms of score. Notice that while evaluating the results over the hyperparameters space we need to choose a order to make a coeteris paribus analysis while tryng to find theis optimal values. Having this in mind, we're going to start to deal with the batch size firstly, for then  procceed the analysis over the alpha of the regularizer, and only at the end we're going to fix the size of the we've adopted this strategy as we know that as higher is the hidden layer, better would be the performance of the MLP. However, not necessarly we're chosing the other paramters in a optimal way, which would constraint our results in terms of performance over scores and efficiency;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Chosing the optimal batch size for the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to find a batch size that is not too costly in terms of computational efficiency and also is able to generate good results in terms of the score over both test and validation set. Having this in mind, our largest threshold for it is going to be at most 20% of the overall sample size i.e. approximately 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50, 100, 150, 200, 250, 300, 350, 400, 450, 500])"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = np.linspace(50,500,10).astype(int)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Batch size : 50\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9626176808266357; Average Validation Score = 0.9048257372654155; Running Time = 2.36s\n",
      "Test number : 2, Batch size : 100\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9697072330654422; Average Validation Score = 0.9258310991957104; Running Time = 1.42s\n",
      "Test number : 3, Batch size : 150\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.966727898966705; Average Validation Score = 0.9179490616621985; Running Time = 1.31s\n",
      "Test number : 4, Batch size : 200\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9624856486796787; Average Validation Score = 0.9111126005361931; Running Time = 1.29s\n",
      "Test number : 5, Batch size : 250\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9569575200918479; Average Validation Score = 0.8954825737265415; Running Time = 1.17s\n",
      "Test number : 6, Batch size : 300\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9538920780711824; Average Validation Score = 0.8893029490616624; Running Time = 1.42s\n",
      "Test number : 7, Batch size : 350\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9494087256027552; Average Validation Score = 0.8768632707774799; Running Time = 1.4s\n",
      "Test number : 8, Batch size : 400\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9447187141216985; Average Validation Score = 0.8645040214477209; Running Time = 1.09s\n",
      "Test number : 9, Batch size : 450\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9411882893226169; Average Validation Score = 0.8564879356568366; Running Time = 1.1s\n",
      "Test number : 10, Batch size : 500\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.93857634902411; Average Validation Score = 0.8532305630026811; Running Time = 1.09s\n"
     ]
    }
   ],
   "source": [
    "batch_tscore=[]\n",
    "batch_vscore=[]\n",
    "for i,v in enumerate(batch):\n",
    "    print(f\"Test number : {i+1}, Batch size : {v}\")\n",
    "    mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter=100, frac_train=0.7,\n",
    "                                                                       hidden_layer_sizes = (100,),\n",
    "                                                                       batch_size = v,\n",
    "                                                                       learning_rate_init=1e-2,\n",
    "                                                                       solver = 'adam',\n",
    "                                                                       learning_rate = 'constant',\n",
    "                                                                       momentum = 0.0,\n",
    "                                                                       nesterovs_momentum = False,\n",
    "                                                                       alpha = 1e-3,\n",
    "                                                                       tol = 1e-3,\n",
    "                                                                       seed = 123456\n",
    "                                                                      )\n",
    "    batch_tscore.append(train_avg)\n",
    "    batch_vscore.append(valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJIWIiGhLUoiIiLYkhYiIaEtSiIiItv8B6PWKir7Fwd8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155ddfec630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(batch)), np.array(batch_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(batch)), np.array(batch_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Optimal Batch Size Analysis')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the Batch Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal Batch Size is : 100\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(batch_vscore):\n",
    "    if j == np.max(batch_vscore):\n",
    "        opt_batch = batch[i]\n",
    "        print(f\"The optimal Batch Size is : {opt_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as long as the batch size increases, not necessarly the test and validation scores also increases. Clearly, there is an optimal number of batches over this dataset that maximizes both test and validation scores. This clearly happens when the batch size is set at $100$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Chosing the optimal penalization term - Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to set the optimal size L2 penalty of the regularization term. For this task, we're going to analyse it on basis of decimals. The analysis will be similar as the one over the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = []\n",
    "for i in range(1,11):\n",
    "    size = 10**(-i)\n",
    "    alpha.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Alpha : 0.1\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9627095292766933; Average Validation Score = 0.9038873994638068; Running Time = 1.47s\n",
      "Test number : 2, Alpha : 0.01\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9666360505166476; Average Validation Score = 0.9231903485254689; Running Time = 1.4s\n",
      "Test number : 3, Alpha : 0.001\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9697072330654422; Average Validation Score = 0.9258310991957104; Running Time = 1.43s\n",
      "Test number : 4, Alpha : 0.0001\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9695522388059701; Average Validation Score = 0.9267828418230565; Running Time = 1.49s\n",
      "Test number : 5, Alpha : 1e-05\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9697990815154995; Average Validation Score = 0.9280563002680964; Running Time = 1.49s\n",
      "Test number : 6, Alpha : 1e-06\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701549942594718; Average Validation Score = 0.9295844504021449; Running Time = 1.43s\n",
      "Test number : 7, Alpha : 1e-07\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701492537313433; Average Validation Score = 0.9295844504021449; Running Time = 1.81s\n",
      "Test number : 8, Alpha : 1e-08\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701492537313433; Average Validation Score = 0.9295844504021449; Running Time = 1.74s\n",
      "Test number : 9, Alpha : 1e-09\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701492537313433; Average Validation Score = 0.9295844504021449; Running Time = 1.69s\n",
      "Test number : 10, Alpha : 1e-10\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701492537313433; Average Validation Score = 0.9295844504021449; Running Time = 1.53s\n"
     ]
    }
   ],
   "source": [
    "alpha_tscore=[]\n",
    "alpha_vscore=[]\n",
    "for i,v in enumerate(alpha):\n",
    "    print(f\"Test number : {i+1}, Alpha : {v}\")\n",
    "    mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter=100, frac_train=0.7,\n",
    "                                                                       hidden_layer_sizes = (100,),\n",
    "                                                                       batch_size = 100,\n",
    "                                                                       learning_rate_init=1e-2,\n",
    "                                                                       solver = 'adam',\n",
    "                                                                       learning_rate = 'constant',\n",
    "                                                                       momentum = 0.0,\n",
    "                                                                       nesterovs_momentum = False,\n",
    "                                                                       alpha = v,\n",
    "                                                                       tol = 1e-3,\n",
    "                                                                       seed = 123456\n",
    "                                                                      )\n",
    "    alpha_tscore.append(train_avg)\n",
    "    alpha_vscore.append(valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKXFbKFEoIpEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155dd436be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(alpha)), np.array(alpha_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(alpha)), np.array(alpha_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Optimal Alpha Analysis')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the Alpha Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal Alpha Size is : 1e-06\n",
      "The optimal Alpha Size is : 1e-07\n",
      "The optimal Alpha Size is : 1e-08\n",
      "The optimal Alpha Size is : 1e-09\n",
      "The optimal Alpha Size is : 1e-10\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(alpha_vscore):\n",
    "    if j == np.max(alpha_vscore):\n",
    "        opt_alpha = alpha[i]\n",
    "        print(f\"The optimal Alpha Size is : {opt_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the learning rate have reached its maximum after the 6th iteration. After this, we can easily notice that it takes more time to run the algorithm and no improvements in terms of test and validation scores have been made (and the running time of the algorithm starts to increase, meaning costly operations). Thus, we're going to maintain the L2 size at $1e-6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Chosing the optimal level of the initialization term - learning_rate_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to procceed on the same way as we've done before with the Alpha: estimating the optimal value of the learning rate based on given values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_rate = []\n",
    "for i in range(1,11):\n",
    "    size = 10**(-i)\n",
    "    learn_rate.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Learning Rate : 0.1\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.8700287026406431; Average Validation Score = 0.7176943699731905; Running Time = 1.4s\n",
      "Test number : 2, Learning Rate : 0.01\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701549942594718; Average Validation Score = 0.9295844504021449; Running Time = 2.0s\n",
      "Test number : 3, Learning Rate : 0.001\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.8681400688863375; Average Validation Score = 0.6845978552278822; Running Time = 2.94s\n",
      "Test number : 4, Learning Rate : 0.0001\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.6230482204362802; Average Validation Score = 0.3016621983914209; Running Time = 2.09s\n",
      "Test number : 5, Learning Rate : 1e-05\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.26202066590126283; Average Validation Score = 0.16056300268096504; Running Time = 1.82s\n",
      "Test number : 6, Learning Rate : 1e-06\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.20888633754305386; Average Validation Score = 0.1762466487935656; Running Time = 2.07s\n",
      "Test number : 7, Learning Rate : 1e-07\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.2073191733639494; Average Validation Score = 0.1796246648793568; Running Time = 1.85s\n",
      "Test number : 8, Learning Rate : 1e-08\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.2064695752009189; Average Validation Score = 0.1796246648793568; Running Time = 1.64s\n",
      "Test number : 9, Learning Rate : 1e-09\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.20608495981630298; Average Validation Score = 0.1796246648793568; Running Time = 1.58s\n",
      "Test number : 10, Learning Rate : 1e-10\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.20608495981630298; Average Validation Score = 0.1796246648793568; Running Time = 1.94s\n"
     ]
    }
   ],
   "source": [
    "learn_rate_tscore=[]\n",
    "learn_rate_vscore=[]\n",
    "for i,v in enumerate(learn_rate):\n",
    "    print(f\"Test number : {i+1}, Learning Rate : {v}\")\n",
    "    mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter=100, frac_train=0.7,\n",
    "                                                                       hidden_layer_sizes = (100,),\n",
    "                                                                       batch_size = 100,\n",
    "                                                                       learning_rate_init=v,\n",
    "                                                                       solver = 'adam',\n",
    "                                                                       learning_rate = 'constant',\n",
    "                                                                       momentum = 0.0,\n",
    "                                                                       nesterovs_momentum = False,\n",
    "                                                                       alpha = 1e-6,\n",
    "                                                                       tol = 1e-3,\n",
    "                                                                       seed = 123456\n",
    "                                                                      )\n",
    "    learn_rate_tscore.append(train_avg)\n",
    "    learn_rate_vscore.append(valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155de9dea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(learn_rate)), np.array(learn_rate_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(learn_rate)), np.array(learn_rate_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Optimal Learning Rate')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the Learning Rate Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal Learning Rate is : 0.01\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(learn_rate_vscore):\n",
    "    if j == np.max(learn_rate_vscore):\n",
    "        opt_lear = learn_rate[i]\n",
    "        print(f\"The optimal Learning Rate is : {opt_lear}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly notice that the scores over both training and validation sets decreases after the $1e-2$ value of the learning rate. With this, we're going to set this value over the learning rate from now on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Hidden Layers Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, our main objective is to find an optimal number of perceptrons over the hidden layer for this dataset. Our first consideration is that we're going to define only one hidden layer for this MLP. This is due to the fact that we can obtain a pretty good result with a sufficiently high number of perceptrons over the first hidden layer on our dataset ([Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem])). We're going to fix the maximum number of perceptrons over the hidden layer to be below 500 as threshold (we've defined this celling with the objective to make the computational cost light to have a good running time of this algorithm) placing an increment of 25 over 25 new perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25,  50,  75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325,\n",
       "       350, 375, 400, 425, 450, 475, 500])"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percep_num = np.linspace(25,500,20).astype(int)\n",
    "percep_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Tol : 25\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.8883696900114814; Average Validation Score = 0.7007640750670245; Running Time = 1.13s\n",
      "Test number : 2, Tol : 50\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9432261768082665; Average Validation Score = 0.8524664879356565; Running Time = 1.43s\n",
      "Test number : 3, Tol : 75\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9610218140068888; Average Validation Score = 0.8977613941018764; Running Time = 1.4s\n",
      "Test number : 4, Tol : 100\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701549942594718; Average Validation Score = 0.9295844504021449; Running Time = 2.02s\n",
      "Test number : 5, Tol : 125\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9682146957520092; Average Validation Score = 0.9274128686327078; Running Time = 2.23s\n",
      "Test number : 6, Tol : 150\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9731859931113662; Average Validation Score = 0.9319302949061661; Running Time = 2.1s\n",
      "Test number : 7, Tol : 175\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.969500574052813; Average Validation Score = 0.9242225201072387; Running Time = 2.14s\n",
      "Test number : 8, Tol : 200\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.97105625717566; Average Validation Score = 0.9260589812332439; Running Time = 2.37s\n",
      "Test number : 9, Tol : 225\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9752009184845006; Average Validation Score = 0.9407908847184987; Running Time = 2.64s\n",
      "Test number : 10, Tol : 250\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9735189437428243; Average Validation Score = 0.9330697050938338; Running Time = 2.62s\n",
      "Test number : 11, Tol : 275\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9713605051664754; Average Validation Score = 0.9323994638069705; Running Time = 2.89s\n",
      "Test number : 12, Tol : 300\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9747014925373133; Average Validation Score = 0.9450134048257371; Running Time = 3.33s\n",
      "Test number : 13, Tol : 325\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9753157290470724; Average Validation Score = 0.940965147453083; Running Time = 4.92s\n",
      "Test number : 14, Tol : 350\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.972313432835821; Average Validation Score = 0.9343699731903485; Running Time = 4.07s\n",
      "Test number : 15, Tol : 375\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9735935706084959; Average Validation Score = 0.943029490616622; Running Time = 5.28s\n",
      "Test number : 16, Tol : 400\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9755625717566017; Average Validation Score = 0.9408847184986595; Running Time = 4.25s\n",
      "Test number : 17, Tol : 425\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9747818599311137; Average Validation Score = 0.9339678284182307; Running Time = 4.54s\n",
      "Test number : 18, Tol : 450\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9709127439724454; Average Validation Score = 0.9372922252010725; Running Time = 4.88s\n",
      "Test number : 19, Tol : 475\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9721699196326062; Average Validation Score = 0.935201072386059; Running Time = 5.76s\n",
      "Test number : 20, Tol : 500\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9726176808266362; Average Validation Score = 0.9334852546916889; Running Time = 5.53s\n"
     ]
    }
   ],
   "source": [
    "percep_num_tscore=[]\n",
    "percep_num_vscore=[]\n",
    "for i,v in enumerate(percep_num):\n",
    "    print(f\"Test number : {i+1}, Tol : {v}\")\n",
    "    mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter=100, frac_train=0.7,\n",
    "                                                                       hidden_layer_sizes = (v,),\n",
    "                                                                       batch_size = 100,\n",
    "                                                                       learning_rate_init=1e-2,\n",
    "                                                                       solver = 'adam',\n",
    "                                                                       learning_rate = 'constant',\n",
    "                                                                       momentum = 0.0,\n",
    "                                                                       nesterovs_momentum = False,\n",
    "                                                                       alpha = 1e-6,\n",
    "                                                                       tol = 1e-4,\n",
    "                                                                       seed = 123456\n",
    "                                                                      )\n",
    "    percep_num_tscore.append(train_avg)\n",
    "    percep_num_vscore.append(valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAETCAYAAADZHBoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155dd3d80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(percep_num)), np.array(percep_num_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(percep_num)), np.array(percep_num_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Optimal Number of Perceptrons over the Hidden Layers')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the Number of Perceptrons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of Perceptrons over the Hidden Layers is : 300\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(percep_num_vscore):\n",
    "    if j == np.max(percep_num_vscore):\n",
    "        opt_percep = percep_num[i]\n",
    "        print(f\"The optimal number of Perceptrons over the Hidden Layers is : {opt_percep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (v) Choosing the optimal number of iterations until convergence - max_iter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've already addressed the main hyperparameters of the MLP model, our objective now is to find an optimal number of iterations to make the algorithm converge in the quickest way as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20,  40,  60,  80, 100, 120, 140, 160, 180, 200, 220, 240, 260,\n",
       "       280, 300, 320, 340, 360, 380, 400])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percep_it = np.linspace(20,400,20).astype(int)\n",
    "percep_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Number of iterations : 20\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.7649540757749712; Average Validation Score = 0.5224530831099196; Running Time = 0.37s\n",
      "Test number : 2, Number of iterations : 40\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.8838260619977035; Average Validation Score = 0.6973190348525471; Running Time = 0.6s\n",
      "Test number : 3, Number of iterations : 60\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9265308075009567; Average Validation Score = 0.8049597855227879; Running Time = 0.93s\n",
      "Test number : 4, Number of iterations : 80\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9511265786452352; Average Validation Score = 0.884936327077748; Running Time = 1.33s\n",
      "Test number : 5, Number of iterations : 100\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9701549942594718; Average Validation Score = 0.9295844504021449; Running Time = 1.74s\n",
      "Test number : 6, Number of iterations : 120\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9745838117106772; Average Validation Score = 0.9341152815013404; Running Time = 2.53s\n",
      "Test number : 7, Number of iterations : 140\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9799983598491063; Average Validation Score = 0.952029873611643; Running Time = 2.84s\n",
      "Test number : 8, Number of iterations : 160\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9839444603903559; Average Validation Score = 0.9586377345844503; Running Time = 3.6s\n",
      "Test number : 9, Number of iterations : 180\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9860441382829441; Average Validation Score = 0.9636356866249628; Running Time = 4.33s\n",
      "Test number : 10, Number of iterations : 200\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.98552812858783; Average Validation Score = 0.9630294906166219; Running Time = 5.62s\n",
      "Test number : 11, Number of iterations : 220\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9885032877570191; Average Validation Score = 0.9742200828661954; Running Time = 6.45s\n",
      "Test number : 12, Number of iterations : 240\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9891934557979334; Average Validation Score = 0.9718833780160858; Running Time = 7.43s\n",
      "Test number : 13, Number of iterations : 260\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9896935441137509; Average Validation Score = 0.977036502371623; Running Time = 8.7s\n",
      "Test number : 14, Number of iterations : 280\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9910242742332295; Average Validation Score = 0.9779634239754882; Running Time = 9.8s\n",
      "Test number : 15, Number of iterations : 300\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9915671641791044; Average Validation Score = 0.9816711349419124; Running Time = 10.35s\n",
      "Test number : 16, Number of iterations : 320\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9906913748564868; Average Validation Score = 0.9793523793565683; Running Time = 12.21s\n",
      "Test number : 17, Number of iterations : 340\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9927517390423448; Average Validation Score = 0.9834884087683331; Running Time = 13.15s\n",
      "Test number : 18, Number of iterations : 360\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9935514734022196; Average Validation Score = 0.9844652963955913; Running Time = 14.02s\n",
      "Test number : 19, Number of iterations : 380\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9938923802042421; Average Validation Score = 0.9849089882884153; Running Time = 14.62s\n",
      "Test number : 20, Number of iterations : 400\n",
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9938906429391504; Average Validation Score = 0.9852211796246649; Running Time = 18.62s\n"
     ]
    }
   ],
   "source": [
    "percep_it_tscore=[]\n",
    "percep_it_vscore=[]\n",
    "for i,v in enumerate(percep_it):\n",
    "    print(f\"Test number : {i+1}, Number of iterations : {v}\")\n",
    "    mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter = v, frac_train=0.7,\n",
    "                                                                       hidden_layer_sizes = (v,),\n",
    "                                                                       batch_size = 100,\n",
    "                                                                       learning_rate_init=1e-2,\n",
    "                                                                       solver = 'adam',\n",
    "                                                                       learning_rate = 'constant',\n",
    "                                                                       momentum = 0.0,\n",
    "                                                                       nesterovs_momentum = False,\n",
    "                                                                       alpha = 1e-6,\n",
    "                                                                       tol = 1e-4,\n",
    "                                                                       seed = 123456\n",
    "                                                                      )\n",
    "    percep_it_tscore.append(train_avg)\n",
    "    percep_it_vscore.append(valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155ddffd0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(percep_it)), np.array(percep_it_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(percep_it)), np.array(percep_it_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Optimal Number of Iterations')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the Number of Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of Iterations is : 400\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(percep_it_vscore):\n",
    "    if j == np.max(percep_it_vscore):\n",
    "        opt_perc_it = percep_it[i]\n",
    "        print(f\"The optimal number of Iterations is : {opt_perc_it}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, as higher is the number of iterations, higher will be the performance of our algorithm. However, this comes at a price of a higher computational cost and time. As our sample is relatively small, we're going to choose the highest number of iterations with the objective to demonstrate the power of our algorithm. In practice, this is not the best approach, as we could have a larger dataset and this procedure is impractible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Final Results of a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting two options of maximum number of iterations: a max_iter size that equals the running time of the Random Forest algorithm and one which would result on a better result but with a higher running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lelec\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Score = 0.9936753731343283; Average Validation Score = 0.9862533512064343; Running Time = 14.17s\n"
     ]
    }
   ],
   "source": [
    "# Setting the max_iter to its highest value with the objective to generate a good convergence of the algorithm\n",
    "mlperc, data, train_scores, valid_scores, train_avg, valid_avg = MLP(max_iter=400, frac_train=0.7,\n",
    "                                                                   hidden_layer_sizes = (300,),\n",
    "                                                                   batch_size = 100,\n",
    "                                                                   learning_rate_init=1e-2,\n",
    "                                                                   solver = 'adam',\n",
    "                                                                   learning_rate = 'constant',\n",
    "                                                                   momentum = 0.0,\n",
    "                                                                   nesterovs_momentum = False,\n",
    "                                                                   alpha = 1e-6,\n",
    "                                                                   tol = 1e-4,\n",
    "                                                                   seed = 123456\n",
    "                                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all results that we've been developing over the last section, the MLP could have provided an Average Validation Score of 98.6% which is rather feasible and reasonable after all fine tuning made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implementing a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The third step over our individual project consists in applying a Random Forest model over our dataset. Our main objective here is to explore the best seeting of this algorithm without over-fitting it. Also, we're going to apply it over our already re-sampled dataset (that we've turned into a balanced using a random sampling technique over the step 2 section) as the results of a random forest is also affected by the unbalanced labels problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Creating the raw Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of what we've done over the MLP syntax can easily be applied over our Random Forest model. Regarding this, the first thing that we need to develop is the basis of what is going to be our Random Forest - this is the case for the init class that we've created. This class as we've suggested earlier, is important to pre-prepare the data initially shuffling it for then dividing into training and validation sets and also setting the possibility of applying PCA over the dataset. After we've been set the init class, we need to define the basis of the Random Forests model. We start doing this creating an auxiliar function that will set the basis of the algorithm without passing over it all parameters that we want to choose later on via validation. Going part by part, the random forest function has the following elements:\n",
    "1. We define the rand_trees based on:\n",
    "     1. The fraciton of the overall sample used for training purpouses;\n",
    "     2. The number of components set to perform the PCA over the dataset (which can also be set as 0 i.e. not performing PCA);\n",
    "     3. A fixed seed to fix the results;\n",
    "     4. Just as we have done with the MLP function, as the random tree demands a lot of parameters, we're going to fix them later on and so we're going to pass them as a dictionary; \n",
    "2. It takes the data class that have been formulated earlier as the dataset for performing the supervised learning task;\n",
    "    - This class has two arguments: (i) frac_train, (ii) n_components and (iii) seed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_trees(frac_train = 0.7, seed = 123457, n_components = 0.9,\n",
    "                 **xtrees_params):\n",
    "    \n",
    "    data = init(frac_train, n_components, seed)\n",
    "    \n",
    "    xtrees = ExtraTreesClassifier(**xtrees_params)\n",
    "    start = t.time()\n",
    "    xtrees.fit(*data.train())\n",
    "    train_score = xtrees.score(*data.train())\n",
    "    valid_score = xtrees.score(*data.valid())\n",
    "    end = t.time()\n",
    "    rt = round(end - start, 2)\n",
    "    print(f\"Average Training Score = {train_score}; Average Validation Score = {valid_score}; Running Time = {rt}s\")\n",
    "    return xtrees, data, train_score, valid_score, rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)  Fine tuning the Random Forests to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of our analysis consists in an optimization of the hyperparameters based on the values that they can assume over the hyperparameters' space. An important practical consideration over the Random Forests model is that its constituents i.e. the decision trees are very prone to overfitting if we consider a rather complex and deep tree. Having this in mind, is very important to create thresholds with the objective of bounding the possible outcomes in such a way that it would be able to generate a rich analysis but strict only to feasible results. In practical terms, we want to create some restrictions \n",
    "We start to explore the hyperparameter space over the theoretical decisions over the hyperparameters of the model for then procceed a exploration over the hyperparameters.\n",
    "1. Theoretical decision:\n",
    "    1. The criterium choosen to define the random forest is based on the entropy concept i.e. minimizing the entropy or maximizing the mutual information of the dataset. The main reason behind this criterium is that it is able to create simple trees that can be much more easily interpretable; \n",
    "    2. We have choosen the minimum needed number of entries to the tree made a split to be 2. This is very important to create a constraint over the complexity of the trees that constitute the random forest, not allowing them to specialize too much over this particular dataset. However, as we've not much observations, we assume that the random forest is able to deal with the overfitting in relation to the extension of the tree.\n",
    "    3. The minimum sample leaf will be set in such a way to avoid the specialization of the model even through noise;\n",
    "    4. The number of features that is going to be analysed has been set to 'auto' - this option takes the square root of the number of features;\n",
    "    5. The depth of the trees is also going to be set in such a way to avoid overfitting over a validation trial;\n",
    "    6. Both boostrap and also the out-of-bag samples to estimate the generalization accuracy (oob_score) are going to be tested at the final with the objective to check the robustness of the results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Setting the minimum number of leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of our practical analysis against overfitting will be through the setting of the number of elements over the final leafs of the tree. This is a way to not allow the singular decision trees to gather noise of the data instead of true signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25])"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_leaf = np.linspace(1,25,25).astype(int)\n",
    "min_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Number of elements on each leaf : 1\n",
      "Number of Components = 12\n",
      "Average Training Score = 1.0; Average Validation Score = 1.0; Running Time = 0.4s\n",
      "Test number : 2, Number of elements on each leaf : 2\n",
      "Number of Components = 12\n",
      "Average Training Score = 1.0; Average Validation Score = 1.0; Running Time = 0.36s\n",
      "Test number : 3, Number of elements on each leaf : 3\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9781859931113662; Average Validation Score = 0.806970509383378; Running Time = 0.36s\n",
      "Test number : 4, Number of elements on each leaf : 4\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.960390355912744; Average Validation Score = 0.6434316353887399; Running Time = 0.39s\n",
      "Test number : 5, Number of elements on each leaf : 5\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9230769230769231; Average Validation Score = 0.4128686327077748; Running Time = 0.38s\n",
      "Test number : 6, Number of elements on each leaf : 6\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9098737083811711; Average Validation Score = 0.38873994638069703; Running Time = 0.38s\n",
      "Test number : 7, Number of elements on each leaf : 7\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.883467278989667; Average Validation Score = 0.30563002680965146; Running Time = 0.4s\n",
      "Test number : 8, Number of elements on each leaf : 8\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.8679678530424799; Average Validation Score = 0.2868632707774799; Running Time = 0.38s\n",
      "Test number : 9, Number of elements on each leaf : 9\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.846727898966705; Average Validation Score = 0.26005361930294907; Running Time = 0.37s\n",
      "Test number : 10, Number of elements on each leaf : 10\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.8381171067738232; Average Validation Score = 0.257372654155496; Running Time = 0.37s\n",
      "Test number : 11, Number of elements on each leaf : 11\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.835820895522388; Average Validation Score = 0.23458445040214476; Running Time = 0.38s\n",
      "Test number : 12, Number of elements on each leaf : 12\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.817451205510907; Average Validation Score = 0.21983914209115282; Running Time = 0.36s\n",
      "Test number : 13, Number of elements on each leaf : 13\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7979334098737084; Average Validation Score = 0.20509383378016086; Running Time = 0.37s\n",
      "Test number : 14, Number of elements on each leaf : 14\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7950631458094145; Average Validation Score = 0.19436997319034852; Running Time = 0.36s\n",
      "Test number : 15, Number of elements on each leaf : 15\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7680826636050516; Average Validation Score = 0.16085790884718498; Running Time = 0.36s\n",
      "Test number : 16, Number of elements on each leaf : 16\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7640642939150402; Average Validation Score = 0.17158176943699732; Running Time = 0.36s\n",
      "Test number : 17, Number of elements on each leaf : 17\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7548794489092997; Average Validation Score = 0.1675603217158177; Running Time = 0.36s\n",
      "Test number : 18, Number of elements on each leaf : 18\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.748564867967853; Average Validation Score = 0.15147453083109919; Running Time = 0.38s\n",
      "Test number : 19, Number of elements on each leaf : 19\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7324913892078071; Average Validation Score = 0.1447721179624665; Running Time = 0.37s\n",
      "Test number : 20, Number of elements on each leaf : 20\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7123995407577497; Average Validation Score = 0.13136729222520108; Running Time = 0.37s\n",
      "Test number : 21, Number of elements on each leaf : 21\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.7089552238805971; Average Validation Score = 0.12734584450402145; Running Time = 0.37s\n",
      "Test number : 22, Number of elements on each leaf : 22\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.6969001148105626; Average Validation Score = 0.10589812332439678; Running Time = 0.35s\n",
      "Test number : 23, Number of elements on each leaf : 23\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.6951779563719862; Average Validation Score = 0.1032171581769437; Running Time = 0.36s\n",
      "Test number : 24, Number of elements on each leaf : 24\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.6969001148105626; Average Validation Score = 0.1126005361930295; Running Time = 0.36s\n",
      "Test number : 25, Number of elements on each leaf : 25\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.6831228473019518; Average Validation Score = 0.0938337801608579; Running Time = 0.36s\n"
     ]
    }
   ],
   "source": [
    "min_leaf_tscore=[]\n",
    "min_leaf_vscore=[]\n",
    "for i,v in enumerate(min_leaf):\n",
    "    print(f\"Test number : {i+1}, Number of elements on each leaf : {v}\")\n",
    "    xtrees, data, train_score, valid_score, rt = rand_trees(\n",
    "                                                frac_train = 0.7, n_components = 0.9,\n",
    "                                                n_estimators = 50,\n",
    "                                                max_features = 'auto',\n",
    "                                                criterion = \"entropy\",\n",
    "                                                max_depth = None,\n",
    "                                                min_samples_split = 2,\n",
    "                                                min_samples_leaf=v,\n",
    "                                                seed = 123457,\n",
    "                                                bootstrap = None,\n",
    "                                                oob_score = None,\n",
    "                                                n_jobs = 4\n",
    "                                                )\n",
    "    min_leaf_tscore.append(train_score)\n",
    "    min_leaf_vscore.append(valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAETCAYAAADUAmpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/gbxYPe1iFNWuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155de79a828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(min_leaf)), np.array(min_leaf_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(min_leaf)), np.array(min_leaf_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=3)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Number of elements in each leaf')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the number of elements over leafs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis, it is clear that this algorithm is fitting a lot of noise over it. As a result, it seems to be prudent take at least 3 elements in each leaf as the lowest number of elements after the algorithm reveals to be overfitting noise on it. We want to make it clear that this boundary have been picked with the main objective to control the robustness of the model over unseen data. We could have choosen only one entrie over each leaf. However, this would result in model specified to this dataset while absorbing all noise over the data as signals (which is wrong)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Setting the number of trees over the Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of our analysis consists in setting an optimal size in terms of trees on the Random Forests model. Our objective is to find a certain size that would generate a good level of randomization in each tree leaving resulting in a good estimation over the true test and validation scores. Also, this is important to balance potential fitting problems within trees, generating unbiased estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25,  50,  75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325,\n",
       "       350, 375, 400, 425, 450, 475, 500])"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trees = np.linspace(25,500,20).astype(int)\n",
    "num_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Number of trees : 25\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9781859931113662; Average Validation Score = 0.8190348525469169; Running Time = 0.35s\n",
      "Test number : 2, Number of trees : 50\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9781859931113662; Average Validation Score = 0.806970509383378; Running Time = 0.38s\n",
      "Test number : 3, Number of trees : 75\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9799081515499426; Average Validation Score = 0.8203753351206434; Running Time = 0.4s\n",
      "Test number : 4, Number of trees : 100\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9804822043628014; Average Validation Score = 0.8324396782841823; Running Time = 0.58s\n",
      "Test number : 5, Number of trees : 125\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.981630309988519; Average Validation Score = 0.8297587131367292; Running Time = 0.57s\n",
      "Test number : 6, Number of trees : 150\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8418230563002681; Running Time = 0.6s\n",
      "Test number : 7, Number of trees : 175\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8418230563002681; Running Time = 0.68s\n",
      "Test number : 8, Number of trees : 200\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8458445040214477; Running Time = 0.73s\n",
      "Test number : 9, Number of trees : 225\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.96s\n",
      "Test number : 10, Number of trees : 250\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8458445040214477; Running Time = 0.95s\n",
      "Test number : 11, Number of trees : 275\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.97s\n",
      "Test number : 12, Number of trees : 300\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 1.0s\n",
      "Test number : 13, Number of trees : 325\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8431635388739946; Running Time = 1.13s\n",
      "Test number : 14, Number of trees : 350\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8431635388739946; Running Time = 1.16s\n",
      "Test number : 15, Number of trees : 375\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8431635388739946; Running Time = 1.17s\n",
      "Test number : 16, Number of trees : 400\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8458445040214477; Running Time = 1.19s\n",
      "Test number : 17, Number of trees : 425\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8458445040214477; Running Time = 1.2s\n",
      "Test number : 18, Number of trees : 450\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8418230563002681; Running Time = 1.43s\n",
      "Test number : 19, Number of trees : 475\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8418230563002681; Running Time = 1.45s\n",
      "Test number : 20, Number of trees : 500\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8418230563002681; Running Time = 1.83s\n"
     ]
    }
   ],
   "source": [
    "num_trees_tscore=[]\n",
    "num_trees_vscore=[]\n",
    "for i,v in enumerate(num_trees):\n",
    "    print(f\"Test number : {i+1}, Number of trees : {v}\")\n",
    "    xtrees, data, train_score, valid_score, rt = rand_trees(\n",
    "                                                frac_train = 0.7, n_components = 0.9,\n",
    "                                                n_estimators = v,\n",
    "                                                max_features = 'auto',\n",
    "                                                criterion = \"entropy\",\n",
    "                                                max_depth = None,\n",
    "                                                min_samples_split = 2,\n",
    "                                                min_samples_leaf=3,\n",
    "                                                seed = 123457,\n",
    "                                                bootstrap = None,\n",
    "                                                oob_score = None,\n",
    "                                                n_jobs = 4\n",
    "                                                )\n",
    "    num_trees_tscore.append(train_score)\n",
    "    num_trees_vscore.append(valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAETCAYAAADH1SqlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcaTJQtjjDGeLFkYY4zx9P+hlin01djumgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155deb3f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(num_trees)), np.array(num_trees_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(num_trees)), np.array(num_trees_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=5)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=10)\n",
    "plt.title('Number of trees')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the number of trees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of Trees is : 225\n",
      "The optimal number of Trees is : 275\n",
      "The optimal number of Trees is : 300\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(num_trees_vscore):\n",
    "    if j == np.max(num_trees_vscore):\n",
    "        opt_num_trees = num_trees[i]\n",
    "        print(f\"The optimal number of Trees is : {opt_num_trees}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, the optimal number of trees over this dataset is both 225, 275 and 300. As a result, we're going to choose the smaller one as it is more efficient in terms of computational cost (it takes only 0.58s to run). Notice that this is a naive approach as our objective is to find the highest non-biased scores on both test and validation set as also maximizing the computational efficiency of the models. However, there are a bunch of other factors that affect this 'optimal' decision, which in turns would generate different results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Setting the depth of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most complicated parts over decision trees algorithms. A rather long decisiont tree is very prone to overfitting while a shallow tree is much easier to interpret and also provides more robust results. Thus, our objective is to find a depth over the tree that would result in an optimal seeting for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  5, 10, 15, 20, 25, 30, 35, 40, 45, 50])"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep = np.linspace(1,50,11).astype(int)\n",
    "dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test number : 1, Depth : 1\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.5166475315729047; Average Validation Score = 0.0; Running Time = 0.95s\n",
      "Test number : 2, Depth : 5\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.6819747416762342; Average Validation Score = 0.11662198391420911; Running Time = 0.64s\n",
      "Test number : 3, Depth : 10\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9098737083811711; Average Validation Score = 0.4973190348525469; Running Time = 0.61s\n",
      "Test number : 4, Depth : 15\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9730195177956372; Average Validation Score = 0.7882037533512064; Running Time = 0.83s\n",
      "Test number : 5, Depth : 20\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9822043628013777; Average Validation Score = 0.8458445040214477; Running Time = 0.94s\n",
      "Test number : 6, Depth : 25\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.95s\n",
      "Test number : 7, Depth : 30\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.95s\n",
      "Test number : 8, Depth : 35\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.85s\n",
      "Test number : 9, Depth : 40\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.94s\n",
      "Test number : 10, Depth : 45\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.86s\n",
      "Test number : 11, Depth : 50\n",
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.96s\n"
     ]
    }
   ],
   "source": [
    "dep_tscore=[]\n",
    "dep_vscore=[]\n",
    "for i,v in enumerate(dep):\n",
    "    print(f\"Test number : {i+1}, Depth : {v}\")\n",
    "    xtrees, data, train_score, valid_score, rt = rand_trees(\n",
    "                                                frac_train = 0.7, n_components = 0.9,\n",
    "                                                n_estimators = 225,\n",
    "                                                max_features = 'auto',\n",
    "                                                criterion = \"entropy\",\n",
    "                                                max_depth = v,\n",
    "                                                min_samples_split = 2,\n",
    "                                                min_samples_leaf=3,\n",
    "                                                seed = 123457,\n",
    "                                                bootstrap = None,\n",
    "                                                oob_score = None,\n",
    "                                                n_jobs = 4\n",
    "                                                )\n",
    "    dep_tscore.append(train_score)\n",
    "    dep_vscore.append(valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155dd1bbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tes, = plt.plot(range(len(dep)), np.array(dep_tscore), color='red', label='Test Score', linestyle='--')\n",
    "val, = plt.plot(range(len(dep)), np.array(dep_vscore), color='blue', label='Validation Score')\n",
    "tes_leg = plt.legend(handles=[tes], loc=4)\n",
    "ax = plt.gca().add_artist(tes_leg)\n",
    "plt.legend(handles=[val], loc=8)\n",
    "plt.title('Depth of the Decision Trees')\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Range of the depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal depth of the trees is : 25\n",
      "The optimal depth of the trees is : 30\n",
      "The optimal depth of the trees is : 35\n",
      "The optimal depth of the trees is : 40\n",
      "The optimal depth of the trees is : 45\n",
      "The optimal depth of the trees is : 50\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(dep_vscore):\n",
    "    if j == np.max(dep_vscore):\n",
    "        opt_dep = dep[i]\n",
    "        print(f\"The optimal depth of the trees is : {opt_dep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results suggests that after the 25 level of deepth all the trees will generate the same scores as they've stabilizes at a constant level. As a result, we prefer to choose the smallest level that generate the best results as it will be less prone to be affected by misleading signals of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Final results of the Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components = 12\n",
      "Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; Running Time = 0.99s\n"
     ]
    }
   ],
   "source": [
    "xtrees, data, train_score, valid_score, rt = rand_trees(\n",
    "                                        frac_train = 0.7, n_components = 0.9,\n",
    "                                        n_estimators = 225,\n",
    "                                        max_features = 'auto',\n",
    "                                        criterion = \"entropy\",\n",
    "                                        max_depth = 25,\n",
    "                                        min_samples_split = 2,\n",
    "                                        min_samples_leaf = 3,\n",
    "                                        seed = 123457,\n",
    "                                        bootstrap = None,\n",
    "                                        oob_score = None,\n",
    "                                        n_jobs = 4\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've finished our calibration, the next step will be based on applying bootstrapping method over our sample for the checking its out-of-box score. This have been made with the purpouse to create a robustness test over the model that we suggest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components = 12\n",
      "Average Training Score = 0.9529276693455798; Average Validation Score = 0.5804289544235925; Running Time = 0.82s\n"
     ]
    }
   ],
   "source": [
    "xtrees, data, train_score, valid_score, rt = rand_trees(\n",
    "                                        frac_train = 0.7, n_components = 0.9,\n",
    "                                        n_estimators = 225,\n",
    "                                        max_features = 'auto',\n",
    "                                        criterion = \"entropy\",\n",
    "                                        max_depth = 25,\n",
    "                                        min_samples_split = 2,\n",
    "                                        min_samples_leaf = 3,\n",
    "                                        seed = 123457,\n",
    "                                        bootstrap = True,\n",
    "                                        oob_score = True,\n",
    "                                        n_jobs = 4\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results over the Random Forests model provided an average validation score of 84.7% without bootstrapping and 58.0% applying this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing all estimations and applying both algorithms over our dataset, we've been able to find the following results:\n",
    "- **Multi-Layer Perceptron:**\n",
    "    1. Optimal MLP settings:\n",
    "        - Average Training Score = 0.9936753731343283; Average Validation Score = 0.9862533512064343; \n",
    "    2. Comparable MLP settings:\n",
    "        - Average Training Score = 0.9265308075009567; Average Validation Score = 0.8049597855227879; \n",
    "- **Random Forests:**\n",
    "    1. Without bootstrapping:\n",
    "        - Average Training Score = 0.9827784156142365; Average Validation Score = 0.8471849865951743; \n",
    "    2. With boostrapping:\n",
    "        - Average Training Score = 0.9529276693455798; Average Validation Score = 0.5804289544235925; \n",
    "        \n",
    "The first thing that it is important to point out it that all our premises over the distribution of the data points over our dataset, re-sampling methods and also calibration of the models via fine tuning over the hyperparameters, turn what we've done specified to this dataset, subjected to the validity of all hypothesis and theoretical implications that have been pointed out throughout this notebook. This clearly shows that different premises would result in better results over one or other model.\n",
    "We now start to compare both results that we've finded. It is clear that our main objective was finding a model that is the best non-biased estimator/ predictor over the data that have been provided. We've started our analysis proposing a solution to the unbalance problem over the class of labels that could potentially bias our result towards the more frequent class. We have suggested to use a weight setting to solve this problem; however the sklearn limitations over the MLP package doesn't have this option (notice that the Linear model perceptron has the _class_weight_ parameter, which is what we would like to set). With this, using a randomized re-sampler even being a naive approach was the solution that we've adopted as it provide good results without demanding a lot of pre-preparation of the data.\n",
    "Over the models that we've estimated, the focus was mainly on creating a good cost-benefit estimation in terms of score and computational efficiency. Over this, it is clear that the Random Forest model is by far much quicker than the MLP, while it have performed our final results in about 5~6% of the total time that the MLP have taken (this over my CPU, it varies depending on the hardware of your computer). Over large datasets this is very important as the computational cost of a MLP could be too high to compensate its estimation. However, as we've pointed, the Random Forest algorithm is very prone to overfitting of noise instead of true signal data over it - when we leave all the parameters on automoatic and we don't set a minimum leaf size it reaches a score of 1 on both training and validationset, which is clearly not true as this decreases significantly changing marginally some parameters, revealing that those estimations are not robust. This point is clear when we analyse the minimum leaf size - setting this parameter to 5 entries decreases significantly the performance of the algorithm. Setting a very small threshold of only 3 entries (about 0.1% over all dataset) was enough to undermine the scores of this model. Over the Multi-Layer Perceptron, one could say that fixing the maximum number of iterations to 400 takes a long time to perform analysis over this dataset, which is true. Having this in mind, we've choosen a more comparable option which takes more or less the same time to run as a random forest and we've find out that the scores are similar. In terms of interpretability of results, it seems that the potential advantages of the Random Forest doesn't apply here, as the the trees are very depth and the size of forest is particularly high, which makes both MLP and Random Forest algorithms on the same side.\n",
    "Thus, we conclude that for this dataset, the Multi-Layer Perceptron is better. Despite its computational cost (in terms of running time) its robustness seems to hold better, providing results that are not affected by little changes over the parameters and also pointing out to a good score over test and validation set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
